<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Earth4D: Production-Ready Space-Time Positional Encoding for World Models | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="How decomposed 4D hash encoding with learned probing enables planetary-scale deep learning with 99% parameter reduction while matching foundation model performance"> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026-iclr-blogpost-earth4d/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/earth4d-world-models/"> <script src="/2026-iclr-blogpost-earth4d/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/template.v2.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Earth4D: Production-Ready Space-Time Positional Encoding for World Models",
            "description": "How decomposed 4D hash encoding with learned probing enables planetary-scale deep learning with 99% parameter reduction while matching foundation model performance",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026-iclr-blogpost-earth4d/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/" rel="external nofollow noopener" target="_blank"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener" target="_blank">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Earth4D: Production-Ready Space-Time Positional Encoding for World Models</h1> <p>How decomposed 4D hash encoding with learned probing enables planetary-scale deep learning with 99% parameter reduction while matching foundation model performance</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#building-blocks">Building Blocks</a> </div> <div> <a href="#the-positional-encoding-challenge">The Positional Encoding Challenge</a> </div> <ul> <li> <a href="#why-earth-observation-needs-4d-encoding">Why Earth Observation Needs 4D Encoding</a> </li> <li> <a href="#limitations-of-existing-approaches">Limitations of Existing Approaches</a> </li> </ul> <div> <a href="#earth4d-architecture">Earth4D Architecture</a> </div> <ul> <li> <a href="#decomposed-spatiotemporal-representation">Decomposed Spatiotemporal Representation</a> </li> <li> <a href="#multi-resolution-hierarchy">Multi-Resolution Hierarchy</a> </li> <li> <a href="#hash-encoding-mechanics">Hash Encoding Mechanics</a> </li> </ul> <div> <a href="#the-hash-collision-problem">The Hash Collision Problem</a> </div> <ul> <li> <a href="#understanding-hash-collisions">Understanding Hash Collisions</a> </li> <li> <a href="#the-uint32-overflow-discovery">The uint32 Overflow Discovery</a> </li> <li> <a href="#collision-patterns-across-scales">Collision Patterns Across Scales</a> </li> </ul> <div> <a href="#learned-hash-probing">Learned Hash Probing</a> </div> <ul> <li> <a href="#how-learned-probing-works">How Learned Probing Works</a> </li> <li> <a href="#extreme-compression-results">Extreme Compression Results</a> </li> </ul> <div> <a href="#experimental-validation">Experimental Validation</a> </div> <ul> <li> <a href="#q1-matching-foundation-models-with-coordinates-alone">Q1: Matching Foundation Models with Coordinates Alone</a> </li> <li> <a href="#q2-can-we-achieve-99-parameter-reduction">Q2: Can We Achieve 99% Parameter Reduction?</a> </li> <li> <a href="#q3-rgb-reconstruction-from-elevation">Q3: RGB Reconstruction from Elevation</a> </li> </ul> <div> <a href="#implications-for-world-models">Implications for World Models</a> </div> <div> <a href="#limitations-and-future-work">Limitations and Future Work</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Imagine predicting wildfire risk across California’s 163,000 square miles using nothing but coordinates: no satellite imagery, no weather data, no topography. Just latitude, longitude, elevation, and time. Impossible?</p> <p>That’s what we thought too—until Earth4D proved otherwise.</p> <p>World models—AI systems that learn to simulate and predict complex spatiotemporal dynamics—have emerged as a powerful paradigm for understanding our planet. From climate forecasting to disaster response, these models traditionally rely on massive multimodal datasets: satellite imagery, weather reanalysis, topographic maps, and sensor networks. The assumption has been that <strong>rich inputs are necessary for rich predictions</strong>.</p> <p>But at the heart of all Earth observation data lies a more fundamental structure: <strong>space and time</strong>. Every measurement, every pixel, every sensor reading exists at specific coordinates (x,y,z,t). What if we could capture the essence of Earth’s dynamics by encoding this 4D spatiotemporal structure more effectively?</p> <p>This is the central question behind <strong>Earth4D</strong>, a production-ready 4D space-time positional encoder developed as part of the DeepEarth world model. Earth4D extends multi-resolution hash encoding from 3D graphics to four dimensions, achieving surprising results:</p> <ul> <li> <strong>Surpasses multimodal foundation models</strong> using only (x,y,z,t) coordinates—no satellite imagery, weather data, or topography required</li> <li> <strong>99% parameter reduction</strong> (724M → 5M) with 4× training speedup while maintaining strong performance</li> <li> <strong>Planetary-scale coverage</strong> from sub-meter to continental scale, with temporal precision from sub-second to centuries</li> </ul> <p>More importantly, Earth4D challenges a fundamental assumption in world modeling: rather than requiring massive multimodal pretraining, we can achieve state-of-the-art performance by learning rich spatiotemporal representations from coordinates alone. The key is getting the positional encoding right.</p> <h3 id="earth4d-in-the-deepearth-world-model">Earth4D in the DeepEarth World Model</h3> <p>Earth4D is a core component of <strong>DeepEarth</strong>, a self-supervised multi-modal world model for planetary-scale Earth observation. While DeepEarth can process diverse data types—satellite imagery, sensor readings, text descriptions—its secret weapon is Earth4D’s rich spatiotemporal encoding.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/deepearth_main_figure.png" alt="DeepEarth Architecture Overview" style="width: 100%;"> <figcaption><b>DeepEarth World Model Architecture.</b> Multi-modal data (images, text, sensor data) sampled around spatiotemporal events are encoded by modality-specific encoders and fused with Earth4D space-time embeddings. These universal tokens are jointly processed through an autoencoder that learns to reconstruct and simulate masked data. Earth4D provides the spatiotemporal grounding that enables the model to reason about where and when phenomena occur. (Figure from DeepEarth paper)</figcaption> </figure> <p>In this blog post, we focus specifically on <strong>Earth4D</strong>—the 4D positional encoder—and demonstrate that even in isolation, without multimodal data, it achieves remarkable performance. This validates that Earth4D captures genuinely useful spatiotemporal structure, not just auxiliary context for other modalities.</p> <hr> <h2 id="building-blocks-what-we-built-upon">Building Blocks: What We Built Upon</h2> <p>Earth4D combines several existing techniques in a novel way for planetary-scale Earth observation. Before diving into our work, it’s important to acknowledge the foundational methods we adapted:</p> <p><strong>InstantNGP (Müller et al., 2022)</strong><d-cite key="muller2022instant"></d-cite>: Introduced multi-resolution hash encoding for 3D neural graphics. Their insight: compress spatial features into fixed-size hash tables across multiple resolution levels. We extend this from 3D to 4D.</p> <p><strong>Grid4D (Xu et al., 2024)</strong><d-cite key="xu2024grid4d"></d-cite>: Pioneered decomposing 4D space-time into four 3D grids (xyz, xyt, yzt, xzt) for dynamic Gaussian splatting. We adopt their decomposition strategy wholesale, adapting it from computer graphics to Earth observation.</p> <p><strong>Learned Hash Probing (Takikawa et al., 2023)</strong><d-cite key="takikawa2023compact"></d-cite>: Developed at NVIDIA Toronto AI Lab to intelligently resolve hash collisions through learned probe offsets. We apply their technique to enable extreme compression for edge deployment.</p> <p><strong>Our contribution</strong>: We didn’t invent these components. Instead, Earth4D demonstrates that combining these graphics techniques—originally designed for rendering virtual scenes—works remarkably well for modeling our actual planet. The novelty is in the application domain, scale (planetary), and empirical validation (state-of-the-art ecological forecasting with coordinates alone).</p> <hr> <h2 id="the-positional-encoding-challenge">The Positional Encoding Challenge</h2> <h3 id="why-earth-observation-needs-4d-encoding">Why Earth Observation Needs 4D Encoding</h3> <p>Earth observation data presents unique challenges that distinguish it from typical deep learning tasks:</p> <p><strong>Continuous spatiotemporal coordinates</strong>: Unlike images with discrete pixel positions or text with sequential tokens, Earth data exists in continuous 4D space-time. A wildfire measurement at (37.77°N, -122.42°W, 50m elevation, March 23 2023 8:58 AM) needs precise encoding.</p> <p><strong>Extreme scale variation</strong>: We need to reason about phenomena at vastly different scales simultaneously—a climate model might track both individual storms (10-100km) and global circulation patterns (10,000km+). Temporal scales range from seconds (lightning strikes) to decades (climate change).</p> <p><strong>Planetary coverage</strong>: Unlike 3D graphics which operate in local coordinate frames, Earth observation requires global consistency. The same encoding must work for data from San Francisco, Sydney, and the South Pole.</p> <p><strong>Memory constraints</strong>: Processing planet-scale data quickly exhausts GPU memory. A naive grid representation at 1-meter resolution would require \(10^{18}\) grid cells globally—completely infeasible.</p> <h3 id="limitations-of-existing-approaches">Limitations of Existing Approaches</h3> <p><strong>Sinusoidal positional encodings</strong><d-cite key="vaswani2017attention"></d-cite>, while elegant for transformers, provide fixed basis functions that cannot adapt to complex spatiotemporal patterns in Earth data. They work well for sequential data but struggle with the intricate multi-scale structure of geospatial phenomena.</p> <p><strong>Learned embeddings</strong> require discretizing continuous coordinates. While Vision Transformers<d-cite key="dosovitskiy2021image"></d-cite> discretize images into patches, Earth observation data doesn’t have natural “patch” boundaries. Discretization either loses fine-grained resolution or creates memory-prohibitive lookup tables.</p> <p><strong>3D multi-resolution hash encoding</strong><d-cite key="muller2022instant"></d-cite> (InstantNGP) solved many of these problems for neural graphics by using hash tables to achieve memory-efficient multi-resolution encoding. However, it was designed for static 3D scenes. Earth observation fundamentally requires modeling <strong>how</strong> spatial patterns evolve <strong>over time</strong>—a 4D problem.</p> <p>Extending to 4D isn’t trivial. A naive 4D grid would explode memory requirements. Simply treating time as another spatial dimension loses the distinct characteristics of temporal dynamics (irreversibility, causality, different resolution requirements).</p> <p>What we need is a 4D encoding that:</p> <ol> <li>Handles continuous coordinates without discretization</li> <li>Scales efficiently to planetary coverage</li> <li>Captures multi-resolution structure in both space and time</li> <li>Adapts to data through learning</li> <li>Fits in GPU memory</li> </ol> <p>Earth4D addresses all of these requirements.</p> <hr> <h2 id="earth4d-architecture">Earth4D Architecture</h2> <h3 id="decomposed-spatiotemporal-representation">Decomposed Spatiotemporal Representation</h3> <p>Earth4D’s core innovation is a <strong>decomposed 4D encoding</strong> that separates spatial and temporal structure while capturing their interactions. Rather than a single 4D hash grid (which would be memory-prohibitive), Earth4D uses four 3D grids:</p> <ol> <li> <strong>XYZ Grid</strong>: Pure spatial encoding in Earth-Centered Earth-Fixed (ECEF) coordinates</li> <li> <strong>XYT Grid</strong>: Equatorial plane + time</li> <li> <strong>YZT Grid</strong>: 90°E meridian plane + time</li> <li> <strong>XZT Grid</strong>: Prime meridian plane + time</li> </ol> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/earth4d_spacetime_encoder.png" alt="Earth4D Space-Time Positional Encoding" style="width: 100%;"> <figcaption><b>Earth4D Space-Time Positional Encoding.</b> A planetary-scale 4D encoder with fully decomposable spatio-temporal representation. Geographic coordinates (latitude, longitude, elevation) are converted to Earth-Centered Earth-Fixed (ECEF) coordinates and normalized. Four grids (xyz, xyt, yzt, xzt) are each learned in 3D space and computed in parallel. Each grid has multiple resolution levels, enabling deep learning of complex joint distributions in multi-modal data across space-time scales. (Figure from DeepEarth paper)</figcaption> </figure> <p><strong>Credit where credit is due</strong>: This decomposition strategy comes directly from <strong>Grid4D</strong><d-cite key="xu2024grid4d"></d-cite> (Xu et al., 2024), developed for high-fidelity dynamic Gaussian splatting. Grid4D pioneered the idea of decomposing 4D space-time into four 3D grids (xyz, xyt, yzt, xzt) rather than using a single 4D grid. We adapted their decomposition approach from computer graphics to planetary-scale Earth observation, but the core architectural insight is theirs.</p> <p>By using three orthogonal spatiotemporal projections, we capture how spatial patterns evolve over time from different perspectives. The XYZ grid provides pure spatial context, while XYT, YZT, and XZT encode temporal dynamics in complementary subspaces—exactly as Grid4D designed.</p> <p><strong>Why ECEF coordinates?</strong> We use Earth-Centered Earth-Fixed (ECEF) coordinates internally rather than latitude/longitude because:</p> <ul> <li>ECEF provides uniform spatial hashing globally (no polar singularities)</li> <li>Distances are Euclidean, making interpolation consistent</li> <li>3D Cartesian coordinates align naturally with hash encoding</li> </ul> <p>Input coordinates (latitude, longitude, elevation, time) are automatically converted to ECEF and normalized to \([-1, 1]\).</p> <h3 id="multi-resolution-hierarchy">Multi-Resolution Hierarchy</h3> <p>Each of the four grids operates at multiple resolution levels simultaneously. This multi-resolution structure is crucial for capturing both local details and global patterns.</p> <p>At level \(L\), the grid resolution is:</p> \[r_L = b \cdot g^L\] <p>where \(b\) is the base resolution (typically 32) and \(g\) is the growth factor (typically \(\sqrt{2}\)). With 24 levels (default configuration), spatial resolution ranges from:</p> <ul> <li> <strong>Level 1</strong>: 398.2 km/cell (continental scale)</li> <li> <strong>Level 12</strong>: 194.4 m/cell (city scale)</li> <li> <strong>Level 24</strong>: 4.75 cm/cell (sub-meter precision)</li> </ul> <p>Temporal resolution similarly spans from years to sub-second precision.</p> <h3 id="hash-encoding-mechanics">Hash Encoding Mechanics</h3> <p><strong>Intuition first</strong>: Think of Earth4D like a hierarchical address system. At the coarsest level, we divide the planet into large regions (like countries). At finer levels, we divide into cities, neighborhoods, streets, and buildings. Each location gets multiple “addresses” at different zoom levels.</p> <p>The challenge: storing a unique feature for every possible location at every resolution level would require astronomical memory. The solution: <strong>hash encoding</strong> compresses this into a fixed-size table using a clever mathematical trick.</p> <p><strong>The process</strong> at each resolution level \(L\):</p> <ol> <li> <p><strong>Discretize</strong>: Convert continuous coordinates to grid positions <code class="language-plaintext highlighter-rouge">pos_grid = floor(coordinate × resolution_L)</code></p> </li> <li> <strong>Index or Hash</strong>: <ul> <li>Coarse levels (few grid cells): Store directly, no collisions</li> <li>Fine levels (many grid cells): Hash multiple positions to the same memory slot</li> </ul> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>if grid_size ≤ hashmap_size:
    index = pos_grid.x + pos_grid.y × stride_y + pos_grid.z × stride_z
else:
    index = hash(pos_grid) mod hashmap_size
</code></pre></div> </div> </li> <li> <strong>Interpolate</strong>: Blend features from 8 surrounding grid corners (trilinear interpolation)</li> </ol> <p>The hash function mixes coordinates using XOR with large primes:</p> \[\text{hash}(\mathbf{p}) = \bigoplus_{d=1}^{D} p_d \cdot \pi_d \pmod{T}\] <p><strong>Why does this work?</strong> The hash function scrambles nearby positions to distant memory locations, preventing clusters of similar coordinates from competing for the same slot. Think of it like distributing people across hotel rooms—random assignment prevents overcrowding.</p> <p><strong>Smoothstep interpolation</strong> \((S(t) = 3t^2 - 2t^3)\) provides C¹ continuous gradients, better than linear interpolation for smooth Earth phenomena like temperature fields or elevation gradients.</p> <p>The final output concatenates features from all grids and levels:</p> <ul> <li>4 grids × 24 levels × 2 features = <strong>192D embedding</strong> per (x,y,z,t) coordinate</li> </ul> <p>This entire process runs on GPU via custom CUDA kernels, enabling <strong>massively parallel encoding</strong> of millions of coordinates simultaneously.</p> <hr> <h2 id="the-hash-collision-problem">The Hash Collision Problem</h2> <h3 id="understanding-hash-collisions">Understanding Hash Collisions</h3> <p>Hash encoding’s memory efficiency comes with a tradeoff: <strong>hash collisions</strong>. When the grid size exceeds the hash table size, multiple different spatial positions can map to the same hash index.</p> <p>For example, with a hash table size of \(2^{22}\) (4 million entries) and level 24 grid resolution of \(2^{28}\) cells, only 1 in 64 grid cells gets a unique hash entry. The other 63 collide.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/collision_heatmap.png" alt="Collision Rate Analysis" style="width: 100%;"> <figcaption>Hash collision rates across resolution levels for different data distributions. Fine levels (high resolution) show expected 2-4% collision rates, except for power-of-2 artifacts at level 23.</figcaption> </figure> <p><strong>Are collisions bad?</strong> Not necessarily. The hash encoding literature<d-cite key="muller2022instant"></d-cite> shows that downstream networks (MLPs) can learn to disambiguate collisions when they’re relatively rare. The hash function’s randomness actually acts as a form of regularization.</p> <p>However, <strong>catastrophic collision patterns</strong> destroy information. If all temporal variations at a location map to the same index, we lose the ability to model temporal dynamics.</p> <h3 id="the-uint32-overflow-discovery">The uint32 Overflow Discovery</h3> <p><strong>A debugging story</strong>: During development, our temporal predictions were inexplicably bad. The model couldn’t distinguish between summer and winter at the same location—as if time didn’t exist.</p> <p>Investigation revealed catastrophic collision patterns:</p> <ul> <li> <strong>Level 8</strong>: 100% collision rate (41,261 coordinates mapped to only ~978 unique indices)</li> <li> <strong>Levels 13-19</strong>: 99.9% collision rate (coordinates with different timestamps but identical spatial positions mapped to the same memory slot)</li> </ul> <p>This violated fundamental expectations—collision rates should decrease as we move to finer resolutions, not stay constant at 99.9%. The hash function was broken, but how?</p> <p>After extensive debugging, we discovered a <strong>critical integer overflow bug</strong> in the CUDA kernel:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// BUGGY CODE</span>
<span class="kt">uint32_t</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">;</span> <span class="n">d</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">index</span> <span class="o">+=</span> <span class="n">pos_grid</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride</span><span class="p">;</span>
    <span class="n">stride</span> <span class="o">*=</span> <span class="n">resolution</span><span class="p">[</span><span class="n">d</span><span class="p">];</span>  <span class="c1">// OVERFLOW!</span>
<span class="p">}</span>
</code></pre></div></div> <p>At level 8 with resolution 2048 per dimension:</p> <ul> <li>After processing first two dimensions: <code class="language-plaintext highlighter-rouge">stride = 2048 × 2048 = 4,194,304</code> </li> <li>Next multiplication: <code class="language-plaintext highlighter-rouge">4,194,304 × 2048 = 8,589,934,592</code> </li> <li><strong>This overflows uint32 (max 4,294,967,295) and wraps to 0!</strong></li> </ul> <p>When stride became 0, the temporal dimension contributed nothing to the hash index. All temporal variation was lost.</p> <p><strong>The fix</strong> was simple but critical—use 64-bit arithmetic for intermediate calculations:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// FIXED CODE</span>
<span class="kt">uint64_t</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// Prevents overflow</span>
</code></pre></div></div> <p>After this fix:</p> <ul> <li>Level 8: 100% → <strong>40.5%</strong> collision rate</li> <li>Level 13: 99.9% → <strong>2.4%</strong> collision rate</li> <li>Level 19: 99.9% → <strong>2.1%</strong> collision rate</li> </ul> <p>This bug hunt revealed an important lesson: <strong>subtle integer overflow can catastrophically corrupt spatiotemporal encodings</strong>. The bug only manifested at specific resolution/hash-table size combinations, making it nearly invisible without careful analysis.</p> <blockquote> <p><strong>Key Takeaway</strong>: When implementing multi-resolution encodings, always use 64-bit arithmetic for index calculations, even if final indices fit in 32 bits. Intermediate products can overflow silently, destroying temporal/spatial information in ways that appear as poor model performance rather than obvious crashes.</p> </blockquote> <h3 id="collision-patterns-across-scales">Collision Patterns Across Scales</h3> <p>Even with the overflow bug fixed, hash collisions are inherent to memory-efficient encoding. We profiled collision rates across 10 different spatiotemporal data distributions:</p> <ol> <li> <strong>Uniform Random</strong>: Global Earth surface sampling</li> <li> <strong>Continental Sparse</strong>: Sparse coverage of North America</li> <li> <strong>City-Scale Cluster</strong>: 10km × 10km dense sampling</li> <li> <strong>Building-Scale</strong>: Single 10m × 10m area over time</li> <li> <strong>Time Series</strong>: Fixed locations sampled repeatedly over time</li> </ol> <p>Results showed collision rates ranging from 0% (coarse levels) to 2-4% (fine levels), with expected power-of-2 artifacts at level 23 (67M grid cells, 4M hash entries = exact 16× ratio).</p> <p>While 2-4% collision rate is acceptable for many applications, we wanted to push further: <strong>Can we reduce hash table size even more while maintaining quality?</strong></p> <p>The answer lies in making collisions smarter, not just rarer.</p> <hr> <h2 id="learned-hash-probing">Learned Hash Probing</h2> <p>Standard hash encoding treats all collisions equally—coordinates compete randomly for memory slots. But what if we could teach the model to <strong>intelligently allocate</strong> memory based on the actual data distribution?</p> <p>This is where learned hash probing becomes crucial for achieving extreme compression.</p> <h3 id="how-learned-probing-works">How Learned Probing Works</h3> <p><strong>The collision problem visualized</strong>: Imagine multiple coordinates competing for the same memory slot. Standard hash encoding assigns them randomly—some get lucky and land in empty slots, others collide and degrade quality.</p> <p><strong>Learned probing solution</strong>: Give the model multiple candidate slots and let it learn which to use. It’s like having backup hotel rooms—if your first choice is crowded, the system learns to route you to a better alternative.</p> <p><strong>Credit</strong>: Learned hash probing was developed by <strong>Takikawa et al. (2023)</strong> at NVIDIA Toronto AI Lab<d-cite key="takikawa2023compact"></d-cite> as an improvement to InstantNGP. We did not invent this technique—we applied it to Earth observation. Their method uses <strong>dual hashing with learned offsets</strong>:</p> \[\text{index} = N_p \times h_1(\mathbf{x}) + \mathcal{D}_c[h_2(\mathbf{x})]\] <p>Breaking this down:</p> <ul> <li>\(h_1(\mathbf{x})\): Primary hash (rough neighborhood)</li> <li>\(h_2(\mathbf{x})\): Secondary hash selecting among \(N_p\) candidates (typically 4-32 options)</li> <li>\(\mathcal{D}_c\): <strong>Learned codebook</strong>—the model discovers which offsets work best for the data</li> </ul> <p><strong>The learning process</strong>: Initially, the model randomly distributes data across all candidate slots. During training, gradients flow backward through a <strong>straight-through estimator</strong>—the forward pass selects a discrete index (hard choice), but the backward pass distributes gradients across all candidates weighted by softmax probabilities (soft gradients).</p> <p>Over thousands of training steps, the model learns patterns like:</p> <ul> <li>“Densely sampled urban regions should use probes 0-7”</li> <li>“Sparse oceanic regions can share probe 31”</li> <li>“Temporal clusters need dedicated probes to avoid interference”</li> </ul> <p>This <strong>data-adaptive collision resolution</strong> outperforms fixed hash functions because it learns the actual distribution of your training data rather than assuming uniform randomness.</p> <h3 id="extreme-compression-results">Extreme Compression Results</h3> <p>Learned hash probing enables dramatic parameter reduction. On the Globe-LFMC 2.0 benchmark<d-cite key="yebra2024globelfmc"></d-cite>:</p> <table> <thead> <tr> <th>Configuration</th> <th>Parameters</th> <th>GPU Memory</th> <th>Speed</th> <th>MAE</th> <th>R²</th> <th>vs Baseline</th> </tr> </thead> <tbody> <tr> <td> <strong>Baseline</strong> (\(2^{22}\) hash)</td> <td>724M</td> <td>12GB+</td> <td>1×</td> <td>16.6 pp</td> <td>0.582</td> <td>—</td> </tr> <tr> <td> <strong>Learned Probing</strong> (\(2^{22}\))</td> <td>724M</td> <td>12GB+</td> <td>1.7×</td> <td><strong>12.4 pp</strong></td> <td><strong>0.745</strong></td> <td>+28% R²</td> </tr> <tr> <td> <strong>Compressed</strong> (\(2^{14}\) hash)</td> <td><strong>5.1M</strong></td> <td><strong>850MB</strong></td> <td><strong>4×</strong></td> <td><strong>15.0 pp</strong></td> <td><strong>0.668</strong></td> <td>+14.7% R²</td> </tr> </tbody> </table> <p>The compressed configuration achieves:</p> <ul> <li> <strong>99.3% parameter reduction</strong> (724M → 5.1M)</li> <li> <strong>93% memory reduction</strong> (12GB → 850MB)</li> <li><strong>4× training speedup</strong></li> <li> <strong>Still outperforms baseline</strong> by 14.7% R²</li> </ul> <p>This is remarkable: by shrinking the hash table by \(256×\) (\(2^{22}\) → \(2^{14}\)) and adding learned probing, we maintain—and even improve—performance while fitting on edge devices.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/compression_tradeoff.png" alt="Compression Tradeoff" style="width: 90%;"> <figcaption>Performance vs parameter count. Learned hash probing (orange) enables 99% compression with minimal quality loss. The compressed 5.1M model outperforms the 724M baseline.</figcaption> </figure> <p><strong>Why does compression improve performance?</strong> We hypothesize that extreme compression acts as regularization, similar to how restricting a student’s note-taking forces deeper understanding rather than verbatim transcription.</p> <p><strong>Concrete analogy</strong>: Imagine learning geography with different-sized notebooks:</p> <ul> <li> <strong>Large notebook (724M params)</strong>: Write down every detail about every location → risk memorizing specific training examples</li> <li> <strong>Small notebook (5M params)</strong>: Must capture essential patterns → forced to learn “coastal regions are moist in winter” rather than “GPS coordinate 37.7749°N has LFMC 87% on Jan 15, 2019”</li> </ul> <p>The forced sharing of hash table entries encourages the model to discover <strong>reusable spatiotemporal features</strong> rather than overfitting to training coordinates. This parallels how dropout or weight decay improve generalization—constraints prevent memorization.</p> <hr> <h2 id="experimental-validation">Experimental Validation</h2> <p>We evaluate Earth4D through three research questions, each testing a critical capability for world models:</p> <blockquote> <p><strong>Q1: Can coordinates alone match multimodal foundation models?</strong> Tests whether spatiotemporal encoding captures enough information to compete with satellite imagery and weather data → <strong>Result</strong>: Earth4D surpasses Galileo foundation model (12.4 vs 12.6 MAE on ecological forecasting)</p> <p><strong>Q2: Can we achieve 99% parameter reduction while maintaining performance?</strong> Tests whether learned hash probing enables extreme compression → <strong>Result</strong>: 724M → 5M parameters with improved R² (0.582 → 0.668)</p> <p><strong>Q3: Can Earth4D learn arbitrary spatiotemporal functions?</strong> Tests generality by predicting RGB pixels from elevation alone → <strong>Result</strong>: 18% lower loss with learned probing on Houston wetlands reconstruction</p> </blockquote> <p>Let’s examine each in detail.</p> <h3 id="q1-matching-foundation-models-with-coordinates-alone">Q1: Matching Foundation Models with Coordinates Alone</h3> <p><strong>Question</strong>: Can Earth4D achieve state-of-the-art performance using only spatiotemporal coordinates, without satellite imagery, weather data, or other multimodal inputs?</p> <p><strong>Dataset</strong>: Globe-LFMC 2.0<d-cite key="yebra2024globelfmc"></d-cite>, a global benchmark for predicting Live Fuel Moisture Content (LFMC)—the percentage of water in vegetation relative to dry weight. LFMC is critical for wildfire risk assessment.</p> <ul> <li>89,764 field measurements across diverse plant species, geographic regions, and temporal periods (2000-2023)</li> <li>Train/test split: 76,467 / 13,297 (official AI2 split for fair comparison)</li> </ul> <p><strong>Baseline</strong>: Galileo<d-cite key="tseng2025galileo"></d-cite>, a Vision Transformer (5.3M parameters) pre-trained by Allen Institute for AI on:</p> <ul> <li>Sentinel-2 optical imagery (10m resolution, 13 spectral bands)</li> <li>Sentinel-1 SAR (radar, cloud-penetrating)</li> <li>ERA-5 weather reanalysis (temperature, precipitation, etc.)</li> <li>TerraClimate soil moisture and climate data</li> <li>SRTM topography (elevation, slope, aspect)</li> <li>(x,y,z,t) coordinates and species type</li> </ul> <p><strong>Earth4D Architecture</strong>:</p> <ul> <li>Earth4D encodes (x,y,z,t) into 192D embeddings</li> <li>Concatenated with learnable species embedding (initialized randomly)</li> <li>MLP predicts LFMC percentage</li> </ul> <p><strong>Results</strong>:</p> <table> <thead> <tr> <th>Model</th> <th>Data Inputs</th> <th>MAE</th> <th>R²</th> </tr> </thead> <tbody> <tr> <td> <strong>Galileo</strong> (pretrained)</td> <td>Coordinates + Species + <strong>Multimodal Remote Sensing</strong> </td> <td>12.6 pp</td> <td>0.72</td> </tr> <tr> <td> <strong>Earth4D</strong> (learned probing)</td> <td><strong>Coordinates + Species only</strong></td> <td><strong>12.4 pp</strong></td> <td><strong>0.745</strong></td> </tr> </tbody> </table> <p>Earth4D <strong>surpasses the pretrained foundation model</strong> (12.4 vs 12.6 MAE, 0.745 vs 0.72 R²) using only coordinates and species embeddings. No satellite imagery. No weather data. No topography.</p> <figure> <div style="display: flex; flex-direction: column; gap: 10px;"> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/geospatial_error_map_epoch_2500.png" alt="LFMC Geographic Error Distribution" style="width: 100%;"> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/temporal_predictions_epoch_2500.png" alt="LFMC Temporal Predictions" style="width: 100%;"> </div> <figcaption><b>Earth4D LFMC Prediction Performance.</b> <b>Top</b>: Geographic error distribution across CONUS shows low error in well-sampled regions, with median absolute error of 7.1 percentage points. <b>Bottom</b>: Temporal predictions (black line) closely track ground truth LFMC measurements (gray distribution) across seasons (2017-2023), demonstrating Earth4D's ability to capture seasonal vegetation moisture dynamics using only spatiotemporal coordinates. (Figures from DeepEarth paper)</figcaption> </figure> <p><strong>How is this possible?</strong> Earth4D learns that spatiotemporal coordinates encode surprisingly rich information about vegetation moisture:</p> <p><strong>Spatial patterns</strong>:</p> <ul> <li>Coastal California (37°N, -122°W, low elevation) → high LFMC in winter (Pacific moisture)</li> <li>Arizona desert (33°N, -111°W, moderate elevation) → low LFMC year-round (arid climate)</li> <li>Pacific Northwest (47°N, -122°W, high elevation) → consistently high LFMC (temperate rainforest)</li> </ul> <p><strong>Temporal patterns</strong>:</p> <ul> <li>Summer months (June-August) → lower LFMC across most regions (heat stress, reduced precipitation)</li> <li>Winter months (December-February) → higher LFMC in Mediterranean climates (wet season)</li> <li>Spring months (March-May) → peak LFMC in temperate zones (snowmelt, spring rains)</li> </ul> <p><strong>Elevation effects</strong>:</p> <ul> <li>Low elevation (&lt;500m) → follows regional climate patterns directly</li> <li>Mid elevation (500-1500m) → extended moisture retention from orographic precipitation</li> <li>High elevation (&gt;1500m) → snowpack buffering creates delayed moisture dynamics</li> </ul> <p><strong>Spatiotemporal interactions</strong>: The same location exhibits different LFMC at different times, and the same time period exhibits different LFMC at different locations. Earth4D’s multi-resolution structure captures both:</p> <ul> <li> <strong>Coarse levels (398km/cell)</strong>: Encode climate zones—Mediterranean, desert, temperate, tropical</li> <li> <strong>Fine levels (4.75cm/cell)</strong>: Encode microclimate variations—north-facing vs south-facing slopes, proximity to water sources, local topographic moisture traps</li> </ul> <p>Crucially, the <strong>species embedding provides botanical context</strong>. Earth4D learns that chaparral shrubs in coastal California have different moisture dynamics than pine trees in the same location, despite identical coordinates. The model discovers species-specific water use strategies encoded in the learnable embedding.</p> <p>This result challenges a fundamental assumption: <strong>you don’t always need satellite imagery and weather data if your positional encoding is expressive enough</strong>. Rich spatiotemporal structure, learned through multi-resolution hash encoding, captures climate patterns that manifest in vegetation moisture.</p> <p><strong>Important caveat</strong>: This doesn’t mean coordinates are universally superior to multimodal data. Earth4D succeeds on LFMC prediction because:</p> <ol> <li>LFMC correlates strongly with climate zones, which are fundamentally spatiotemporal</li> <li>The dataset spans multiple years, allowing temporal patterns to be learned</li> <li>Species identity provides crucial botanical context</li> </ol> <p>For tasks requiring fine-grained visual understanding (crop disease detection, infrastructure damage assessment), satellite imagery would likely remain essential. Earth4D’s success highlights that <strong>some Earth observation tasks may be overengineered</strong>, relying on expensive multimodal data when simpler coordinate-based approaches suffice.</p> <h3 id="q2-can-we-achieve-99-parameter-reduction">Q2: Can We Achieve 99% Parameter Reduction?</h3> <p><strong>Question</strong>: Does the extreme compression result (99% parameter reduction, 4× speedup) shown earlier generalize across different configurations?</p> <p><strong>Experiment</strong>: We systematically vary hash table size and probing range across the LFMC benchmark:</p> <table> <thead> <tr> <th>Hash Size</th> <th>Probing</th> <th>Parameters</th> <th>Speed</th> <th>MAE</th> <th>R²</th> <th>Memory</th> </tr> </thead> <tbody> <tr> <td>\(2^{22}\)</td> <td>Disabled</td> <td>724M</td> <td>1.0×</td> <td>16.6</td> <td>0.582</td> <td>12GB+</td> </tr> <tr> <td>\(2^{22}\)</td> <td>\(N_p=4\)</td> <td>724M</td> <td>1.5×</td> <td>13.2</td> <td>0.698</td> <td>12GB+</td> </tr> <tr> <td>\(2^{22}\)</td> <td>\(N_p=32\)</td> <td>724M</td> <td>1.7×</td> <td><strong>12.4</strong></td> <td><strong>0.745</strong></td> <td>12GB+</td> </tr> <tr> <td>\(2^{18}\)</td> <td>\(N_p=32\)</td> <td>45M</td> <td>1.8×</td> <td>13.8</td> <td>0.672</td> <td>1.5GB</td> </tr> <tr> <td>\(2^{14}\)</td> <td>\(N_p=32\)</td> <td><strong>5.1M</strong></td> <td><strong>4.0×</strong></td> <td>15.0</td> <td>0.668</td> <td>850MB</td> </tr> </tbody> </table> <p><strong>Key findings</strong>:</p> <ol> <li> <strong>Learned probing consistently improves performance</strong> even at full hash table size (16.6 → 12.4 MAE)</li> <li> <strong>Larger probing range (\(N_p\)) improves quality</strong> but adds training overhead</li> <li> <strong>Sweet spot: \(2^{18}\) hash + \(N_p=32\)</strong> balances quality and efficiency (93.8% reduction, strong performance)</li> <li> <strong>Extreme compression (\(2^{14}\)) remains viable</strong> for edge deployment</li> </ol> <p>The 99% reduction result is robust across multiple trials and random seeds. The key enabler is learned probing’s ability to adaptively resolve collisions based on data distribution.</p> <h3 id="q3-rgb-reconstruction-from-elevation">Q3: RGB Reconstruction from Elevation</h3> <p><strong>Question</strong>: Can Earth4D learn to infer RGB pixel values from (x,y,z,t) coordinates alone?</p> <p>This tests a different capability: <strong>pure spatiotemporal function approximation</strong> without any auxiliary labels (like species type in LFMC).</p> <p><strong>Dataset</strong>: 5.8M coordinate-color pairs from Houston coastal wetlands:</p> <ul> <li> <strong>Input</strong>: USGS 3DEP LiDAR elevation (x,y,z in ECEF, t = acquisition date)</li> <li> <strong>Target</strong>: USDA NAIP RGB imagery (R,G,B values at corresponding location/time)</li> </ul> <p>The objective is \((x,y,z,t) \rightarrow (r,g,b)\): given only coordinates, predict the RGB color.</p> <p><strong>Architecture</strong>: Earth4D (192D) → MLP (3 hidden layers, 128 units) → RGB (3 channels)</p> <p><strong>Results</strong>:</p> <table> <thead> <tr> <th>Configuration</th> <th>Validation Loss</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>Baseline (no probing)</td> <td>0.0847</td> <td>—</td> </tr> <tr> <td>Learned Probing (\(N_p=32\))</td> <td><strong>0.0694</strong></td> <td><strong>-18%</strong></td> </tr> </tbody> </table> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/rgb_reconstruction_houston.png" alt="RGB Reconstruction from LiDAR Elevation" style="width: 100%;"> <figcaption><b>RGB Reconstruction from LiDAR Elevation.</b> Houston coastal wetlands, 2018. <b>Left to right</b>: LiDAR height (input), ground truth RGB, baseline reconstruction (no probing), learned probing reconstruction (18% lower loss). Earth4D learns to infer RGB pixel values from elevation and coordinates alone, capturing correlations between topography and land cover. Water bodies (low, flat elevation) appear blue/green, vegetation (moderate elevation) appears green, and urban areas (varied elevation) show gray/brown tones. (Data from USGS 3DEP and USDA NAIP)</figcaption> </figure> <p>The model learns complex correlations:</p> <ul> <li> <strong>Water bodies</strong> (low elevation, flat) → blue/green hues</li> <li> <strong>Vegetation</strong> (moderate elevation, rough terrain) → green</li> <li> <strong>Urban areas</strong> (high elevation variance) → gray/brown</li> <li> <strong>Coastal transitions</strong> (elevation gradients) → color gradients</li> </ul> <p>Learned probing significantly improves reconstruction quality, especially in fine-detail regions like coastline boundaries and vegetation patches.</p> <p><strong>Limitations of this experiment</strong>: While visually impressive, the RGB reconstruction task has significant limitations:</p> <ol> <li> <strong>Overfitting to local correlations</strong>: The model learns Houston-specific patterns (local vegetation types, soil colors, urban development). It wouldn’t generalize to other regions with different elevation-color relationships (e.g., desert regions where low elevation doesn’t imply water).</li> <li> <strong>Limited semantic understanding</strong>: The model doesn’t understand “what” it’s reconstructing—it’s purely statistical correlation between elevation and color in this specific dataset.</li> <li> <strong>Validation loss vs perceptual quality</strong>: 18% loss reduction doesn’t necessarily mean 18% better visual quality. Some fine details may improve while overall structure remains similar.</li> </ol> <p>This experiment demonstrates Earth4D’s ability to fit <strong>implicit spatiotemporal functions</strong>, but doesn’t prove it learns generalizable representations beyond the training distribution. For production applications, task-specific validation would be essential.</p> <hr> <h2 id="implications-for-world-models">Implications for World Models</h2> <p>Earth4D’s results suggest several important implications for building world models of Earth observation data:</p> <h3 id="1-positional-encodings-as-first-class-features">1. Positional Encodings as First-Class Features</h3> <p>Traditional approaches treat positional information as auxiliary context for satellite imagery or sensor data. Earth4D flips this: <strong>positional encodings can be primary features</strong> that capture rich spatiotemporal patterns.</p> <p>This is analogous to how CLIP<d-cite key="radford2021learning"></d-cite> showed that text alone (without pixel-level annotations) could guide image understanding through contrastive learning. Here, coordinates alone (without multimodal data) can achieve competitive performance through expressive encoding.</p> <p>For world models, this means we can:</p> <ul> <li> <strong>Bootstrap</strong> from coordinate-only data when multimodal inputs are unavailable</li> <li> <strong>Reduce dependency</strong> on expensive satellite imagery or weather reanalysis</li> <li> <strong>Generalize</strong> to regions/times with sparse observational coverage</li> </ul> <h3 id="2-extreme-efficiency-enables-edge-deployment">2. Extreme Efficiency Enables Edge Deployment</h3> <p>The 99% parameter reduction (724M → 5M) makes Earth4D viable for real-world deployment scenarios that were previously impossible:</p> <p><strong>Satellite onboard processing</strong>: Modern satellites like Planet Labs’ Doves have limited computational resources. Earth4D’s 5M parameter compressed model (850MB memory) can run on satellite GPUs, enabling real-time wildfire risk assessment as imagery is captured—eliminating the latency of downlinking data to ground stations.</p> <p><strong>Mobile disaster response</strong>: During wildfires or floods, field teams often operate in areas with limited connectivity. A tablet or smartphone running Earth4D can provide location-specific risk predictions (fire spread likelihood, flood extent forecasts) using only GPS coordinates and local time—no network connection required.</p> <p><strong>IoT sensor networks</strong>: Environmental monitoring stations deployed across forests or agricultural lands often run on solar power with limited energy budgets. Earth4D’s 4× faster inference enables battery-powered edge devices to perform hourly moisture monitoring, triggering alerts when fire risk exceeds thresholds.</p> <p><strong>Developing regions</strong>: Many countries lack access to expensive satellite imagery subscriptions or high-performance computing clusters. Earth4D democratizes Earth observation AI by requiring only coordinate data—freely available from GPS—rather than costly multimodal datasets.</p> <p>This shifts world models from datacenter-scale infrastructure to <strong>ubiquitous deployment</strong>, enabling real-time decision-making where it matters most: on satellites, in the field, and in resource-constrained environments.</p> <h3 id="3-learned-probing-as-universal-compression">3. Learned Probing as Universal Compression</h3> <p><strong>Broader applications</strong>: Takikawa et al.’s learned hash probing technique isn’t specific to Earth observation—it’s a <strong>general method for compressing hash-based neural representations</strong>. Beyond our Earth4D application, it has been used for:</p> <ul> <li>Neural radiance fields (NeRF) for 3D reconstruction</li> <li>Implicit neural representations for any spatiotemporal data</li> <li>Memory-efficient transformers with positional embeddings</li> </ul> <p>The key insight: <strong>let the model learn to resolve collisions</strong> rather than sizing hash tables conservatively.</p> <h3 id="4-downstream-task-agnostic">4. Downstream Task Agnostic</h3> <p>Earth4D produces a 192D embedding per (x,y,z,t) coordinate. This embedding can feed into:</p> <ul> <li> <strong>Classification</strong>: Crop type, land cover, disaster detection</li> <li> <strong>Regression</strong>: Temperature, precipitation, soil moisture</li> <li> <strong>Segmentation</strong>: Flood extent, deforestation boundaries</li> <li> <strong>Generation</strong>: Synthesizing satellite imagery from coordinates</li> <li> <strong>Forecasting</strong>: Predicting future states from current embeddings</li> </ul> <p>By separating the positional encoder from task-specific heads, we enable <strong>transfer learning</strong> across Earth observation tasks. Pretrain Earth4D on one task (LFMC), then fine-tune on another (crop yield), reusing the spatiotemporal representations.</p> <h3 id="5-foundation-for-multimodal-world-models">5. Foundation for Multimodal World Models</h3> <p>While Earth4D succeeds with coordinates alone, it’s designed for <strong>fusion with multimodal encoders</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Satellite Image] → Vision Encoder → 512D
[Weather Data]    → Time Series Enc → 256D
[(x,y,z,t)]       → Earth4D         → 192D
────────────────────────────────────────────
Concatenate       → Transformer     → Predictions
</code></pre></div></div> <p>The 4D positional embedding provides <strong>spatiotemporal grounding</strong> for other modalities. An image patch at (lat, lon) gets enriched with Earth4D’s multi-resolution features encoding its geospatial context.</p> <p>This mirrors how language models use positional encodings—not as replacements for tokens, but as essential context that enables attention mechanisms to reason about structure.</p> <hr> <h2 id="limitations-and-future-work">Limitations and Future Work</h2> <p>While Earth4D demonstrates strong performance, several limitations and open questions remain:</p> <h3 id="power-of-2-collision-artifacts">Power-of-2 Collision Artifacts</h3> <p>At level 23 (resolution \(2^{26}\), hash table \(2^{22}\)), collision rate jumps to 3.8% due to exact power-of-2 ratio. This creates periodic artifacts in hash distribution.</p> <p><strong>Mitigation</strong>: Use non-power-of-2 hash table sizes (large primes) or increase hash capacity. However, power-of-2 sizes align with GPU memory boundaries and enable bitwise optimizations.</p> <h3 id="hyperparameter-sensitivity">Hyperparameter Sensitivity</h3> <p>Earth4D has several hyperparameters:</p> <ul> <li>Number of resolution levels (default 24)</li> <li>Hash table size per grid (\(2^{14}\) to \(2^{22}\))</li> <li>Probing range \(N_p\) (2, 4, 8, 16, 32)</li> <li>Codebook size \(N_c\) (512 to 4096)</li> </ul> <p>While we provide reasonable defaults, <strong>optimal settings vary by task</strong>. Automated hyperparameter search (e.g., using validation loss) would improve usability.</p> <h3 id="learning-rate-tuning-for-probing">Learning Rate Tuning for Probing</h3> <p>Index logits gradients are 5-7 orders of magnitude smaller than embedding gradients (inherent to straight-through estimators). We recommend 100× higher learning rate for index logits:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span>
    <span class="p">{</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">:</span> <span class="n">encoder</span><span class="p">.</span><span class="n">embeddings</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">},</span>
    <span class="p">{</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">:</span> <span class="n">encoder</span><span class="p">.</span><span class="n">index_logits</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1e-1</span><span class="p">}</span>
<span class="p">])</span>
</code></pre></div></div> <p>This dual learning rate requirement adds complexity. <strong>Automatic gradient rescaling</strong> could simplify training.</p> <h3 id="global-vs-regional-tradeoffs">Global vs Regional Tradeoffs</h3> <p>Earth4D uses a single hash table for the entire planet. This is memory-efficient but treats all regions equally. Some regions (densely sampled urban areas) may benefit from finer-grained encoding than sparse regions (oceans, deserts).</p> <p><strong>Future work</strong>: Adaptive hash allocation based on data density. Allocate more hash capacity to high-information regions, less to homogeneous areas.</p> <h3 id="temporal-resolution-assumptions">Temporal Resolution Assumptions</h3> <p>Our experiments normalize time to \([0, 1]\) over the dataset’s temporal range. For applications spanning centuries (climate modeling), we may need explicit multi-scale temporal encoding (years, months, days, hours) similar to spatial multi-resolution.</p> <h3 id="interpretability">Interpretability</h3> <p>While Earth4D learns effective representations, understanding <strong>what</strong> it learns remains challenging. Visualization of hash table features could reveal:</p> <ul> <li>Which spatiotemporal patterns activate specific hash entries?</li> <li>How do features at different resolution levels specialize?</li> <li>Can we interpret learned probe offsets?</li> </ul> <p>Techniques from mechanistic interpretability<d-cite key="olah2020zoom"></d-cite> could shed light on Earth4D’s internal representations.</p> <hr> <h2 id="conclusion">Conclusion</h2> <p>When we set out to build Earth4D, we had a simple hypothesis: <strong>spatiotemporal structure matters more than we think</strong>. The conventional wisdom in Earth observation AI emphasizes collecting more modalities—higher resolution imagery, more spectral bands, denser weather data, richer metadata. But what if the key isn’t adding more data types, but rather encoding the fundamental structure—space and time—more effectively?</p> <p>Earth4D validates this hypothesis. Through decomposed 4D hash encoding and learned hash probing, it achieves state-of-the-art ecological forecasting using only (x,y,z,t) coordinates. No satellite imagery. No weather reanalysis. No topography. Just position in space-time.</p> <p><strong>Key results</strong>:</p> <ol> <li> <strong>Surpasses multimodal foundation models</strong> pretrained on diverse Earth observation data (12.4 vs 12.6 MAE on Globe-LFMC 2.0)</li> <li> <strong>99% parameter reduction</strong> (724M → 5M) with maintained or improved performance</li> <li> <strong>4× training speedup</strong> enabling edge deployment on satellites, mobile devices, and IoT sensors</li> <li> <strong>Planetary-scale coverage</strong> from sub-meter (4.75cm) to continental (398km) resolution, across sub-second to century timescales</li> </ol> <p>These results have implications beyond Earth observation. They suggest that for spatiotemporal problems more broadly—video understanding, robotics, autonomous navigation—<strong>positional encoding deserves first-class treatment</strong>, not just auxiliary context for pixel features.</p> <p><strong>The path forward</strong>: Earth4D is a component of the DeepEarth world model, where it provides spatiotemporal grounding for multimodal encoders. We envision future world models that:</p> <ul> <li> <strong>Bootstrap</strong> from coordinates when multimodal data is unavailable (sparse regions, historical periods, privacy-sensitive applications)</li> <li> <strong>Compress</strong> through learned hash probing, enabling deployment beyond datacenters</li> <li> <strong>Generalize</strong> by learning spatiotemporal patterns that transfer across tasks</li> </ul> <p>Earth4D is fully open source and ready for integration:</p> <p><strong>GitHub</strong>: <a href="https://github.com/legel/deepearth" rel="external nofollow noopener" target="_blank">https://github.com/legel/deepearth</a></p> <p>As we build AI systems to understand and simulate our planet—for climate adaptation, disaster response, agricultural resilience, and ecosystem monitoring—<strong>how we encode space and time fundamentally shapes what patterns models can discover</strong>.</p> <p>Earth4D demonstrates that with the right positional encoding, coordinates alone can capture the essence of Earth’s spatiotemporal dynamics. The future of world models may not require encoding everything about our planet. Perhaps we just need to encode the structure of space-time effectively—and let the model discover the rest.</p> <hr> <h2 id="acknowledgments">Acknowledgments</h2> <p>We thank the Allen Institute for AI for releasing the Globe-LFMC 2.0 dataset and Galileo baseline. We thank NVIDIA for open-sourcing InstantNGP, which inspired Earth4D’s architecture. We thank the USGS 3DEP and USDA NAIP programs for providing public LiDAR and imagery data.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026-iclr-blogpost-earth4d/assets/bibliography/2026-04-27-earth4d-world-models.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/useful-calibrated-uncertainties/">What (and What Not) are Calibrated Uncertainties Actually Useful for?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/tracing-principles-behind-modern-diffusion-models/">Tracing the Principles Behind Modern Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/symbolic-connect/">From Dense Monoliths to Modular Minds: The Rise of Symbolic Routing in LLMs</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026-iclr-blogpost-earth4d/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-data.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>