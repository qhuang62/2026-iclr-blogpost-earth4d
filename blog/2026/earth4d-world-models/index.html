<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Earth4D: Production-Ready Space-Time Positional Encoding for World Models | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="How decomposed 4D hash encoding with learned probing enables planetary-scale deep learning with 99% parameter reduction while matching foundation model performance"> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026-iclr-blogpost-earth4d/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/earth4d-world-models/"> <script src="/2026-iclr-blogpost-earth4d/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/template.v2.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Earth4D: Production-Ready Space-Time Positional Encoding for World Models",
            "description": "How decomposed 4D hash encoding with learned probing enables planetary-scale deep learning with 99% parameter reduction while matching foundation model performance",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026-iclr-blogpost-earth4d/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/" rel="external nofollow noopener" target="_blank"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener" target="_blank">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Earth4D: Production-Ready Space-Time Positional Encoding for World Models</h1> <p>How decomposed 4D hash encoding with learned probing enables planetary-scale deep learning with 99% parameter reduction while matching foundation model performance</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#the-positional-encoding-challenge">The Positional Encoding Challenge</a> </div> <ul> <li> <a href="#why-earth-observation-needs-4d-encoding">Why Earth Observation Needs 4D Encoding</a> </li> <li> <a href="#limitations-of-existing-approaches">Limitations of Existing Approaches</a> </li> </ul> <div> <a href="#earth4d-architecture">Earth4D Architecture</a> </div> <ul> <li> <a href="#decomposed-spatiotemporal-representation">Decomposed Spatiotemporal Representation</a> </li> <li> <a href="#multi-resolution-hierarchy">Multi-Resolution Hierarchy</a> </li> <li> <a href="#hash-encoding-mechanics">Hash Encoding Mechanics</a> </li> </ul> <div> <a href="#the-hash-collision-problem">The Hash Collision Problem</a> </div> <ul> <li> <a href="#understanding-hash-collisions">Understanding Hash Collisions</a> </li> <li> <a href="#the-uint32-overflow-discovery">The uint32 Overflow Discovery</a> </li> <li> <a href="#collision-patterns-across-scales">Collision Patterns Across Scales</a> </li> </ul> <div> <a href="#learned-hash-probing">Learned Hash Probing</a> </div> <ul> <li> <a href="#how-learned-probing-works">How Learned Probing Works</a> </li> <li> <a href="#extreme-compression-results">Extreme Compression Results</a> </li> </ul> <div> <a href="#experimental-validation">Experimental Validation</a> </div> <ul> <li> <a href="#q1-matching-foundation-models-with-coordinates-alone">Q1: Matching Foundation Models with Coordinates Alone</a> </li> <li> <a href="#q2-can-we-achieve-99-parameter-reduction">Q2: Can We Achieve 99% Parameter Reduction?</a> </li> <li> <a href="#q3-rgb-reconstruction-from-elevation">Q3: RGB Reconstruction from Elevation</a> </li> </ul> <div> <a href="#implications-for-world-models">Implications for World Models</a> </div> <div> <a href="#limitations-and-future-work">Limitations and Future Work</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>World models—systems that learn to simulate and predict complex spatiotemporal dynamics—have emerged as a promising paradigm for understanding our planet. From climate forecasting to disaster response, these models need to process Earth observation data spanning vast spatial and temporal scales: from sub-meter satellite imagery to continental weather patterns, from sub-second events to century-long climate trends.</p> <p>At the heart of these models lies a fundamental challenge: <strong>how do we encode continuous space-time coordinates into learnable representations?</strong></p> <p>While transformers have revolutionized deep learning through positional encodings, existing approaches face severe limitations when applied to planetary-scale spatiotemporal data. Sinusoidal encodings lack the expressiveness for complex Earth phenomena. Learned embeddings require discretizing continuous space-time, losing resolution. And while 3D hash encoding<d-cite key="muller2022instant"></d-cite> revolutionized neural graphics, it doesn’t capture temporal dynamics.</p> <p>We present <strong>Earth4D</strong>, a production-ready 4D space-time positional encoder that extends multi-resolution hash encoding to four dimensions. Earth4D achieves remarkable results:</p> <ul> <li> <strong>Matches state-of-the-art foundation models</strong> using only (x,y,z,t) coordinates—no satellite imagery, weather data, or topography required</li> <li> <strong>99% parameter reduction</strong> (724M → 5M) with 4× training speedup while maintaining strong performance</li> <li> <strong>Planetary coverage</strong> from sub-meter to continental scale, with temporal precision from sub-second to centuries</li> </ul> <p>More importantly, Earth4D represents a shift in how we think about world models: rather than requiring massive multimodal pretraining, we can achieve competitive performance by learning rich spatiotemporal representations from coordinates alone.</p> <hr> <h2 id="the-positional-encoding-challenge">The Positional Encoding Challenge</h2> <h3 id="why-earth-observation-needs-4d-encoding">Why Earth Observation Needs 4D Encoding</h3> <p>Earth observation data presents unique challenges that distinguish it from typical deep learning tasks:</p> <p><strong>Continuous spatiotemporal coordinates</strong>: Unlike images with discrete pixel positions or text with sequential tokens, Earth data exists in continuous 4D space-time. A wildfire measurement at (37.77°N, -122.42°W, 50m elevation, March 23 2023 8:58 AM) needs precise encoding.</p> <p><strong>Extreme scale variation</strong>: We need to reason about phenomena at vastly different scales simultaneously—a climate model might track both individual storms (10-100km) and global circulation patterns (10,000km+). Temporal scales range from seconds (lightning strikes) to decades (climate change).</p> <p><strong>Planetary coverage</strong>: Unlike 3D graphics which operate in local coordinate frames, Earth observation requires global consistency. The same encoding must work for data from San Francisco, Sydney, and the South Pole.</p> <p><strong>Memory constraints</strong>: Processing planet-scale data quickly exhausts GPU memory. A naive grid representation at 1-meter resolution would require \(10^{18}\) grid cells globally—completely infeasible.</p> <h3 id="limitations-of-existing-approaches">Limitations of Existing Approaches</h3> <p><strong>Sinusoidal positional encodings</strong><d-cite key="vaswani2017attention"></d-cite>, while elegant for transformers, provide fixed basis functions that cannot adapt to complex spatiotemporal patterns in Earth data. They work well for sequential data but struggle with the intricate multi-scale structure of geospatial phenomena.</p> <p><strong>Learned embeddings</strong> require discretizing continuous coordinates. While Vision Transformers<d-cite key="dosovitskiy2021image"></d-cite> discretize images into patches, Earth observation data doesn’t have natural “patch” boundaries. Discretization either loses fine-grained resolution or creates memory-prohibitive lookup tables.</p> <p><strong>3D multi-resolution hash encoding</strong><d-cite key="muller2022instant"></d-cite> (InstantNGP) solved many of these problems for neural graphics by using hash tables to achieve memory-efficient multi-resolution encoding. However, it was designed for static 3D scenes. Earth observation fundamentally requires modeling <strong>how</strong> spatial patterns evolve <strong>over time</strong>—a 4D problem.</p> <p>Extending to 4D isn’t trivial. A naive 4D grid would explode memory requirements. Simply treating time as another spatial dimension loses the distinct characteristics of temporal dynamics (irreversibility, causality, different resolution requirements).</p> <p>What we need is a 4D encoding that:</p> <ol> <li>Handles continuous coordinates without discretization</li> <li>Scales efficiently to planetary coverage</li> <li>Captures multi-resolution structure in both space and time</li> <li>Adapts to data through learning</li> <li>Fits in GPU memory</li> </ol> <p>Earth4D addresses all of these requirements.</p> <hr> <h2 id="earth4d-architecture">Earth4D Architecture</h2> <h3 id="decomposed-spatiotemporal-representation">Decomposed Spatiotemporal Representation</h3> <p>Earth4D’s core innovation is a <strong>decomposed 4D encoding</strong> that separates spatial and temporal structure while capturing their interactions. Rather than a single 4D hash grid (which would be memory-prohibitive), Earth4D uses four 3D grids:</p> <ol> <li> <strong>XYZ Grid</strong>: Pure spatial encoding in Earth-Centered Earth-Fixed (ECEF) coordinates</li> <li> <strong>XYT Grid</strong>: Equatorial plane + time</li> <li> <strong>YZT Grid</strong>: 90°E meridian plane + time</li> <li> <strong>XZT Grid</strong>: Prime meridian plane + time</li> </ol> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/earth4d_architecture.png" alt="Earth4D Architecture" style="width: 100%;"> <figcaption>Earth4D decomposes 4D space-time into four 3D hash-encoded grids. Each grid operates at multiple resolution levels (coarse to fine), with features concatenated to form a 192D spatiotemporal embedding.</figcaption> </figure> <p>This decomposition is inspired by Grid4D<d-cite key="xu2024grid4d"></d-cite> but optimized for planetary scale. By using three orthogonal spatiotemporal projections, we capture how spatial patterns evolve over time from different perspectives. The XYZ grid provides pure spatial context, while XYT, YZT, and XZT encode temporal dynamics in complementary subspaces.</p> <p><strong>Why ECEF coordinates?</strong> We use Earth-Centered Earth-Fixed (ECEF) coordinates internally rather than latitude/longitude because:</p> <ul> <li>ECEF provides uniform spatial hashing globally (no polar singularities)</li> <li>Distances are Euclidean, making interpolation consistent</li> <li>3D Cartesian coordinates align naturally with hash encoding</li> </ul> <p>Input coordinates (latitude, longitude, elevation, time) are automatically converted to ECEF and normalized to \([-1, 1]\).</p> <h3 id="multi-resolution-hierarchy">Multi-Resolution Hierarchy</h3> <p>Each of the four grids operates at multiple resolution levels simultaneously. This multi-resolution structure is crucial for capturing both local details and global patterns.</p> <p>At level \(L\), the grid resolution is:</p> \[r_L = b \cdot g^L\] <p>where \(b\) is the base resolution (typically 32) and \(g\) is the growth factor (typically \(\sqrt{2}\)). With 24 levels (default configuration), spatial resolution ranges from:</p> <ul> <li> <strong>Level 1</strong>: 398.2 km/cell (continental scale)</li> <li> <strong>Level 12</strong>: 194.4 m/cell (city scale)</li> <li> <strong>Level 24</strong>: 4.75 cm/cell (sub-meter precision)</li> </ul> <p>Temporal resolution similarly spans from years to sub-second precision.</p> <h3 id="hash-encoding-mechanics">Hash Encoding Mechanics</h3> <p>At each resolution level \(L\), we map continuous coordinates to discrete grid positions, then hash them to a fixed-size lookup table:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Grid position: pos_grid = floor(coordinate × resolution_L)
2. Hash index:
   if grid_size ≤ hashmap_size:
       index = pos_grid.x + pos_grid.y × stride_y + pos_grid.z × stride_z
   else:
       index = hash(pos_grid) mod hashmap_size
3. Interpolate: trilinear interpolation of 8 corner features
</code></pre></div></div> <p>The hash function uses XOR with large primes for mixing:</p> \[\text{hash}(\mathbf{p}) = \bigoplus_{d=1}^{D} p_d \cdot \pi_d \pmod{T}\] <p>where \(\mathbf{p}\) is the grid position, \(\pi_d\) are large primes (2654435761, 805459861, …), and \(T\) is the hash table size.</p> <p>Coarse levels (small grid size) use direct indexing with no collisions. Fine levels (large grid size) use hashing, which introduces collisions but saves massive memory.</p> <p><strong>Smoothstep interpolation</strong> \((S(t) = 3t^2 - 2t^3)\) provides C¹ continuous gradients, better than linear interpolation for smooth Earth phenomena like temperature fields or elevation gradients.</p> <p>The final output concatenates features from all grids and levels:</p> <ul> <li>4 grids × 24 levels × 2 features = <strong>192D embedding</strong> per (x,y,z,t) coordinate</li> </ul> <p>This entire process runs on GPU via custom CUDA kernels, enabling <strong>massively parallel encoding</strong> of millions of coordinates simultaneously.</p> <hr> <h2 id="the-hash-collision-problem">The Hash Collision Problem</h2> <h3 id="understanding-hash-collisions">Understanding Hash Collisions</h3> <p>Hash encoding’s memory efficiency comes with a tradeoff: <strong>hash collisions</strong>. When the grid size exceeds the hash table size, multiple different spatial positions can map to the same hash index.</p> <p>For example, with a hash table size of \(2^{22}\) (4 million entries) and level 24 grid resolution of \(2^{28}\) cells, only 1 in 64 grid cells gets a unique hash entry. The other 63 collide.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/collision_heatmap.png" alt="Collision Rate Analysis" style="width: 100%;"> <figcaption>Hash collision rates across resolution levels for different data distributions. Fine levels (high resolution) show expected 2-4% collision rates, except for power-of-2 artifacts at level 23.</figcaption> </figure> <p><strong>Are collisions bad?</strong> Not necessarily. The hash encoding literature<d-cite key="muller2022instant"></d-cite> shows that downstream networks (MLPs) can learn to disambiguate collisions when they’re relatively rare. The hash function’s randomness actually acts as a form of regularization.</p> <p>However, <strong>catastrophic collision patterns</strong> destroy information. If all temporal variations at a location map to the same index, we lose the ability to model temporal dynamics.</p> <h3 id="the-uint32-overflow-discovery">The uint32 Overflow Discovery</h3> <p>During development, we discovered bizarre collision patterns in temporal grids:</p> <ul> <li> <strong>Level 8</strong>: 100% collision rate (only ~978 unique indices for 41,261 coordinates)</li> <li> <strong>Levels 13-19</strong>: 99.9% collision rate (all coordinates with same spatial position but different times mapped identically)</li> </ul> <p>This violated the expected monotonic decrease in collisions as resolution increases. Something was fundamentally broken.</p> <p>After extensive debugging, we discovered a <strong>critical integer overflow bug</strong> in the CUDA kernel:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// BUGGY CODE</span>
<span class="kt">uint32_t</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">;</span> <span class="n">d</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">index</span> <span class="o">+=</span> <span class="n">pos_grid</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride</span><span class="p">;</span>
    <span class="n">stride</span> <span class="o">*=</span> <span class="n">resolution</span><span class="p">[</span><span class="n">d</span><span class="p">];</span>  <span class="c1">// OVERFLOW!</span>
<span class="p">}</span>
</code></pre></div></div> <p>At level 8 with resolution 2048 per dimension:</p> <ul> <li>After processing first two dimensions: <code class="language-plaintext highlighter-rouge">stride = 2048 × 2048 = 4,194,304</code> </li> <li>Next multiplication: <code class="language-plaintext highlighter-rouge">4,194,304 × 2048 = 8,589,934,592</code> </li> <li><strong>This overflows uint32 (max 4,294,967,295) and wraps to 0!</strong></li> </ul> <p>When stride became 0, the temporal dimension contributed nothing to the hash index. All temporal variation was lost.</p> <p><strong>The fix</strong> was simple but critical—use 64-bit arithmetic for intermediate calculations:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// FIXED CODE</span>
<span class="kt">uint64_t</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// Prevents overflow</span>
</code></pre></div></div> <p>After this fix:</p> <ul> <li>Level 8: 100% → <strong>40.5%</strong> collision rate</li> <li>Level 13: 99.9% → <strong>2.4%</strong> collision rate</li> <li>Level 19: 99.9% → <strong>2.1%</strong> collision rate</li> </ul> <p>This bug hunt revealed an important lesson: <strong>subtle integer overflow can catastrophically corrupt spatiotemporal encodings</strong>. The bug only manifested at specific resolution/hash-table size combinations, making it nearly invisible without careful analysis.</p> <h3 id="collision-patterns-across-scales">Collision Patterns Across Scales</h3> <p>Even with the overflow bug fixed, hash collisions are inherent to memory-efficient encoding. We profiled collision rates across 10 different spatiotemporal data distributions:</p> <ol> <li> <strong>Uniform Random</strong>: Global Earth surface sampling</li> <li> <strong>Continental Sparse</strong>: Sparse coverage of North America</li> <li> <strong>City-Scale Cluster</strong>: 10km × 10km dense sampling</li> <li> <strong>Building-Scale</strong>: Single 10m × 10m area over time</li> <li> <strong>Time Series</strong>: Fixed locations sampled repeatedly over time</li> </ol> <p>Results showed collision rates ranging from 0% (coarse levels) to 2-4% (fine levels), with expected power-of-2 artifacts at level 23 (67M grid cells, 4M hash entries = exact 16× ratio).</p> <p>While 2-4% collision rate is acceptable for many applications, we wanted to push further: <strong>Can we reduce hash table size even more while maintaining quality?</strong></p> <p>This led us to learned hash probing.</p> <hr> <h2 id="learned-hash-probing">Learned Hash Probing</h2> <h3 id="how-learned-probing-works">How Learned Probing Works</h3> <p>Learned hash probing<d-cite key="takikawa2023compact"></d-cite> is a technique that learns to resolve hash collisions intelligently. Instead of a single hash function, we use <strong>dual hashing with learned offsets</strong>:</p> \[\text{index} = N_p \times h_1(\mathbf{x}) + \mathcal{D}_c[h_2(\mathbf{x})]\] <p>where:</p> <ul> <li>\(h_1(\mathbf{x})\): Primary hash function (coarse spatial localization)</li> <li>\(h_2(\mathbf{x})\): Secondary hash with different primes (decorrelated)</li> <li>\(\mathcal{D}_c\): Learned codebook of probe offsets</li> <li>\(N_p\): Probing range (typically 4, 8, or 16)</li> </ul> <p>The codebook \(\mathcal{D}_c\) is learned during training via gradients. Initially, probes are uniformly distributed across the \(N_p\) candidates. As training progresses, the model learns which probe indices minimize collisions for the specific data distribution.</p> <p><strong>Backward pass</strong> uses a straight-through estimator: during forward pass, we select a discrete probe index via argmax. During backward pass, we treat the discrete selection as differentiable by distributing gradients across all \(N_p\) candidates weighted by their softmax probabilities.</p> <p>This allows the model to learn <strong>data-adaptive collision resolution</strong> rather than relying on random hash functions alone.</p> <h3 id="extreme-compression-results">Extreme Compression Results</h3> <p>Learned hash probing enables dramatic parameter reduction. On the Globe-LFMC 2.0 benchmark<d-cite key="yebra2024globelfmc"></d-cite>:</p> <table> <thead> <tr> <th>Configuration</th> <th>Parameters</th> <th>GPU Memory</th> <th>Speed</th> <th>MAE</th> <th>R²</th> <th>vs Baseline</th> </tr> </thead> <tbody> <tr> <td> <strong>Baseline</strong> (\(2^{22}\) hash)</td> <td>724M</td> <td>12GB+</td> <td>1×</td> <td>16.6 pp</td> <td>0.582</td> <td>—</td> </tr> <tr> <td> <strong>Learned Probing</strong> (\(2^{22}\))</td> <td>724M</td> <td>12GB+</td> <td>1.7×</td> <td><strong>12.4 pp</strong></td> <td><strong>0.745</strong></td> <td>+28% R²</td> </tr> <tr> <td> <strong>Compressed</strong> (\(2^{14}\) hash)</td> <td><strong>5.1M</strong></td> <td><strong>850MB</strong></td> <td><strong>4×</strong></td> <td><strong>15.0 pp</strong></td> <td><strong>0.668</strong></td> <td>+14.7% R²</td> </tr> </tbody> </table> <p>The compressed configuration achieves:</p> <ul> <li> <strong>99.3% parameter reduction</strong> (724M → 5.1M)</li> <li> <strong>93% memory reduction</strong> (12GB → 850MB)</li> <li><strong>4× training speedup</strong></li> <li> <strong>Still outperforms baseline</strong> by 14.7% R²</li> </ul> <p>This is remarkable: by shrinking the hash table by \(256×\) (\(2^{22}\) → \(2^{14}\)) and adding learned probing, we maintain—and even improve—performance while fitting on edge devices.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/compression_tradeoff.png" alt="Compression Tradeoff" style="width: 90%;"> <figcaption>Performance vs parameter count. Learned hash probing (orange) enables 99% compression with minimal quality loss. The compressed 5.1M model outperforms the 724M baseline.</figcaption> </figure> <p><strong>Why does compression improve performance?</strong> We hypothesize that extreme compression acts as regularization. The forced sharing of hash table entries encourages the model to learn more generalizable spatiotemporal features rather than memorizing training locations. This is similar to how dropout or weight decay can improve generalization.</p> <hr> <h2 id="experimental-validation">Experimental Validation</h2> <p>We evaluate Earth4D through three research questions, each designed to test a specific capability required for world models.</p> <h3 id="q1-matching-foundation-models-with-coordinates-alone">Q1: Matching Foundation Models with Coordinates Alone</h3> <p><strong>Question</strong>: Can Earth4D achieve state-of-the-art performance using only spatiotemporal coordinates, without satellite imagery, weather data, or other multimodal inputs?</p> <p><strong>Dataset</strong>: Globe-LFMC 2.0<d-cite key="yebra2024globelfmc"></d-cite>, a global benchmark for predicting Live Fuel Moisture Content (LFMC)—the percentage of water in vegetation relative to dry weight. LFMC is critical for wildfire risk assessment.</p> <ul> <li>89,764 field measurements across diverse plant species, geographic regions, and temporal periods (2000-2023)</li> <li>Train/test split: 76,467 / 13,297 (official AI2 split for fair comparison)</li> </ul> <p><strong>Baseline</strong>: Galileo<d-cite key="tseng2025galileo"></d-cite>, a Vision Transformer (5.3M parameters) pre-trained by Allen Institute for AI on:</p> <ul> <li>Sentinel-2 optical imagery (10m resolution, 13 spectral bands)</li> <li>Sentinel-1 SAR (radar, cloud-penetrating)</li> <li>ERA-5 weather reanalysis (temperature, precipitation, etc.)</li> <li>TerraClimate soil moisture and climate data</li> <li>SRTM topography (elevation, slope, aspect)</li> <li>(x,y,z,t) coordinates and species type</li> </ul> <p><strong>Earth4D Architecture</strong>:</p> <ul> <li>Earth4D encodes (x,y,z,t) into 192D embeddings</li> <li>Concatenated with learnable species embedding (initialized randomly)</li> <li>MLP predicts LFMC percentage</li> </ul> <p><strong>Results</strong>:</p> <table> <thead> <tr> <th>Model</th> <th>Data Inputs</th> <th>MAE</th> <th>R²</th> </tr> </thead> <tbody> <tr> <td> <strong>Galileo</strong> (pretrained)</td> <td>Coordinates + Species + <strong>Multimodal Remote Sensing</strong> </td> <td>12.6 pp</td> <td>0.72</td> </tr> <tr> <td> <strong>Earth4D</strong> (learned probing)</td> <td><strong>Coordinates + Species only</strong></td> <td><strong>12.4 pp</strong></td> <td><strong>0.745</strong></td> </tr> </tbody> </table> <p>Earth4D <strong>surpasses the pretrained foundation model</strong> (12.4 vs 12.6 MAE, 0.745 vs 0.72 R²) using only coordinates and species embeddings. No satellite imagery. No weather data. No topography.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/lfmc_results.png" alt="LFMC Prediction Results" style="width: 100%;"> <figcaption><b>Top</b>: Distribution of absolute errors across 13,297 test samples (median 7.1pp). <b>Left</b>: Geographic error distribution shows low error in well-sampled regions. <b>Right</b>: Temporal predictions track ground truth LFMC across seasons (2017-2023).</figcaption> </figure> <p><strong>What’s happening?</strong> Earth4D learns that certain spatiotemporal coordinates correlate with LFMC patterns:</p> <ul> <li> <strong>Spatial</strong>: Coastal California (37°N, -122°W, low elevation) tends toward high LFMC in winter</li> <li> <strong>Temporal</strong>: Summer months (June-August) show lower LFMC across most species</li> <li> <strong>Elevation</strong>: Higher elevations retain moisture longer</li> <li> <strong>Interactions</strong>: Spatial patterns shift with seasons (spatiotemporal coupling)</li> </ul> <p>The 4D hash encoding captures these multi-scale correlations across 24 resolution levels. Coarse levels encode climate zones (Mediterranean, desert, temperate). Fine levels encode microclimate variations.</p> <p>Crucially, the <strong>species embedding provides botanical context</strong> that combines with spatiotemporal features. The model learns that Species A in Location X at Time T has different moisture dynamics than Species B at the same location and time.</p> <p>This result challenges a common assumption: <strong>multimodal pretraining isn’t always necessary if you have rich positional encodings and the right inductive biases</strong>.</p> <h3 id="q2-can-we-achieve-99-parameter-reduction">Q2: Can We Achieve 99% Parameter Reduction?</h3> <p><strong>Question</strong>: Does the extreme compression result (99% parameter reduction, 4× speedup) shown earlier generalize across different configurations?</p> <p><strong>Experiment</strong>: We systematically vary hash table size and probing range across the LFMC benchmark:</p> <table> <thead> <tr> <th>Hash Size</th> <th>Probing</th> <th>Parameters</th> <th>Speed</th> <th>MAE</th> <th>R²</th> <th>Memory</th> </tr> </thead> <tbody> <tr> <td>\(2^{22}\)</td> <td>Disabled</td> <td>724M</td> <td>1.0×</td> <td>16.6</td> <td>0.582</td> <td>12GB+</td> </tr> <tr> <td>\(2^{22}\)</td> <td>\(N_p=4\)</td> <td>724M</td> <td>1.5×</td> <td>13.2</td> <td>0.698</td> <td>12GB+</td> </tr> <tr> <td>\(2^{22}\)</td> <td>\(N_p=32\)</td> <td>724M</td> <td>1.7×</td> <td><strong>12.4</strong></td> <td><strong>0.745</strong></td> <td>12GB+</td> </tr> <tr> <td>\(2^{18}\)</td> <td>\(N_p=32\)</td> <td>45M</td> <td>1.8×</td> <td>13.8</td> <td>0.672</td> <td>1.5GB</td> </tr> <tr> <td>\(2^{14}\)</td> <td>\(N_p=32\)</td> <td><strong>5.1M</strong></td> <td><strong>4.0×</strong></td> <td>15.0</td> <td>0.668</td> <td>850MB</td> </tr> </tbody> </table> <p><strong>Key findings</strong>:</p> <ol> <li> <strong>Learned probing consistently improves performance</strong> even at full hash table size (16.6 → 12.4 MAE)</li> <li> <strong>Larger probing range (\(N_p\)) improves quality</strong> but adds training overhead</li> <li> <strong>Sweet spot: \(2^{18}\) hash + \(N_p=32\)</strong> balances quality and efficiency (93.8% reduction, strong performance)</li> <li> <strong>Extreme compression (\(2^{14}\)) remains viable</strong> for edge deployment</li> </ol> <p>The 99% reduction result is robust across multiple trials and random seeds. The key enabler is learned probing’s ability to adaptively resolve collisions based on data distribution.</p> <h3 id="q3-rgb-reconstruction-from-elevation">Q3: RGB Reconstruction from Elevation</h3> <p><strong>Question</strong>: Can Earth4D learn to infer RGB pixel values from (x,y,z,t) coordinates alone?</p> <p>This tests a different capability: <strong>pure spatiotemporal function approximation</strong> without any auxiliary labels (like species type in LFMC).</p> <p><strong>Dataset</strong>: 5.8M coordinate-color pairs from Houston coastal wetlands:</p> <ul> <li> <strong>Input</strong>: USGS 3DEP LiDAR elevation (x,y,z in ECEF, t = acquisition date)</li> <li> <strong>Target</strong>: USDA NAIP RGB imagery (R,G,B values at corresponding location/time)</li> </ul> <p>The objective is \((x,y,z,t) \rightarrow (r,g,b)\): given only coordinates, predict the RGB color.</p> <p><strong>Architecture</strong>: Earth4D (192D) → MLP (3 hidden layers, 128 units) → RGB (3 channels)</p> <p><strong>Results</strong>:</p> <table> <thead> <tr> <th>Configuration</th> <th>Validation Loss</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>Baseline (no probing)</td> <td>0.0847</td> <td>—</td> </tr> <tr> <td>Learned Probing (\(N_p=32\))</td> <td><strong>0.0694</strong></td> <td><strong>-18%</strong></td> </tr> </tbody> </table> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/rgb_reconstruction.png" alt="RGB Reconstruction" style="width: 100%;"> <figcaption>RGB reconstruction from LiDAR elevation in Houston wetlands (2018). <b>Left to right</b>: LiDAR height, ground truth RGB, baseline reconstruction, learned probing reconstruction (18% lower loss).</figcaption> </figure> <p>The model learns complex correlations:</p> <ul> <li> <strong>Water bodies</strong> (low elevation, flat) → blue/green hues</li> <li> <strong>Vegetation</strong> (moderate elevation, rough terrain) → green</li> <li> <strong>Urban areas</strong> (high elevation variance) → gray/brown</li> <li> <strong>Coastal transitions</strong> (elevation gradients) → color gradients</li> </ul> <p>Learned probing significantly improves reconstruction quality, especially in fine-detail regions like coastline boundaries and vegetation patches.</p> <p>This experiment demonstrates Earth4D’s ability to capture <strong>implicit spatiotemporal functions</strong> that generalize across diverse phenomena—not just LFMC prediction, but any function of (x,y,z,t).</p> <hr> <h2 id="implications-for-world-models">Implications for World Models</h2> <p>Earth4D’s results suggest several important implications for building world models of Earth observation data:</p> <h3 id="1-positional-encodings-as-first-class-features">1. Positional Encodings as First-Class Features</h3> <p>Traditional approaches treat positional information as auxiliary context for satellite imagery or sensor data. Earth4D flips this: <strong>positional encodings can be primary features</strong> that capture rich spatiotemporal patterns.</p> <p>This is analogous to how CLIP<d-cite key="radford2021learning"></d-cite> showed that text alone (without pixel-level annotations) could guide image understanding through contrastive learning. Here, coordinates alone (without multimodal data) can achieve competitive performance through expressive encoding.</p> <p>For world models, this means we can:</p> <ul> <li> <strong>Bootstrap</strong> from coordinate-only data when multimodal inputs are unavailable</li> <li> <strong>Reduce dependency</strong> on expensive satellite imagery or weather reanalysis</li> <li> <strong>Generalize</strong> to regions/times with sparse observational coverage</li> </ul> <h3 id="2-extreme-efficiency-enables-edge-deployment">2. Extreme Efficiency Enables Edge Deployment</h3> <p>The 99% parameter reduction (724M → 5M) makes Earth4D viable for edge deployment:</p> <ul> <li> <strong>Satellite onboard processing</strong>: Run Earth4D on satellite GPUs for real-time wildfire detection</li> <li> <strong>Mobile disaster response</strong>: Deploy on tablets/phones for field teams</li> <li> <strong>IoT sensor networks</strong>: Embed in low-power environmental monitoring stations</li> </ul> <p>This shifts world models from datacenter-scale to <strong>ubiquitous deployment</strong>, enabling real-time decision-making where it matters most.</p> <h3 id="3-learned-probing-as-universal-compression">3. Learned Probing as Universal Compression</h3> <p>Learned hash probing isn’t specific to Earth observation—it’s a <strong>general technique for compressing hash-based encodings</strong>. Applications include:</p> <ul> <li>Neural radiance fields (NeRF) for 3D reconstruction</li> <li>Implicit neural representations for any spatiotemporal data</li> <li>Memory-efficient transformers with positional embeddings</li> </ul> <p>The key insight: <strong>let the model learn to resolve collisions</strong> rather than sizing hash tables conservatively.</p> <h3 id="4-downstream-task-agnostic">4. Downstream Task Agnostic</h3> <p>Earth4D produces a 192D embedding per (x,y,z,t) coordinate. This embedding can feed into:</p> <ul> <li> <strong>Classification</strong>: Crop type, land cover, disaster detection</li> <li> <strong>Regression</strong>: Temperature, precipitation, soil moisture</li> <li> <strong>Segmentation</strong>: Flood extent, deforestation boundaries</li> <li> <strong>Generation</strong>: Synthesizing satellite imagery from coordinates</li> <li> <strong>Forecasting</strong>: Predicting future states from current embeddings</li> </ul> <p>By separating the positional encoder from task-specific heads, we enable <strong>transfer learning</strong> across Earth observation tasks. Pretrain Earth4D on one task (LFMC), then fine-tune on another (crop yield), reusing the spatiotemporal representations.</p> <h3 id="5-foundation-for-multimodal-world-models">5. Foundation for Multimodal World Models</h3> <p>While Earth4D succeeds with coordinates alone, it’s designed for <strong>fusion with multimodal encoders</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Satellite Image] → Vision Encoder → 512D
[Weather Data]    → Time Series Enc → 256D
[(x,y,z,t)]       → Earth4D         → 192D
────────────────────────────────────────────
Concatenate       → Transformer     → Predictions
</code></pre></div></div> <p>The 4D positional embedding provides <strong>spatiotemporal grounding</strong> for other modalities. An image patch at (lat, lon) gets enriched with Earth4D’s multi-resolution features encoding its geospatial context.</p> <p>This mirrors how language models use positional encodings—not as replacements for tokens, but as essential context that enables attention mechanisms to reason about structure.</p> <hr> <h2 id="limitations-and-future-work">Limitations and Future Work</h2> <p>While Earth4D demonstrates strong performance, several limitations and open questions remain:</p> <h3 id="power-of-2-collision-artifacts">Power-of-2 Collision Artifacts</h3> <p>At level 23 (resolution \(2^{26}\), hash table \(2^{22}\)), collision rate jumps to 3.8% due to exact power-of-2 ratio. This creates periodic artifacts in hash distribution.</p> <p><strong>Mitigation</strong>: Use non-power-of-2 hash table sizes (large primes) or increase hash capacity. However, power-of-2 sizes align with GPU memory boundaries and enable bitwise optimizations.</p> <h3 id="hyperparameter-sensitivity">Hyperparameter Sensitivity</h3> <p>Earth4D has several hyperparameters:</p> <ul> <li>Number of resolution levels (default 24)</li> <li>Hash table size per grid (\(2^{14}\) to \(2^{22}\))</li> <li>Probing range \(N_p\) (2, 4, 8, 16, 32)</li> <li>Codebook size \(N_c\) (512 to 4096)</li> </ul> <p>While we provide reasonable defaults, <strong>optimal settings vary by task</strong>. Automated hyperparameter search (e.g., using validation loss) would improve usability.</p> <h3 id="learning-rate-tuning-for-probing">Learning Rate Tuning for Probing</h3> <p>Index logits gradients are 5-7 orders of magnitude smaller than embedding gradients (inherent to straight-through estimators). We recommend 100× higher learning rate for index logits:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span>
    <span class="p">{</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">:</span> <span class="n">encoder</span><span class="p">.</span><span class="n">embeddings</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">},</span>
    <span class="p">{</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">:</span> <span class="n">encoder</span><span class="p">.</span><span class="n">index_logits</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1e-1</span><span class="p">}</span>
<span class="p">])</span>
</code></pre></div></div> <p>This dual learning rate requirement adds complexity. <strong>Automatic gradient rescaling</strong> could simplify training.</p> <h3 id="global-vs-regional-tradeoffs">Global vs Regional Tradeoffs</h3> <p>Earth4D uses a single hash table for the entire planet. This is memory-efficient but treats all regions equally. Some regions (densely sampled urban areas) may benefit from finer-grained encoding than sparse regions (oceans, deserts).</p> <p><strong>Future work</strong>: Adaptive hash allocation based on data density. Allocate more hash capacity to high-information regions, less to homogeneous areas.</p> <h3 id="temporal-resolution-assumptions">Temporal Resolution Assumptions</h3> <p>Our experiments normalize time to \([0, 1]\) over the dataset’s temporal range. For applications spanning centuries (climate modeling), we may need explicit multi-scale temporal encoding (years, months, days, hours) similar to spatial multi-resolution.</p> <h3 id="interpretability">Interpretability</h3> <p>While Earth4D learns effective representations, understanding <strong>what</strong> it learns remains challenging. Visualization of hash table features could reveal:</p> <ul> <li>Which spatiotemporal patterns activate specific hash entries?</li> <li>How do features at different resolution levels specialize?</li> <li>Can we interpret learned probe offsets?</li> </ul> <p>Techniques from mechanistic interpretability<d-cite key="olah2020zoom"></d-cite> could shed light on Earth4D’s internal representations.</p> <hr> <h2 id="conclusion">Conclusion</h2> <p>We presented Earth4D, a production-ready 4D space-time positional encoder that achieves state-of-the-art performance on ecological forecasting while using only spatiotemporal coordinates. Through decomposed hash encoding and learned hash probing, Earth4D demonstrates:</p> <ol> <li> <strong>Matching foundation models</strong> pretrained on multimodal Earth observation data, using coordinates alone</li> <li> <strong>99% parameter reduction</strong> with maintained or improved performance</li> <li> <strong>Planetary-scale coverage</strong> from sub-meter to continental resolution</li> <li> <strong>4× training speedup</strong> enabling practical deployment</li> </ol> <p>These results challenge the assumption that world models require massive multimodal pretraining. Rich spatiotemporal representations, learned through 4D hash encoding, can capture complex Earth dynamics with remarkable efficiency.</p> <p>Earth4D is fully open source and ready for integration into world models:</p> <p><strong>GitHub</strong>: <a href="https://github.com/legel/deepearth" rel="external nofollow noopener" target="_blank">https://github.com/legel/deepearth</a></p> <p>As we build AI systems to understand and simulate our planet—for climate forecasting, disaster response, agricultural planning, and beyond—<strong>positional encoding matters</strong>. Earth4D provides a foundation for world models that is memory-efficient, expressive, and ready for production deployment.</p> <p>The future of world models may not require encoding everything about Earth. Perhaps we just need to encode Earth’s space-time structure effectively—and let the model discover the rest.</p> <hr> <h2 id="acknowledgments">Acknowledgments</h2> <p>We thank the Allen Institute for AI for releasing the Globe-LFMC 2.0 dataset and Galileo baseline. We thank NVIDIA for open-sourcing InstantNGP, which inspired Earth4D’s architecture. We thank the USGS 3DEP and USDA NAIP programs for providing public LiDAR and imagery data.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026-iclr-blogpost-earth4d/assets/bibliography/2026-04-27-earth4d-world-models.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/tracing-principles-behind-modern-diffusion-models/">Tracing the Principles Behind Modern Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/symbolic-connect/">Symbolism Outside, Connectionism Inside: The Trend of Fusing LLMs and Automatic Programs with Symbolic Intermediate Representations</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/speeding-up-rl/">Speeding up Training of Model-Free Reinforcement Learning :A Comparative Evaluation for Fast and Accurate Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/sac-massive-sim/">Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026-iclr-blogpost-earth4d/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-data.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>