<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> An Impossibility Trilemma for Data-Free Sampler Evaluation | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Neural samplers aim to learn to sample a target unnormalized energy potential. Sampler quality can be evaluated in a data-free manner, using only the model and the target potential, or in a data-driven manner, with additional data about the target distribution such as known modes, summary statistics, and reference MCMC samples. While data-driven eval is valuable, data-free eval has compelling conceptual advantages, raising the question of how well data-free eval could work. Here, we prove an impossibility trilemma for data-free sampler evaluation; we can only have two among i) mode-covering metric, ii) stable with finite variance, iii) universal ranking (dominance transitivity guarantee / model score does not depend on other models). This note surveys underexplored design space of data-free sampler eval metrics, and asks the community which eval properties we are willing to sacrifice in the face of the impossibility of satisfying all of them."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026-iclr-blogpost-earth4d/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/sampler-eval-trilemma/"> <script src="/2026-iclr-blogpost-earth4d/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/template.v2.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "An Impossibility Trilemma for Data-Free Sampler Evaluation",
            "description": "Neural samplers aim to learn to sample a target unnormalized energy potential. Sampler quality can be evaluated in a data-free manner, using only the model and the target potential, or in a data-driven manner, with additional data about the target distribution such as known modes, summary statistics, and reference MCMC samples. While data-driven eval is valuable, data-free eval has compelling conceptual advantages, raising the question of how well data-free eval could work. Here, we prove an impossibility trilemma for data-free sampler evaluation; we can only have two among i) mode-covering metric, ii) stable with finite variance, iii) universal ranking (dominance transitivity guarantee / model score does not depend on other models). This note surveys underexplored design space of data-free sampler eval metrics, and asks the community which eval properties we are willing to sacrifice in the face of the impossibility of satisfying all of them.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026-iclr-blogpost-earth4d/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/" rel="external nofollow noopener" target="_blank"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener" target="_blank">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>An Impossibility Trilemma for Data-Free Sampler Evaluation</h1> <p>Neural samplers aim to learn to sample a target unnormalized energy potential. Sampler quality can be evaluated in a data-free manner, using only the model and the target potential, or in a data-driven manner, with additional data about the target distribution such as known modes, summary statistics, and reference MCMC samples. While data-driven eval is valuable, data-free eval has compelling conceptual advantages, raising the question of how well data-free eval could work. Here, we prove an impossibility trilemma for data-free sampler evaluation; we can only have two among i) mode-covering metric, ii) stable with finite variance, iii) universal ranking (dominance transitivity guarantee / model score does not depend on other models). This note surveys underexplored design space of data-free sampler eval metrics, and asks the community which eval properties we are willing to sacrifice in the face of the impossibility of satisfying all of them.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#informal-overview-of-trilemma">Informal overview of trilemma</a> </div> <div> <a href="#trilemma">Trilemma</a> </div> <ul> <li> <a href="#characterizing-mode-covering-with-sensitivity-analysis">Characterizing mode-covering with sensitivity analysis</a> </li> <li> <a href="#proof-of-dilemma-for-single-model-data-free-eval">Proof of dilemma for single-model data-free eval</a> </li> <li> <a href="#pairwise-comparators">Pairwise comparators</a> </li> <li> <a href="#loss-of-transitivity">Loss of transitivity</a> </li> <li> <a href="#general-properties-of-pairwise-comparators">General properties of pairwise comparators</a> </li> </ul> <div> <a href="#limitations">Limitations</a> </div> <div> <a href="#discussion">Discussion</a> </div> <div> <a href="#code">Code</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Sampling from unnormalized energy potentials is a key problem in probabilistic inference, statistical physics, and molecular dynamics. In recent years, deep generative modeling approaches for sampling has been investigated in diffusion samplers<d-cite key="liu2025adjoint"></d-cite> as well as generative flow networks<d-cite key="towardsunderstandinggflownets"></d-cite> and normalizing flows<d-cite key="schopmans2025temperatureannealed"></d-cite>. These models aim to learn a generative model distribution $q_\theta(x)$ that approximates a known unnormalized density. For instance, in molecular dynamics, the energy potential $U(x)$ of molecular conformations is known analytically in closed-form, and we aim to sample the Boltzmann distribution $p(x) \propto \exp(-U(x))$. Mode discovery, possibly by generalizing from known modes to efficiently discover new modes, is a critical aspect to evaluate in neural samplers, because traditional MCMC methods can mix poorly for rugged, high dimensional densities.</p> <p>There are two main approaches for evaluating how well a sampler matches the target distribution: data-free, and data-driven. In data-free sampler eval, we only have the target unnormalized density, and consider metrics like KL divergence, kernelized maximum mean discrepancy, and Stein discrepancy.</p> <p>In data-driven sampler eval, we have access to additional data about $p$ beyond its unnormalized density. For example, synthetic eval settings can be constructed with a known number of modes and locations, which can be used to count how many modes neural samplers recover. In molecular dynamics, molecules like alanine dipeptide (~20 atoms) and chignolin (~200 atoms) are deeply understood with known modes. For more complex molecules, experimental observables such as protein folding stability can be used for evaluation<d-cite key="doi:10.1126/science.adv9817"></d-cite>, but this can conflate sampler evaluation with misalignments between the target potential’s model of reality and actual reality. For example, we might sample the target potential perfectly, but still fit observables poorly, because the target potential is imperfect with respect to reality. In other cases, for instance molecules that are less well understood, samplers may be evaluated using reference MCMC samples used as a “gold standard”. However, it can be unclear how accurate these reference MCMC samples are, especially for target densities that are highly challenging to sample. In the most challenging sampling problems with no prior knowledge, data-driven eval faces a “catch-22” situation where we need trustworthy samples to evaluate whether our samples are trustworthy, making it difficult to self-bootstrap off the ground.</p> <p>While data-driven eval is valuable, the upsides of data-free sampler eval are appealing. In an ideal world where data-free sampler eval worked perfectly, research on neural samplers could be performed in gym-like environments with little overhead for supporting a huge diversity of target potentials, like with virtual environments or video games in reinforcement learning research. These considerations motivate asking how well data-free sampler eval could work.</p> <p>Here, we prove an impossibility trilemma for data-free sampler evaluation; we can only have two among: i) mode-covering metric, ii) stable with finite variance, iii) allows sampler ranking without cyclic dominance (disallows A&gt;B&gt;C&gt;A).</p> <h2 id="informal-overview-of-trilemma">Informal overview of trilemma</h2> <p>Data-free sampler evaluation is challenging because we have samples only from the model, and not from the target distribution (otherwise the sampling problem is solved). With model samples and likelihoods, we can stably estimate the reverse KL \(\mathbb{E}_{q}[\log(q/p)]\), but this is mode-seeking – the reverse KL strongly rewards $q$ matching \(p\) among model samples, and does not strongly penalize missing modes in \(p\). If we hill-climb the reverse KL as an evaluation metric, we would generally reward samplers that fit a subset of modes very well, even if they are missing other important target modes, over samplers that discovered more target modes. This means the reverse KL is not a very useful sampler evaluation metric because it ignores the problem of mode discovery.</p> <p>The forward KL \(\mathbb{E}_{p}[\log(p/q)]\) is mode-covering: it strongly rewards the model for covering modes of \(p\), making it ideal for evaluating mode discovery. Unfortunately, it is unstable to estimate. With access only to model samples, we require importance reweighting to estimate it as \(\mathbb{E}_{q}[(p/q) \log(p/q)]\). This theoretically does not have bounded, finite variance, and in practice is prohibitively high variance in high-dimensional settings of interest to be a useful evaluation metric.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/rkl-fkl-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/rkl-fkl-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/rkl-fkl-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/rkl-fkl.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Reverse KL is mode-seeking, while forward KL is mode-covering. Axes depict mean and std parameters for a Gaussian. Values plot discrepancy to a two-Gaussian mixture with mean, std depicted with the red x's. </div> <p>The contrast between the reverse KL and forward KL introduces the tension between items i) mode-covering metric, and ii) stable with finite variance, in our trilemma. By thinking beyond single-model eval metrics to pairwise comparators, which evaluate whether one model is better than the other, we can design metrics that are stable and mode-covering (among the support of both models). This pairwise comparator could thus score if a sampler $q_1$ is more mode-covering than $q_2$ head-to-head, while ignoring target modes that are unseen by both samplers. Unfortunately, we will show that such pairwise comparators introduce the third element of the trilemma: they lose universal ranking, which means they can introduce dominance cycles, and/or do not have pool independence.</p> <h2 id="trilemma">Trilemma</h2> <p>In this section, we provide a more precise characterization of the trilemma. First, we set up a definition of a “mode-covering” metric via sensitivity analysis. We then offer a short proof of the aforementioned dilemma between mode-covering and importance weights for single-model evaluation metrics. We then consider pairwise model comparison evaluation metrics, which can achieve both mode-covering and stability, but at the cost of the third item of the trilemma.</p> <p>Technical note: Our introduced setting assumes access only to the unnormalized density of $p$. However, in the remainder of this note, we focus on comparing samplers for a fixed target distribution, where we can safely ignore the unknown normalizing constant $Z$. Thus, for notation simplicity, we work directly with $p$ instead of introducing a different symbol for its unnormalized density.</p> <h3 id="characterizing-mode-covering-with-sensitivity-analysis">Characterizing mode-covering with sensitivity analysis</h3> <p>What does it mean for a metric to be mode-covering or mode-seeking? One natural approach is to consider how much the metric changes when the model likelihood shrinks to zero at a target mode. A small change shows the metric does not penalize a model for dropping modes – less mode-covering – while a large change represents more mode-covering behavior.</p> <p>To quantify how much a metric changes when the model likelihood shrinks to zero at a target mode, we can take its derivative with respect to $q(x)$, and study its form as $q(x) \to 0$ at some $x$ that is a target mode (i.e., with $p(x) &gt; 0$).</p> <p>For the Forward KL, at a given $x$:</p> \[-\frac{\partial p(x) \log\frac{p(x)}{q(x)} }{\partial q(x)} = \frac{p(x)}{q(x)} = O\left( \frac{p}{q} \right)\] <p>For the reverse KL, at a given $x$:</p> \[\begin{align} -\frac{\partial q(x) \log\frac{q(x)}{p(x)} }{\partial q(x)} &amp;= -\frac{\partial}{\partial q(x)} q(x) \log q(x) + \frac{\partial}{\partial q(x)} q(x) \log p(x) \\ &amp;= -\left( 1 \log q(x) + q(x) \frac{1}{q(x)} \right) + \log p(x) \\ &amp;= \log \frac{p(x)}{q(x)} + 1 \\ &amp;= O \left(\log \frac{p}{q} \right) \end{align}\] <p>We can see that the reverse KL is exponentially less sensitive to $q \to 0$ than the forward KL. This provides a more quantitative way to characterize that the forward KL is more mode-seeking. For this work, we will operate with this definition:</p> <p><strong>Definition</strong>: A metric is mode-covering if its partial derivative with respect to $q(x)$ is $O(p/q)$.</p> <p>As an aside, we can apply the same analysis to Stein discrepancy, and find that the partial derivative is $O(|\nabla_x \log p(x) |)$, which is constant in terms of $q$. Thus, the Stein discrepancy is even less mode-seeking than the reverse KL, which we can visualize experimentally.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/stein_discrepancy_experiment-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/stein_discrepancy_experiment-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/stein_discrepancy_experiment-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/stein_discrepancy_experiment.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Stein discrepancy vs. reverse and forward KL. Axes depict mean and std parameters for a Gaussian. Values plot discrepancy to a two-Gaussian mixture with mean, std depicted with the red x's. </div> <h3 id="proof-of-dilemma-for-single-model-data-free-eval">Proof of dilemma for single-model data-free eval</h3> <p>First, let’s focus on evaluation metrics that consider one model at a time. These can be written $\mathcal{D}(p, q)$, in contrast to pairwise comparators $\mathcal{D}(p, q_1, q_2)$ which we will consider later.</p> <p><strong>Lemma</strong>: Let the evaluation metric be a function $\mathcal{D}(p, q)$ defined as an integral over the domain, for any integrand:</p> \[\mathcal{D}(p, q) = \int \phi(x, p(x), q(x)) ~dx\] <p>$\mathcal{D}(p, q)$ cannot satisfy both properties:</p> <ol> <li>Mode-covering</li> <li>No importance sampling, when estimating the metric as an expectation under $q$, meaning no importance weight factors like $p(x)/q(x)$.</li> </ol> <p><strong>Proof</strong>. The proof follows by understanding that the $O(p/q)$ term arises if and only if the integrand has a leading term $p \log (q)$, because:</p> \[\frac{\partial}{\partial q} p\log(q) = p \frac{1}{q} = O(p/q)\] <p>Importantly, the term must be $p \log q$. For example, $q p \log q$ does not work, because the $q$ cancels the desired $1/q$ term by the product rule:</p> \[\frac{\partial}{\partial q} q p \log(q) = q\frac{1}{q}p + p \log(1/q) = O(p\log(1/q))\] <p>When the integrand has a leading term proportional to $p \log q$, if we wish to estimate the integral as an expectation under $q$, we must incur importance weights $\frac{p}{q}$. This shows the two conditions are incompatible.</p> <h4 id="importance-weights-incur-unbounded-variance">Importance weights incur unbounded variance</h4> <p>Our estimator is an expectation under model samples. Its stability depends on its variance, which is governed by the second moment $\mathbb{E}_{x \sim q}[(…)^2]$. If the estimator contains importance weights, its variance depends on:</p> \[\mathbb{E}_{x \sim q} \left[ \left( \frac{p}{q} \right)^2 \right] = \int \frac{p^2}{q} ~dx\] <p>This integral diverges when $p$ has heavier tails than $q$, i.e., when there are missing modes, so the importance weights have unbounded variance.</p> <h3 id="pairwise-comparators">Pairwise comparators</h3> <p>We saw that achieving both stability and mode-covering is impossible for evaluation metrics that compare one model to the target. However, to drive progress in machine learning research, we don’t need a “global” measure of sampler quality; it can suffice instead to just have a relative, or local measure, of whether one sampler is better than the other. In this section, we’ll show that pairwise comparators can achieve both stability and mode-covering, but at the cost of other desirable attributes.</p> <p>A key challenge with sampler evalution is that with only model samples, it’s challenging to know what target modes we’ve missed. However, when we compare two samplers, it becomes easy to tell if one sampler missed modes that the other sampler found, by comparing samplers with the mixture distribution.</p> \[m(x) = 0.5 q_1(x) + 0.5q_2(x)\] <p>For stability, let’s focus on the set of points with non-vanishing probability under the mixture:</p> \[\Omega_\epsilon = \{ x : q_1(x) &gt; \epsilon ~~ \text{or} ~~ q_2(x) &gt; \epsilon \}\] <p>Consider this pairwise comparator $\mathcal{D}(p, q_1, q_2)$ that compares two models $q_1$ and $q_2$.</p> \[\int_{ \Omega_\epsilon } p(x) \log \frac{q_1(x)}{ q_2(x)} ~dx\] <p>which can be estimated from samples $x_i \sim m(x)$ as:</p> \[\sum_{i=1}^N \left[ \mathbb{1} \left( m(x_i) &gt; \epsilon \right) \frac{p(x_i)}{m(x_i)} \log \frac{q_1(x_i)}{q_2(x_i)} \right]\] <p>On the shared support $\Omega_\epsilon$, this estimate is related to the difference in forward KL. It is mode-covering in the mixture support, as the sensitivity of the integrand to a mode of $p$ found by $q_1$ is:</p> \[-\frac{\partial p(x) \log(q_1(x) / q_2(x)) }{\partial q_2(x)} = O\left( \frac{p}{q_2} \right)\] <p>Note that this pairwise comparator is blind to modes missed by both models, though this is a reasonable property in an evaluation metric. Further, by limiting importance weights to $\Omega_\epsilon$ such that $m(x_i) &gt; \epsilon/2$, the importance weights are capped and do not explode to infinity. The estimator thus has bounded variance, improving the stability of this metric.</p> <p>Finally, we note that this pairwise comparator is presented primarily for exposition, and should not be considered a bullet proof proposal, until its properties such as being proper (optimal) for a scaled version of $p$<d-footnote>For any fixed $\Omega$ (determined by the choice of two models), the optimal distribution scored by the pairwise comparator is a scaled version of $p$ on $\Omega$ with zero density outside of $\Omega$.</d-footnote>, and its computation on un-normalized truncated model densities<d-footnote>Denote $\tilde{p}, \tilde{q_1}, \tilde{q_2}$ as the normalized densities on $\Omega$. Then, the true forward KL difference on the restricted set $KL(\tilde{p}\|\tilde{q_1}) - KL(\tilde{p}\|\tilde{q_2}) = \frac{1}{Z_p} \mathcal{D}(p, q_1, q_2) + \log(\frac{Z_{q_1}}{Z_{q_2}})$, where $Z_p = \int_{\Omega} p(x)~dx$ and similarly for $Z_{q_1}, Z_{q_2}$. Overall, the pairwise comparator favors models that put more total probability mass into $\Omega$, which is a reasonable property.</d-footnote> are more deeply studied.</p> <h3 id="loss-of-transitivity">Loss of transitivity</h3> <p>Sounds great, right? This pairwise comparator is both stable and mode-covering. What is the cost of this? Our pairwise comparator is no longer decomposable into the difference of single model evaluation metrics: $\mathcal{D}(p, q_1, q_2) \neq h(p, q_1) - h(p, q_2)$. In this situation, we lose a guarantee on transitivity. There can exist sets of samplers where $A&gt;B&gt;C&gt;A$, forming a dominance cycle. This happens because the “evaluation set” $\Omega_\epsilon$ is dynamic and depends on the comparison participants.</p> <p><strong>Lemma</strong>: There exists a set of samplers that form a dominance cycle when scored by this pairwise comparator.</p> <p><strong>Proof</strong>. We provide a proof by construction. Consider a target $p$ with three equal Gaussian modes at -5, 0, and +5. We have three samplers A, B, C, each with the same shape. Sampler A places large mass on the mode -5, small mass on mode 0, and no mass on mode +5. Sampler B and C are like sampler A but rotated on the modes (see figure). Here, epsilon masks no mass and small mass.</p> <p>Comparing A to B, the union support is modes -5 and 0, with equal target $p$ mass on both, but A beats B on -5 more than B beats A on 0. Thus overall, A wins. Applying the same argument, B&gt;C, and C&gt;A. This yields A&gt;B&gt;C&gt;A, a dominance cycle. We confirmed this argument holds experimentally by simulation, and provide code in the appendix.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/cycle_plot-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/cycle_plot-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/cycle_plot-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-sampler-eval-trilemma/cycle_plot.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> A constructed example of a dominance cycle, with three samplers A&gt;B&gt;C&gt;A. </div> <p>We remark that losing the guarantee of transitivity does not mean that the pairwise comparator will commonly encounter dominance cycles. For realistic evaluation settings, dominance cycles are likely rare, or if they do occur due to “merry-go-round” mode discovery, this cause can be identified and understood to build better samplers. Furthermore, non-transitive pairwise comparisons can still be usefully ranked in a leaderboard via ELO scores<d-cite key="chiang2024chatbotarenaopenplatform"></d-cite>.</p> <h3 id="general-properties-of-pairwise-comparators">General properties of pairwise comparators</h3> <p>Three properties of a pairwise comparator:</p> <ul> <li>Transitivity guarantee (no dominance cycles)</li> <li>Completeness (all pairs are comparable)</li> <li>Pool independence (results do not depend on the identity of all opponents)</li> </ul> <p>are all jointly satisfied if and only if the pairwise comparator can be decomposed into single-model evaluation metrics: $\mathcal{D}(p, q_1, q_2) = h(p, q_1) - h(p, q_2)$. The connections between preference relations and utility functions has been studied in microeconomics Debreu’s theorems<d-cite key="Debreu2008-dd"></d-cite>, Luce’s Choice Axiom, and Bradley-Terry models<d-cite key="Bradley1952-tn"></d-cite>.</p> <p>In our example above, we studied a particular pairwise comparator where the evaluation set depended on the support of the mixture distribution, which violates the single-model decomposition, and thereby breaks the guarantee of transitivity, as well as pool independence (as the exact score depends on the opponent).</p> <p>For a leaderboard ranking of a set of models, we can improve transitivity among the model set by evaluating using the joint support of all models. Then, among models on the leaderboard, transitivity is retained. However, we still lack pool independence, so whether A&gt;B can flip when a new model C is added to the leaderboard.</p> <h2 id="limitations">Limitations</h2> <p>Our analysis has only considered single-model eval metrics expressible as integrals over the domain. While this family is fairly broad and such metrics are directly compatible with evaluation with iid model samples, it is possible that other families of single-model eval metrics can exist with improved properties.</p> <p>Our working definition of “mode-covering” considers point-wise partial derivatives $O(p/q)$, which matches that of the forward KL. It could be possible that more mathematically relaxed definitions of mode-covering could still be useful for scoring mode discovery, and could lead to sampler eval metrics with improved properties that may “bypass” the trilemma as proved here.</p> <h2 id="discussion">Discussion</h2> <p>In this work, we investigated the design space of data-free sampler evaluation metrics. We first showed that for single-model eval, there is an impossibility dilemma between stability and mode-covering. We then showed that by moving from single-model eval to pairwise comparators, we can achieve both stability and mode-covering, but lose universal ranking, which can manifest in losing some or all of: transitivity guarantee, completeness, and pool independence.</p> <p>In particular, we studied a specific pairwise comparator that estimates the forward KL difference on the support of the mixture distribution with non-vanishing probability from either model. We showed that this pairwise comparator loses the transitivity guarantee, by constructing an example target where three samplers beat each other in a rock-paper-scissors cycle. Nevertheless, we expect that such dominance cycles might be rare in practice. Furthermore, ELO rankings can be used to convert pairwise scores into leaderboard rankings, even if dominance cycles exist.</p> <p>We further discussed the possibility of a leaderboard that compares models on the joint support of all models submitted to the leaderboard. In this option, we ensure that transitivity holds for models in the leaderboard, but we still violate pool independence, wherein model rankings can flip when new models are added to the leaderboard.</p> <p>Evaluation is an important engine for driving research progress, and sampler evaluation is a particularly challenging area to set up evaluations. In this note, we proved the impossibility of simultaneously achieving many desirable properties for an eval metric, which are easily achieved, and perhaps taken for granted, in other subfields. For example in computer vision, FID is stable, relevant to image quality (the analogue to mode-covering for sampler eval), and produces leaderboards with universal ranking, guaranteeing transitivity, pool independence, and completeness.</p> <p>We hope that this note highlights underexplored aspects of the design space of data-free sampler evaluation methods, and may spur the community to think and discuss more about eval design, as well as desiderata on the best subset of properties, and agreement on which properties to sacrifice, for improved eval metrics to drive research progress.</p> <h2 id="code">Code</h2> <p>This code demonstrates three samplers with a dominance cycle using our pairwise comparator.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
</pre></td> <td class="code"><pre><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">class</span> <span class="nc">GaussianMixture</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">means</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">stds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">stds</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">means</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">stds</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">):</span>
            <span class="n">prob</span> <span class="o">+=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prob</span>
    
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
        <span class="c1"># Choose component indices
</span>        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
        <span class="c1"># Sample from selected components
</span>        <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">means</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">stds</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">samples</span>

<span class="k">def</span> <span class="nf">pairwise_snis_score</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span> <span class="n">q2</span><span class="p">,</span> <span class="n">target_p</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="n">epsilon_threshold</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Computes the Pairwise SNIS score: Score(q1) - Score(q2)
    Positive value means q1 is better than q2.
    </span><span class="sh">"""</span>
    <span class="c1"># 1. Sample from the mixture m = 0.5*q1 + 0.5*q2
</span>    <span class="c1"># We simulate this by sampling n/2 from q1 and n/2 from q2
</span>    <span class="n">n_half</span> <span class="o">=</span> <span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">samples_q1</span> <span class="o">=</span> <span class="n">q1</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">n_half</span><span class="p">)</span>
    <span class="n">samples_q2</span> <span class="o">=</span> <span class="n">q2</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">n_half</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">samples_q1</span><span class="p">,</span> <span class="n">samples_q2</span><span class="p">])</span>
    
    <span class="c1"># 2. Evaluate densities
</span>    <span class="n">prob_p</span> <span class="o">=</span> <span class="n">target_p</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">prob_q1</span> <span class="o">=</span> <span class="n">q1</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">prob_q2</span> <span class="o">=</span> <span class="n">q2</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># 3. Compute mixture density m(x)
</span>    <span class="n">prob_m</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">prob_q1</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">prob_q2</span>
    
    <span class="c1"># 4. Filter by Threshold (The "Blindness" condition)
</span>    <span class="c1"># Only keep samples where the mixture density is significant
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">prob_m</span> <span class="o">&gt;</span> <span class="n">epsilon_threshold</span>
    
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.0</span> <span class="c1"># No visible overlap
</span>        
    <span class="n">x_valid</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">prob_p_valid</span> <span class="o">=</span> <span class="n">prob_p</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">prob_m_valid</span> <span class="o">=</span> <span class="n">prob_m</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">prob_q1_valid</span> <span class="o">=</span> <span class="n">prob_q1</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">prob_q2_valid</span> <span class="o">=</span> <span class="n">prob_q2</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    
    <span class="c1"># 5. Compute Self-Normalized Importance Weights
</span>    <span class="c1"># w = p / m
</span>    <span class="n">log_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">prob_p_valid</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">prob_m_valid</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
    
    <span class="c1"># Normalize weights (SNIS step)
</span>    <span class="n">norm_weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
    
    <span class="c1"># 6. Compute Weighted Difference of Log Likelihoods
</span>    <span class="c1"># Delta = log(q1) - log(q2)
</span>    <span class="c1"># Add tiny constant to avoid log(0)
</span>    <span class="n">log_diff</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">prob_q1_valid</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">prob_q2_valid</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>
    
    <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">norm_weights</span> <span class="o">*</span> <span class="n">log_diff</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">score</span>

<span class="k">def</span> <span class="nf">run_simulation</span><span class="p">():</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="c1"># --- Configuration ---
</span>    <span class="c1"># Target P: 3 Equal Modes at -5, 0, 5
</span>    <span class="n">means</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
    <span class="n">std</span> <span class="o">=</span> <span class="mf">0.6</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="p">[</span><span class="n">std</span><span class="p">]</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">])</span>
    
    <span class="c1"># --- The "Leakage" Cycle Construction ---
</span>    <span class="c1"># We design the weights such that:
</span>    <span class="c1"># A dominates B on Mode 1.
</span>    <span class="c1"># B dominates A on Mode 2 (but less severely because A leaks there).
</span>    <span class="c1"># Mode 3 is "invisible" to the A vs B metric (below threshold).
</span>    
    <span class="c1"># Weights format: [Mode 1, Mode 2, Mode 3]
</span>    <span class="c1"># A: High on 1, Leaks on 2, Blind on 3
</span>    <span class="n">w_a</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.00</span><span class="p">]</span> 
    
    <span class="c1"># B: High on 2, Leaks on 3, Blind on 1 (Cyclic Shift)
</span>    <span class="n">w_b</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">]</span>
    
    <span class="c1"># C: High on 3, Leaks on 1, Blind on 2 (Cyclic Shift)
</span>    <span class="n">w_c</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">]</span>
    
    <span class="n">q_a</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="p">[</span><span class="n">std</span><span class="p">]</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">w_a</span><span class="p">)</span>
    <span class="n">q_b</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="p">[</span><span class="n">std</span><span class="p">]</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">w_b</span><span class="p">)</span>
    <span class="n">q_c</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="p">[</span><span class="n">std</span><span class="p">]</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">w_c</span><span class="p">)</span>
    
    <span class="c1"># --- Threshold Selection ---
</span>    <span class="c1"># Max density of a single gaussian ~0.66.
</span>    <span class="c1"># Mixture density at Main mode ~ 0.5 * 0.85 * 0.66 ≈ 0.28
</span>    <span class="c1"># Mixture density at Leak mode ~ 0.5 * (0.85 + 0.15) * 0.66 ≈ 0.33
</span>    <span class="c1"># Mixture density at Blind mode ~ 0.5 * (0.00 + 0.15) * 0.66 ≈ 0.05
</span>    <span class="c1"># We need a threshold roughly between 0.05 and 0.28 to hide the Blind mode.
</span>    <span class="c1"># Let's pick 0.1.
</span>    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">--- Running Validation (Threshold = </span><span class="si">{</span><span class="n">epsilon</span><span class="si">}</span><span class="s">) ---</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># --- Run Comparisons ---
</span>    <span class="c1"># 1. A vs B
</span>    <span class="n">score_ab</span> <span class="o">=</span> <span class="nf">pairwise_snis_score</span><span class="p">(</span><span class="n">q_a</span><span class="p">,</span> <span class="n">q_b</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">epsilon_threshold</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Match 1 (A vs B): Score = </span><span class="si">{</span><span class="n">score_ab</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">  =&gt; </span><span class="si">{</span><span class="sh">'</span><span class="s">A Wins</span><span class="sh">'</span> <span class="k">if</span> <span class="n">score_ab</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="sh">'</span><span class="s">B Wins</span><span class="sh">'</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># 2. B vs C
</span>    <span class="n">score_bc</span> <span class="o">=</span> <span class="nf">pairwise_snis_score</span><span class="p">(</span><span class="n">q_b</span><span class="p">,</span> <span class="n">q_c</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">epsilon_threshold</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Match 2 (B vs C): Score = </span><span class="si">{</span><span class="n">score_bc</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">  =&gt; </span><span class="si">{</span><span class="sh">'</span><span class="s">B Wins</span><span class="sh">'</span> <span class="k">if</span> <span class="n">score_bc</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="sh">'</span><span class="s">C Wins</span><span class="sh">'</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># 3. C vs A
</span>    <span class="n">score_ca</span> <span class="o">=</span> <span class="nf">pairwise_snis_score</span><span class="p">(</span><span class="n">q_c</span><span class="p">,</span> <span class="n">q_a</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">epsilon_threshold</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Match 3 (C vs A): Score = </span><span class="si">{</span><span class="n">score_ca</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">  =&gt; </span><span class="si">{</span><span class="sh">'</span><span class="s">C Wins</span><span class="sh">'</span> <span class="k">if</span> <span class="n">score_ca</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="sh">'</span><span class="s">A Wins</span><span class="sh">'</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">score_ab</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">score_bc</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">score_ca</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">RESULT: Strict Dominance Cycle Confirmed (A &gt; B &gt; C &gt; A)</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">RESULT: Cycle not found (Check parameters)</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># --- Visualization ---
</span>    <span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    
    <span class="c1"># Plot Target
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_plot</span><span class="p">),</span> <span class="sh">'</span><span class="s">k--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Target p</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    
    <span class="c1"># Plot Samplers
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">q_a</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_plot</span><span class="p">),</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Sampler A (High 1, Leak 2)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">q_b</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_plot</span><span class="p">),</span> <span class="sh">'</span><span class="s">g-</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Sampler B (High 2, Leak 3)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">q_c</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_plot</span><span class="p">),</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Sampler C (High 3, Leak 1)</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># Plot Threshold visualizer for A vs B
</span>    <span class="n">m_ab</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">q_a</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_plot</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">q_b</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_plot</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">m_ab</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Blind Spot (A vs B)</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="nf">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">:</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Threshold Epsilon</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">The Leakage Cycle Construction</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">leakage_cycle_plot.png</span><span class="sh">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Plot saved to </span><span class="sh">'</span><span class="s">leakage_cycle_plot.png</span><span class="sh">'"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">run_simulation</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026-iclr-blogpost-earth4d/assets/bibliography/2026-04-27-sampler-eval-trilemma.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/ppo-batch-size/">The Trade-off Between Parallel Environments and Steps in PPO</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/agent-evaluation/">A Hitchhiker's Guide to Agent Evaluation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026-iclr-blogpost-earth4d/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-data.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>