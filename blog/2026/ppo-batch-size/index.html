<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Trade-off Between Parallel Environments and Steps in PPO | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="This blog post explores batch size in PPO-what happens when we increase the number of parallel environments versus the number of rollout steps, while keeping the total samples per update fixed. We discuss how this affects bias and variance in gradient estimation."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026-iclr-blogpost-earth4d/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/ppo-batch-size/"> <script src="/2026-iclr-blogpost-earth4d/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/template.v2.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Trade-off Between Parallel Environments and Steps in PPO",
            "description": "This blog post explores batch size in PPO-what happens when we increase the number of parallel environments versus the number of rollout steps, while keeping the total samples per update fixed. We discuss how this affects bias and variance in gradient estimation.",
            "published": "November 13, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026-iclr-blogpost-earth4d/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/" rel="external nofollow noopener" target="_blank"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener" target="_blank">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Trade-off Between Parallel Environments and Steps in PPO</h1> <p>This blog post explores batch size in PPO-what happens when we increase the number of parallel environments versus the number of rollout steps, while keeping the total samples per update fixed. We discuss how this affects bias and variance in gradient estimation.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#clearing-up-the-terminology">Clearing up the Terminology</a> </div> <div> <a href="#data-distribution">Data Distribution</a> </div> <div> <a href="#bias-and-variance-in-policy-gradients">Bias and Variance in Policy Gradients</a> </div> <div> <a href="#sources-of-variance-in-rl">Sources of variance in RL</a> </div> <div> <a href="#using-mini-batches">Using Mini-batches</a> </div> <div> <a href="#deconstructing-the-gradient-and-its-variance">Deconstructing the Gradient and Its Variance</a> </div> <div> <a href="#unpacking-the-variance">Unpacking the Variance</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>In this post, we are going to explore a common dilemma we face when tuning PPO hyperparameters: constructing the batch size.</p> <p>It’s easy to think of batch size as just a single number, but in PPO, it is actually the product of two distinct levers we can pull:</p> <ul> <li>The number of parallel environments ($N$).</li> <li>The number of steps collected per environment ($T$).</li> </ul> <p>Does it matter if we reach a batch size of 2,048 by running 2 environments for 1,024 steps, or by running 1,024 environments for 2 steps?</p> <p>We will look at how this choice affects the bias and variance of our gradient estimation.</p> <p>We will keep the heavy equations to a minimum and rely on illustrations to build an intuition for what is happening under the hood.</p> <h2 id="clearing-up-the-terminology-defining-batch-vs-mini-batch">Clearing up the Terminology: Defining Batch vs. Mini-Batch</h2> <p>If you have ever looked at PPO implementations across different libraries, you might have noticed that “batch size” doesn’t always mean the same thing.</p> <p>To keep things clear in this post, here is the hierarchy we will use:</p> <ul> <li>Rollout Buffer (Total Batch Size): This is the full dataset collected before a policy update. It is the product of the number of parallel environments ($N$) and the number of steps collected per environment ($T$).</li> </ul> \[\text{Total Batch Size} = N \times T\] <ul> <li>Mini-Batch: This is the subset of the Rollout Buffer used for a single Gradient Descent step. We shuffle the Rollout Buffer and chop it into these smaller pieces to perform the updates.</li> </ul> <p><strong>Why is this confusing?</strong></p> <p>The confusion stems from the fact that popular libraries label these variables differently. For instance, Stable Baselines3 <d-cite key="stable-baselines3"></d-cite>, Dopamine <d-cite key="castro18dopamine"></d-cite>, OpenAI Baselines (PPO1) <d-cite key="baselines"></d-cite>, and Ray RLlib <d-cite key="liang2018rllib,liang2021rllib"></d-cite> often refer to the mini-batch variable simply as batch.</p> <p>This naming convention can lead to subtle but critical implementation errors. As pointed out by <d-cite key="shengyi2022the37implementation"></d-cite>:</p> <blockquote style="font-size:0.9em; margin:6px 0; padding-left:12px; border-left:3px solid #ccc;">Some common mis-implementations include (1) always using the whole batch for the update, and (2) implementing mini-batches by randomly fetching from the training data, which does not guarantee all training data points are fetched.</blockquote> <p>If we aren’t careful, we might think we are tuning the total experience collected ($N \times T$), when we are actually just changing how much data fits into a single GPU update step.</p> <h3 id="data-distribution">Data Distribution</h3> <p>In reinforcement learning, a trajectory $\tau = (s_0,a_0,s_1,a_1,\dots,s_T)$ is generated jointly by the policy and environment dynamics. Its probability under policy $\pi_\theta$ and transition matrix $P$ is:</p> \[P(\tau\mid \pi_\theta) = p(s_0)\, \prod_{t=0}^{T-1} \pi_\theta(a_t\mid s_t)\, P(s_{t+1}\mid s_t,a_t).\] <p>As the policy updates online, this trajectory distribution shifts between updates. PPO optimizes expected return using samples from the current distribution; near convergence, updates become small and the distribution stabilizes.</p> <p>Our focus here is not convergence, but how structuring a fixed total batch $NT$ (many short vs. few long rollouts) influences the variance of gradient estimates.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/data_distribution-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/data_distribution-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/data_distribution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/data_distribution.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="bias-and-variance-in-policy-gradients">Bias and Variance in Policy Gradients</h3> <p>Assumption for this section: gradients refer to those computed from the full collected batch per update ($N$ environments $\times$ $T$ steps), not mini-batches.</p> <p>We don’t see all possible trajectories—only a sample from the current distribution—so our policy gradient estimate is noisy, similar to stochastic gradients in supervised learning. Variance describes how much the gradient would change if we re-sampled another batch from the same distribution; larger effective sample sizes lower this variance.</p> <p>When variance is high, successive updates can point in very different directions, which makes learning unstable. Our goal here is to reason about one important source of variance: how rollout length $T$ (and thus temporal correlation) interacts with the number of environments $N$ when $NT$ is fixed.</p> <p>Bias can arise through how we estimate advantages and values from limited or skewed data (e.g., sparse rewards or highly stochastic transitions), nudging updates off the true gradient direction. Bias is intuitively a systematic drift of the mean gradients from the true gradients arising due to insufficient knowledge of the environment, introduced via incorrect estimates of the value function.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/batch_true_gradient-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/batch_true_gradient-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/batch_true_gradient-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/batch_true_gradient.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We established that noise in gradient updates makes training unstable. To control it, we must understand its sources.</p> <p>Unlike supervised learning, where noise usually just comes from drawing random samples from a fixed dataset, Reinforcement Learning adds other sources of inherent variability: environment dynamics and the tricky problem of credit assignment.</p> <h3 id="sources-of-variance-in-rl">Sources of variance in RL</h3> <p>We can think of our sampled gradient estimate ($G_B$) as the ideal gradient ($G_*$) plus these various sources of error:</p> \[G_B \approx G_* + \text{sampling noise} + \color{blue}{\text{trajectory variability}} + \color{brown}{\text{credit assignment noise}},\] <p>Sampling Noise: This is the familiar variance from using a finite batch size, common in all stochastic gradient methods.</p> <p>$\color{blue}{\text{Trajectory Variability}}$: This is noise inherent to the RL loop. It accounts for how much trajectories can differ due to environment stochasticity, sensitive dynamics (a small change in $s_0$ leads to a vastly different $s_T$), or policy stochasticity.</p> <ul> <li>Mitigation: Increasing the effective sample size—meaning increasing the number of independent trajectories ($N$).</li> </ul> <p>$\color{brown}{\text{Credit Assignment Noise}}$: This reflects the difficulty of figuring out which early actions caused a distant reward, especially with sparse or delayed feedback.</p> <ul> <li>Mitigation: Improving the advantage estimation, primarily by adjusting the Generalized Advantage Estimation (GAE) parameters or using longer rollouts ($T$).</li> </ul> <h3 id="practical-consideration-using-mini-batches">Practical Consideration: using Mini-batches</h3> <p>The full batch ($N\times T$) is often too large for a single gradient update due to memory limits or compute cost. To address this, we split the batch into mini-batches. Intuitively, let $G_B$ denote the gradient computed from the full batch. Let $G_{MB}$ denote the gradient from a mini-batch. In expectation, $G_{MB}$ updates parameters in approximately the same direction as $G_B$, as illustrated below.</p> <div style="width:55%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/mini_batch-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/mini_batch-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/mini_batch-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/mini_batch.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Mini-batches introduce additional variance because each mini-batch is a smaller sample of the full batch, yielding noisier gradient estimates. To isolate the effects we care about, in our analysis we compute gradients from the entire collected batch per update and avoid policy changes during collection.</p> <p>Under this setup, the probability ratio $r$ (defined below) is approximately 1 across the batch, since $\theta$ is not updated between samples. In practice, PPO uses clipping to enable multiple mini-batch updates while preventing the policy from changing too much between mini-batch steps.</p> <h3 id="deconstructing-the-gradient-and-its-variance">Deconstructing the Gradient and Its Variance</h3> <p>We’ve set the stage: our goal is to manage the bias (from short rollouts $T$) and the variance (from correlation, or high $T$). Now, let’s look at the PPO math to see exactly where the variance comes from.</p> <p><strong>PPO Gradient Equation</strong>: The core of PPO’s objective is to maximize the expected advantage, weighted by the probability ratio. The PPO objective (without the clipping mechanism initially) is:</p> \[L = \frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} \underbrace{\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}}_{r} \, A^{\theta_{\text{old}}}(s_t, a_t)\] <p>(To keep things simple, we assume using entire collected batch $\mathcal{D}$ for a single policy update, ignoring mini-batches and hence clipping).</p> <p>Where:</p> <ul> <li>$N \times T$ is our Total Batch Size.</li> <li>$r$ is the Probability Ratio, comparing the new policy $\pi_\theta$ to the old $\pi_{\theta_{\text{old}}}$.</li> <li>$A^{\theta_{\text{old}}}(s_t, a_t)$ is the Advantage Estimate calculated on the collected data.</li> </ul> <p>Taking the gradient of this loss function gives us our gradient estimate, $G_B$:</p> \[G_B = \nabla_\theta L = \frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} \nabla_\theta \left[ r \cdot A^{\theta_{\text{old}}}(s_t, a_t) \right]\] <p>Since the advantage $A$ is constant with respect to the new policy $\theta$, and knowing that $\nabla_\theta r = r \cdot \nabla_\theta \log \pi_\theta(a_t \mid s_t)$, the gradient simplifies to:</p> \[\nabla_\theta L = \frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} r \, A^{\theta_{\text{old}}}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(a_t \mid s_t)\] <p>For the initial full-batch gradient calculation, the old and new policies are very close, so $r \approx 1$. This gives us the simplest form of the gradient estimate:</p> \[G_B = \frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} \underbrace{ A^{\theta_{\text{old}}}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(a_t \mid s_t) }_{g_i}\] <p>This tells us something fundamental: Our policy gradient estimate $G_B$ is simply the mean of the individual policy gradients ($g_i$) calculated for every single step in our batch.</p> <h3 id="unpacking-the-variance">Unpacking the Variance</h3> <p>The variance of a mean is critical because it tells us how noisy $G_B$ is. High variance means we can’t trust the update.</p> <p>The total variance of our gradient estimate $G_B$ can be mathematically decomposed into two parts:</p> \[\text{Var}(G_B) = \text{Var}\Bigg(\frac{1}{N T} \sum_i g_i \Bigg) = \underbrace{\frac{1}{(N T)^2} \sum_i \text{Var}(g_i)}_{\text{Term 1: Individual Variance}} + \underbrace{\frac{1}{(N T)^2} \sum_{i \neq j} \text{Cov}(g_i, g_j)}_{\text{Term 2: Covariance (Correlation)}}\] <p>This decomposition holds the key to the $N$ vs. $T$ trade-off:</p> <ul> <li>Term 1: Individual Variance</li> </ul> <p>This is the noise coming from each step $g_i$. Since the sum is divided by $(NT)^2$, this term shrinks rapidly as the Total Batch Size ($NT$) increases. If every sample were independent, this is all we would have to worry about.</p> <ul> <li>Term 2: Covariance (The Correlation Problem)</li> </ul> <p>This is the term that makes RL different from Supervised Learning. $\text{Cov}(g_i, g_j)$ is not zero because $g_i$ and $g_{i+1}$ come from consecutive steps in the same environment. Consecutive states are highly related, so the policy gradients derived from them are also highly correlated.</p> <p>The Impact: When $T$ is large, we have many highly correlated steps in our batch, leading to a large positive covariance term. This effectively lowers the “effective sample size” of our batch. Even if $NT$ is large, if $T$ is too long, the covariance term can keep the overall $\text{Var}(G_B)$ high.</p> <p>The take-away is clear: To aggressively reduce variance, we must minimize the covariance term. This requires breaking the temporal correlation, which means we need more independent starting points—that is, increasing the number of parallel environments, $\mathbf{N}$.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026-iclr-blogpost-earth4d/assets/bibliography/2026-04-27-ppo-batch-size.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/agent-evaluation/">A Hitchhiker's Guide to Agent Evaluation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/witness-problem/">The Witness Problem in Multi-Agent Cooperation</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026-iclr-blogpost-earth4d/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-data.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>