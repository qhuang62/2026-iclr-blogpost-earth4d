<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond Black-Box Predictions: Neural Operators as a Bridge to Interpretable Governing Equations in Biology | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="This study introduces a hybrid computational framework that integrates Neural Operators with Sparse Identification of Nonlinear Dynamics (SINDy) to recover interpretable governing equations from sparse, partially observed biological data. Using an NF-κB signaling model, we demonstrate that Fourier Neural Operators effectively reconstruct hidden state trajectories from limited measurements, serving as a resolution-independent surrogate that enables the discovery of parsimonious dynamical laws. This operator-plus-symbolic paradigm offers a scalable workflow for extracting mechanistic insights from experimental readouts where classical inference methods typically fail."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026-iclr-blogpost-earth4d/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/neural-ops-in-biology/"> <script src="/2026-iclr-blogpost-earth4d/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/template.v2.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Beyond Black-Box Predictions: Neural Operators as a Bridge to Interpretable Governing Equations in Biology",
            "description": "This study introduces a hybrid computational framework that integrates Neural Operators with Sparse Identification of Nonlinear Dynamics (SINDy) to recover interpretable governing equations from sparse, partially observed biological data. Using an NF-κB signaling model, we demonstrate that Fourier Neural Operators effectively reconstruct hidden state trajectories from limited measurements, serving as a resolution-independent surrogate that enables the discovery of parsimonious dynamical laws. This operator-plus-symbolic paradigm offers a scalable workflow for extracting mechanistic insights from experimental readouts where classical inference methods typically fail.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "https://en.wikipedia.org/wiki/Albert_Einstein",
                "affiliations": [
                  {
                    "name": "IAS, Princeton",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026-iclr-blogpost-earth4d/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/" rel="external nofollow noopener" target="_blank"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener" target="_blank">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Beyond Black-Box Predictions: Neural Operators as a Bridge to Interpretable Governing Equations in Biology</h1> <p>This study introduces a hybrid computational framework that integrates Neural Operators with Sparse Identification of Nonlinear Dynamics (SINDy) to recover interpretable governing equations from sparse, partially observed biological data. Using an NF-κB signaling model, we demonstrate that Fourier Neural Operators effectively reconstruct hidden state trajectories from limited measurements, serving as a resolution-independent surrogate that enables the discovery of parsimonious dynamical laws. This operator-plus-symbolic paradigm offers a scalable workflow for extracting mechanistic insights from experimental readouts where classical inference methods typically fail.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#1-introduction">1. Introduction</a> </div> <div> <a href="#2-background-neural-operators-sparse-identification">2. Background: Neural Operators &amp; Sparse Identification</a> </div> <ul> <li> <a href="#2-1-neural-operators">2.1 Neural Operators</a> </li> <li> <a href="#2-2-sparse-identification-sindy">2.2 Sparse Identification (SINDy)</a> </li> </ul> <div> <a href="#3-biological-motivation-challenges">3. Biological Motivation &amp; Challenges</a> </div> <div> <a href="#4-case-study-simplified-nf-%CE%BAb-signaling-model">4. Case Study: Simplified NF-κB Signaling Model</a> </div> <div> <a href="#5-methods">5. Methods</a> </div> <div> <a href="#6-results-interpretation">6. Results &amp; Interpretation</a> </div> <div> <a href="#7-key-contributions-insights">7. Key Contributions &amp; Insights</a> </div> <div> <a href="#8-discussion-limitations">8. Discussion &amp; Limitations</a> </div> <div> <a href="#9-conclusion-outlook">9. Conclusion &amp; Outlook</a> </div> <div> <a href="#references">References</a> </div> </nav> </d-contents> <h2 id="1-introduction">1. Introduction</h2> <p>Biological systems such as gene regulatory networks, signaling cascades, and cellular feedback loops are often modeled as nonlinear dynamical systems: coupled ODEs or PDEs with feedback, cooperativity, and saturation <d-cite key="brunton2016discovering, dunn2022identification"></d-cite>. In principle, learning such equations directly from data would give compact, mechanistic models that extrapolate beyond the training set and suggest new experiments—a much stronger outcome than pure black-box prediction.</p> <p>In practice, however, biological data sit in an awkward regime for most existing tools. Experiments usually observe only a few readouts (for example, one or two fluorescent reporters) while many internal species remain hidden, measurements are noisy and sparsely sampled in time, and perturbation budgets are limited <d-cite key="hoffmann2002ikb, dunn2022identification"></d-cite>. Classical parameter inference assumes a correct mechanistic form and can become ill-posed in this setting, while generic deep networks, neural ODEs, and PINNs tend to learn accurate but opaque predictors that entangle biology-specific structure with generic function approximation, making it hard to extract interpretable “governing equations.”</p> <p>Two strands of recent work point toward an interesting different perspective:</p> <ul> <li> <strong>Neural Operators (NOs)</strong> learn maps between <em>function spaces</em>, such as initial conditions or input signals to full solution trajectories, with architectures that are approximately mesh / resolution independent <d-cite key="kovachki2023neural, li2021fourier"></d-cite>.</li> <li> <strong>Sparse Identification of Nonlinear Dynamics (SINDy)</strong> discovers parsimonious governing equations from time-series data by selecting a small number of terms from a library of candidate nonlinear functions <d-cite key="desilva2020pysindy, brunton2016discovering, champion2022ensemble"></d-cite>.</li> </ul> <p>This post investigates how these two ideas can be combined to move “beyond black-box predictions” in biology. The case study is a five-dimensional ODE model inspired by NF-κB signaling, in which only three species are treated as observed; a Fourier Neural Operator (FNO) is trained to reconstruct full trajectories, including hidden variables, from roughly twenty measurements per trajectory, after which SINDy is applied to dense trajectories from either the ground-truth simulator or the learned operator <d-cite key="hoffmann2002ikb"></d-cite>. Empirically, the FNO accurately interpolates unobserved species with Hill-type nonlinearities and supports arbitrary temporal resampling, while SINDy finds only approximate governing equations and performs similarly on simulator- and FNO-generated data—suggesting that the bottleneck lies in the complexity of biological dynamics and library design, not in operator learning itself. A GitHub repository with the code for experiments performed in this post is <a href="https://github.com/karthik-d/neurops-with-sindy" rel="external nofollow noopener" target="_blank">available here</a>.</p> <hr> <h2 id="2-background-neural-operators--sparse-identification">2. Background: Neural Operators &amp; Sparse Identification</h2> <h3 id="21-neural-operators">2.1 Neural Operators</h3> <p>A neural operator is designed to approximate a mapping between function spaces, for example \(\mathcal{G}: u(\cdot) \mapsto v(\cdot),\) where both the input $u$ and output $v$ are functions (such as fields over space and/or time), rather than finite-dimensional vectors <d-cite key="kovachki2023neural, li2021fourier"></d-cite>.</p> <p>Architectures such as the Fourier Neural Operator (FNO) parameterize $\mathcal{G}$ by repeatedly lifting the input into a higher-dimensional channel space, applying global convolutions in a spectral basis (e.g. via FFT), and then projecting back, yielding a model that can be evaluated on different meshes than it was trained on. This mesh-independence is crucial in scientific computing and makes neural operators particularly attractive for biological applications where temporal or spatial sampling can vary across experiments.</p> <p>Key properties relevant here are:</p> <ul> <li> <strong>Resolution flexibility:</strong> once trained, an FNO can be evaluated on arbitrarily dense time grids (and space grids in PDE settings) without retraining.</li> <li> <strong>Family-level generalization:</strong> if trained over a range of initial conditions or parameters, the neural operator represents a <em>family</em> of dynamical responses, not a single trajectory.</li> <li> <strong>Compatibility with partial observability:</strong> encoders and decoders can map between partial observations (e.g. a subset of species) and full state trajectories, enabling hidden-state reconstruction.</li> </ul> <p>These features differentiate NOs from standard MLPs, RNNs, or neural ODEs, which typically operate on fixed-size vectors and are tied to specific discretizations or sampling schemes <d-cite key="brunton2016discovering"></d-cite>.</p> <h3 id="22-sparse-identification-sindy">2.2 Sparse Identification (SINDy)</h3> <p>Sparse Identification of Nonlinear Dynamics (SINDy) starts from time-series data $x(t)$ and seeks a sparse representation of the vector field $f$ in the ODE \(\frac{dx}{dt} = f(x)\) using a library of candidate basis functions $\Theta(x)$, such as polynomials, trigonometric terms, or domain-specific nonlinearities. The method writes as, \(\frac{dx}{dt} \approx \Theta(x)\,\Xi,\) where $\Xi$ is a sparse matrix of coefficients, and solves a sequence of regularized least-squares problems to find a sparse $\Xi$ that still explains the observed dynamics.</p> <p>In practice, SINDy works best when:</p> <ul> <li>Time derivatives can be estimated accurately (requiring dense and relatively low-noise data).</li> <li>The true dynamics can be represented by a relatively small number of basis functions present in $\Theta$.</li> <li>The measurement space is not too high-dimensional and does not mix many hidden processes.</li> </ul> <p>Extensions such as SINDy-PI for implicit dynamics <d-cite key="kaheman2020sindypi"></d-cite>, Ensemble-SINDy <d-cite key="fasel2022ensemble"></d-cite> for robustness to low-data and noise, and SINDy-CRN / Reactive SINDy for biochemical reaction networks broaden the method’s applicability and robustness.</p> <p>Biological systems pose challenges for SINDy: hidden species, cooperative Hill nonlinearities with unknown exponents, and stiff binding/unbinding kinetics can all make sparse recovery difficult, especially from realistically noisy, sparse data.</p> <hr> <h2 id="3-biological-motivation--challenges">3. Biological Motivation &amp; Challenges</h2> <p>Many biological systems are naturally described as dynamical systems: gene regulatory networks, signaling pathways, and cell–cell communication can often be written as coupled ODEs or PDEs with nonlinear regulatory terms (for example, Hill functions, saturating kinetics, or cooperative binding). In principle, methods like FNOs and SINDy are appealing because they promise two complementary capabilities: learning how trajectories evolve from data (operator learning) and extracting why they evolve that way in the form of interpretable governing equations (sparse identification).</p> <p>However, several aspects of biological data make direct application of existing tools difficult. First, experiments usually provide only partial observability: one or a few fluorescent reporters or bulk readouts, while many internal species (mRNA, complexes, post-translationally modified proteins) remain hidden but dynamically important. Second, measurements are sparse in time and noisy, because high-frequency imaging or repeated sampling is expensive or damaging, and individual cells exhibit strong variability. Third, nonlinearities such as cooperative Hill regulation, complex formation, and feedback loops introduce stiffness and multi-timescale behavior that can be hard for both classical parameter inference and generic deep networks to fit robustly <d-cite key="hoffmann2002ikb, champion2019data"></d-cite>.</p> <p>Classical parameter inference approaches, which fit a mechanistic ODE or PDE model to data, require committing to a specific functional form and often struggle when many parameters are unidentifiable from limited, noisy measurements <d-cite key="dunn2022identification"></d-cite>. Generic deep learning models, including standard sequence models or vanilla neural ODEs, can interpolate trajectories but typically do not generalize across conditions in a controlled way and offer little interpretability about underlying mechanisms. SINDy provides interpretability by selecting sparse libraries of candidate terms, but it assumes access to reasonably dense, relatively low-noise trajectories in a space where the “right” basis functions are already present—conditions rarely met in biology <d-cite key="desilva2020pysindy, kaheman2020sindypi"></d-cite>.</p> <p>This motivates a hybrid approach in which (i) a neural operator learns to reconstruct full, dense trajectories—including hidden species—from limited observations, and (ii) SINDy operates on these richer trajectories to test how far sparse equation discovery can go in a more realistic biological regime. The NF-κB signaling case study below is chosen as a concrete, yet tractable, arena to explore this idea.</p> <hr> <h2 id="4-case-study-simplified-nf-κb-signaling-model">4. Case Study: Simplified NF-κB Signaling Model</h2> <p>NF-κB signaling is a canonical nonlinear feedback system in immunology and systems biology, involving cytoplasmic–nuclear shuttling of NF-κB, synthesis and degradation of its inhibitor IκB, and negative feedback that produces pulsed or oscillatory nuclear localization dynamics <d-cite key="hoffmann2002ikb"></d-cite>. Many established NF-κB models include Hill-type transcriptional activation, formation of NF-κB:IκB complexes, and nonlinear degradation, making them representative of the kinetic motifs seen across signaling and gene regulation.</p> <p>To explore the use of neural operators and SINDy in a controlled setting, this work uses a five-variable ODE model inspired by classic NF-κB signaling modules. The state variables are:</p> <ul> <li>$N$: cytoplasmic NF-κB,</li> <li>$N_n$: nuclear NF-κB,</li> <li>$I$: IκB protein,</li> <li>$M$: IκB mRNA,</li> <li>$C$: cytoplasmic NF-κB:IκB complex.</li> </ul> <p>In the <em>synthetic experiment</em>, only three of these are treated as “observable”:</p> <ul> <li>$N$, $N_n$, and $I$ are taken as observable, mimicking fluorescent readouts of NF-κB localization and IκB abundance.</li> <li>$M$ and $C$ are treated as <em>hidden</em>.</li> </ul> <p>The governing equations are of the form <br> \(\begin{aligned} \dot N &amp;= -k_{\mathrm{imp}}\,N + k_{\mathrm{exp}}\,N_n - k_{\mathrm{bind}}\,I\,N + k_{\mathrm{diss}}\,C,\\ \dot N_n &amp;= k_{\mathrm{imp}}\,N - k_{\mathrm{exp}}\,N_n,\\ \dot I &amp;= \alpha_I\,M - \delta_I\,I + k_{\mathrm{diss}}\,C,\\ \dot M &amp;= \alpha_m\,\frac{N_n^n}{K_m^n + N_n^n} - \delta_m\,M,\\ \dot C &amp;= k_{\mathrm{bind}}\,I\,N - k_{\mathrm{diss}}\,C - k_{\mathrm{degC}}\,C. \end{aligned}\)</p> <p>Here:</p> <ul> <li>$k_{\mathrm{imp}}$, $k_{\mathrm{exp}}$ govern NF-κB nuclear import/export.</li> <li>$k_{\mathrm{bind}}$, $k_{\mathrm{diss}}$, $k_{\mathrm{degC}}$ govern complex formation and degradation.</li> <li>$\alpha_m$, $\delta_m$, $\alpha_I$, $\delta_I$ govern transcription/translation and decay.</li> <li>$n$ and $K_m$ define a Hill-type transcriptional activation of $M$ by nuclear NF-κB.</li> </ul> <p>Appropriate parameter choices yield rich transient and oscillatory-like response dynamics under variations in initial conditions and parameter jitter, consistent with simplified NF-κB models in the literature. This model is deliberately chosen because it concentrates several key challenges for data-driven discovery in biology within a small ODE system: (i) only a subset of variables is measured, (ii) one equation contains a nonlinear Hill function with exponent $n&gt;1$, and (iii) feedback and binding introduce multi-timescale dynamics. At the same time, it is simple enough to simulate efficiently and to serve as a controlled benchmark for comparing dense trajectories from the ground-truth ODE to trajectories reconstructed by a learned FNO, before passing both into SINDy. This makes it a natural testbed for assessing how well neural operators can interpolate hidden biological state and how robust sparse equation discovery remains under realistic biological constraints.</p> <hr> <h2 id="5-methods">5. Methods</h2> <p>A GitHub repository with the code for experiments performed in this post is <a href="https://github.com/karthik-d/neurops-with-sindy" rel="external nofollow noopener" target="_blank">available here</a>.</p> <h3 id="51-synthetic-data-generation">5.1 Synthetic data generation</h3> <p>To emulate realistic biological experiments, an ensemble of trajectories is generated by numerically integrating the NF-κB ODE system under multiple initial conditions and small parameter variation.</p> <ul> <li> <strong>Initial conditions:</strong> about 40 random initial states for $(N, N_n, I, M, C)$, sampled around a baseline equilibrium.</li> <li> <strong>Parameter jitter:</strong> multiplicative perturbations of key parameters (e.g. $\pm 10\%-15\%$) to mimic cell-to-cell variability.</li> <li> <strong>Sampling:</strong> each trajectory is sampled at a modest temporal resolution (e.g. 20 measurement times over the response window), reflecting realistic imaging frequencies.</li> <li> <strong>Noise:</strong> additive noise is applied to the observable channels $(N, N_n, I)$ to approximate measurement noise.</li> </ul> <p>The resulting dataset consists of partial, noisy time series for $(N, N_n, I)$ and corresponding full-state trajectories from the simulator (used only as “ground truth” for evaluation, not exposed to the learner in the FNO training objective).</p> <h3 id="52-fno-for-full-state-reconstruction">5.2 FNO for full-state reconstruction</h3> <p>A one-dimensional Fourier Neural Operator is trained to map sparse trajectories of the observable species to full trajectories of all five state variables. Concretely:</p> <ul> <li> <strong>Input to the FNO:</strong> a time series of shape (time, channels) for the three observed variables $(N, N_n, I)$ on a coarse temporal grid.</li> <li> <strong>Output:</strong> a time series for all five variables $(N, N_n, I, M, C)$ on the same grid.</li> <li> <strong>Architecture:</strong> a compact FNO implementation in PyTorch with a few spectral convolution layers, following the general pattern of Li et al. (2020) and Kovachki et al. (2023) <d-cite key="kovachki2023neural, li2021fourier"></d-cite>.</li> <li> <strong>Loss:</strong> mean squared error between predicted and true trajectories for all five variables, with an emphasis on the observed variables during early training epochs.</li> </ul> <p>Because the FNO learns an operator from <em>functions of time</em> (partial observations) to <em>functions of time</em> (full-state trajectories), it can naturally be evaluated at any temporal resolution once trained, simply by providing the input on its original grid and requesting output on a finer time grid.</p> <h3 id="53-generating-dense-trajectories-ode-vs-fno">5.3 Generating dense trajectories: ODE vs FNO</h3> <p>To support SINDy, two sources of “dense” trajectories are constructed:</p> <ol> <li> <strong>Dense ODE ground truth:</strong> the NF-κB ODE system is integrated with a fine step size, and the full state is saved at a dense time grid (e.g. hundreds to thousands of timepoints). This represents an idealized, noise-free, fully observed dataset that is not attainable experimentally but serves as a benchmark for SINDy.</li> <li> <strong>Dense FNO reconstructions:</strong> the trained FNO takes the original sparse, noisy observation trajectories for $(N, N_n, I)$ and outputs full-state trajectories on a <em>dense</em> time grid, expanded from the original 20 measurement points to a much finer mesh.</li> </ol> <p>The ability to <em>expand the mesh</em> in this way — leveraging operator learning rather than pointwise interpolation — is a key practical advantage of FNOs over traditional feedforward networks trained to map fixed-length input vectors to fixed-length output vectors, which are meant to mimic what a real biological experiment might produce.</p> <p>To query the FNO on a finer time grid, these sparse observations are first linearly interpolated in time, creating a higher-resolution input signal for the operator. The key point is that this interpolation is just a way of providing the operator with values on the new grid; it is not the mechanism that creates meaningful dynamics between measurement times. We illustrate in the following figure; notice that linear interpolated points that fall off-trend are smoothened by FNO.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/interpolation-fno-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/interpolation-fno-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/interpolation-fno-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/interpolation-fno.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Shown are the ODE simulation ground truth (grey), dense timepoints generated by linear interpolation between sparse (synthetic) experimental points used to train the FNO (blue), and the FNO predictions using the dense timepoints (red). Notice that although the interpolated points show stark linear trends, FNO predictions have a smoothing effect that predicts a system closer to ground truth. </div> <p>The important dynamical structure actually comes from the trained FNO itself. Once trained, the FNO represents an operator that maps any reasonable time series of observables (including their interpolated versions) to a full-state trajectory consistent with what it learned during training.[@kovachki2023neural; @li2021fourier] When fed linearly interpolated inputs on a dense grid, the FNO effectively smooths and regularizes the observable channels and, at the same time, predicts the hidden species at those new timepoints according to the learned dynamics. In other words, the operator “fills in” the trajectory in a way that is constrained by the NF-κB-like ODE behavior it has internalized, rather than simply connecting the dots between measurements.</p> <p>Without a trained neural operator (or an equivalent dynamical model), linear interpolation alone would only give straight-line segments between data points for the observables and no information at all about the unobserved variables. By contrast, in this pipeline linear interpolation is merely a numerical convenience to sample the learned operator at arbitrary times: the FNO uses those interpolated observables as a conditioning signal and then generates a dynamically coherent, multi-species trajectory on the dense mesh. This is precisely the kind of densified, multi-variable, low-noise dataset that SINDy and related sparse model discovery methods are designed to work with, but that cannot be obtained from raw experimental traces alone.</p> <h3 id="54-sindy-setup">5.4 SINDy setup</h3> <p>SINDy is applied to both dense datasets using the full 5D state \((N, N_n, I, M, C)\) with numerical derivatives estimated via smoothed finite differences. The library is biology-informed and includes:</p> <ul> <li>Polynomials up to degree 2 for each variable (e.g., \(N, N^2, N_n, N_n^2, ...\)),</li> <li>Bilinear interaction terms motivated by binding kinetics (e.g., \(I \cdot N\) for complex formation),</li> <li>A fixed Hill-type nonlinearity \(\frac{N_n^n}{K^n + N_n^n}\) to capture the transcriptional activation of \(M\).</li> </ul> <p>Sparse regression with sequential \(\ell_1\) thresholding identifies the nonzero coefficients \(\Xi\) in each equation \(\frac{dx_i}{dt} \approx \Theta(x) \cdot \Xi_i,\) following PySINDy defaults. Two experiments isolate the neural operator’s contribution:</p> <ul> <li> <strong>Ideal SINDy</strong>: dense trajectories from direct ODE integration (ground truth benchmark),</li> <li> <strong>FNO-SINDy</strong>: dense trajectories generated by the trained FNO from sparse, noisy observations.</li> </ul> <p>This comparison helps us disentangle the reconstruction quality from the intrinsic challenges of symbolic recovery in complex biological dynamics.</p> <hr> <h2 id="6-results--interpretation">6. Results &amp; Interpretation</h2> <h3 id="61-fno-reconstruction-quality">6.1 FNO reconstruction quality</h3> <p>The trained FNO achieves low mean squared error on the observed channels and reconstructs the hidden variables $M$ and $C$ with qualitatively correct dynamics across test trajectories. In particular, it captures:</p> <ul> <li>Delayed transcriptional pulses in $M$ following nuclear NF-κB translocation.</li> <li>Gradual accumulation and decay of IκB protein $I$.</li> <li>Transient formation and degradation of the NF-κB:IκB complex $C$.</li> </ul> <p>These behaviors are recovered despite training on only ~20 measurement points per trajectory and with additive noise on the observables, highlighting the sample efficiency and denoising properties of neural operators in this setting.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-fit-1-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-fit-1-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-fit-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-fit-1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-fit-2-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-fit-2-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-fit-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-fit-2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Two sample predictions from Fourier Neural Operator model fit (in red) on sparse experimental (synthetic) data with noise and jitter added (in purple). The (grey) line shows the ideal trajectory based on numerically simulating the 5-ODE system. </div> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-perf-table-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-perf-table-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-perf-table-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/fno-perf-table.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Quantitative performance measure of FNO model predictions after training evaluated using a separate test set from the synthetic dataset. </div> <h3 id="62-sindy-on-dense-ode-vs-dense-fno-data">6.2 SINDy on dense ODE vs dense FNO data</h3> <p>A key objective of this study is to evaluate how the temporal resolution and data source affect symbolic equation recovery using SINDy. To this end, we compare SINDy’s performance on:</p> <ul> <li>Sparse timepoints sampled from the FNO reconstructions,</li> <li>Dense timepoints produced by the FNO on a much finer temporal mesh (hundreds to thousands of points), and</li> <li>Dense timepoints obtained by direct numerical integration of the ground-truth NF-κB ODE system (ideal data).</li> </ul> <p>When SINDy is applied to sparse FNO-generated time series, its recovery of underlying dynamics is limited by the coarse temporal sampling and noise inherent in the synthetic observations. However, when the temporally dense FNO trajectories are used instead, there is a clear improvement in the identification of correct equation terms and coefficient stability. This demonstrates that increasing temporal resolution — made possible by the FNO’s mesh-independent operator representation — provides richer information for numerical derivative estimation and sparse regression, which are critical for SINDy’s success.</p> <p>To benchmark the quality of FNO-generated dense data, SINDy is also applied to dense trajectories directly sampled from the NF-κB ODE system. The results show qualitative similarity: SINDy recovers the same key terms and nonlinear structures from both ideal ODE data and dense FNO reconstructions. SINDy’s symbolic recovery on FNO-generated trajectories does not suffer in a substantial way compared to the ideal scenario. This suggests that the FNO produces sufficiently accurate and consistent dense data for downstream symbolic modeling. In other words, the neural operator effectively acts as a high-quality surrogate and data densifier without compromising the interpretability obtained via SINDy.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_high-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_high-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_high-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_high.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_low-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_low-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_low-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_low.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_ideal-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_ideal-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_ideal-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-perf_ideal.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-fit_high-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-fit_high-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-fit_high-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-fit_high.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-fit_low-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-fit_low-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-fit_low-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-fit_low.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-ideal-fit_high-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-ideal-fit_high-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-ideal-fit_high-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-ideal-fit_high.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Column 1 shows SINDy fit to dense FNO data with 8000 points, while column 2 shows fit to sparser FNO data with 4000 points. Column 3 shows ideal SINDy fit using ODE simulation-generated data with 8000 timepoints. Notice that columns 1 and 3 produce similar performance. </div> <p>Despite improvements afforded by dense data, SINDy’s ability to recover accurate governing equations remains limited by intrinsic biological complexities. The NF-κB model features nonlinear Hill functions with sharp sensitivities that challenge sparse regression techniques, while complex formation, multi-timescale feedback, and partial observability further complicate equation recovery. Additionally, many inferred equations, as shown below, include low-magnitude, spurious terms lacking biological meaning, revealing a critical challenge: the design and selection of an appropriate function library for SINDy strongly influence both model fidelity and interpretability.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-ode_high-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-ode_high-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-ode_high-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-neural-ops-in-biology/sindy-ode_high.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> ODE forms inferred by SINDy using dense FNO predicted timepoints (left) and using dense ODE simulated timepoints (right). </div> <p>The presence of these non-essential terms highlights an important open problem: the selection and design of the function library from which SINDy chooses terms critically affects equation quality and interpretability. Future advances could come from incorporating biological priors, reducing library redundancy, or dynamically learning basis functions tailored to the system under study.</p> <hr> <h2 id="7-key-contributions--insights">7. Key Contributions &amp; Insights</h2> <p>Summarizing the main insights from this case study:</p> <ul> <li> <strong>Hidden-state interpolation from sparse observations:</strong> The FNO successfully reconstructs unobserved species with complex Hill-type dynamics from limited measurements of three observable species, illustrating how operator learning can bridge gaps in biological observability.</li> <li> <strong>Mesh expansion and dense resampling:</strong> Once trained, the FNO can generate dense trajectories at arbitrary temporal resolution, providing the kind of data SINDy needs but is rarely available experimentally, without retraining a separate model for each resolution.</li> <li> <strong>Comparable SINDy performance on ODE vs FNO data:</strong> The similar quality of SINDy’s output on dense ODE and dense FNO trajectories indicates that neural operators can be used as front-ends for symbolic discovery without fundamentally limiting performance in this setting.</li> <li> <strong>Biology-aware modeling:</strong> The experiment explicitly targets features common in biological systems — Hill kinetics, complex formation, feedback — and shows that FNOs handle these dynamics robustly, complementing existing uses of neural operators for more traditional physical PDE systems.</li> <li> <strong>Extensibility to PDE models:</strong> While this case study focuses on ODEs, the same methodology applies directly to PDE-based biological models (e.g. NF-κB or morphogen gradients in tissues, reaction–diffusion systems), where neural operators have already been used as surrogates for complex PDE solvers <d-cite key="kovachki2023neural"></d-cite>.</li> </ul> <hr> <h2 id="8-discussion--limitations">8. Discussion &amp; Limitations</h2> <p>From a method perspective, this work suggests that neural operators are particularly well-suited to the realities of biological data:</p> <ul> <li>They naturally operate on functions (time series, spatial fields) rather than fixed-length vectors.</li> <li>They can integrate information across time and conditions to infer hidden variables.</li> <li>They enable re-sampling at arbitrary time grids, helpful for downstream tasks like SINDy.</li> </ul> <p>Compared to neural ODEs or PINNs, which typically require explicit parameterization of the underlying ODE/PDE and strong supervision, neural operators can be trained more directly on input–output pairs of functions, making them attractive in settings where the mechanistic equations are only partially known or too complex to write down.</p> <p>However, several important limitations remain:</p> <ul> <li> <strong>Symbolic recovery is hard:</strong> Even with ideal dense ODE data, SINDy only partially recovers the NF-κB equations. Cooperative Hill nonlinearities, stiff binding dynamics, and partial observability push the method beyond its most comfortable regime.</li> <li> <strong>Library design:</strong> Including Hill functions with unknown exponents, saturating terms, and domain-informed motifs in the SINDy library is nontrivial. Learning or adapting the library, potentially guided by the neural operator’s local response, remains an open problem.</li> <li> <strong>Data richness:</strong> The NF-κB case study uses a modest set of initial conditions and parameter jitters. Larger, multi-perturbation datasets (e.g. varying stimuli, genetic backgrounds) would likely be needed to more fully identify nonlinear regulatory structure.</li> <li> <strong>Model mismatch:</strong> The synthetic ODE model, while biologically inspired, is still a simplification. In real systems, unmodeled processes, delays, and stochasticity further complicate both operator learning and symbolic discovery.</li> </ul> <p>These limitations suggest that the current results should be viewed as a <em>step in the right direction</em> rather than a complete solution: neural operators make it feasible to reconstruct hidden states and densify data, but full equation discovery in realistic biological systems remains a challenging research frontier.</p> <hr> <h2 id="9-conclusion--outlook">9. Conclusion &amp; Outlook</h2> <p>This blog post presented a proof-of-concept integration of neural operators and sparse identification for a biologically motivated NF-κB signaling model. The main conclusion is that neural operators can act as powerful, data-efficient surrogates that reconstruct hidden species and provide dense, noise-reduced trajectories from limited measurements, thereby enabling symbolic methods like SINDy to be applied where they otherwise could not.</p> <p>While the SINDy results are not yet “textbook perfect,” their similar performance on dense ODE and FNO-generated data indicates that neural operators do not fundamentally hinder symbolic discovery in this setting. Instead, the difficulty lies in the complexity of biological kinetics and the need for richer, more biologically structured libraries and datasets. Advancing library design and hybrid approaches integrating mechanistic insight with data-driven learning stand as promising avenues to unlock more accurate and actionable models of complex biological dynamics.</p> <p>Looking forward, this operator-plus-symbolic paradigm is promising for:</p> <ul> <li>PDE-based biological models (morphogen gradients, tissue-scale NF-κB waves, reaction–diffusion patterning),</li> <li>single-cell dynamical systems with partial readouts (e.g. live-cell imaging with a few fluorescent reporters),</li> <li>spatial omics dynamics (e.g. coarse-grained spatial transcriptomics over time),</li> <li>and synthetic biology control systems where interpretable equations remain key for design and analysis.</li> </ul> <p>Neural operators such as the FNO provide a flexible bridge between sparse biological measurement and dense, mechanistically meaningful dynamical models. Combining them with advances in sparse model discovery may eventually yield a practical workflow for “learning the equations of life” from real experimental data.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026-iclr-blogpost-earth4d/assets/bibliography/2026-04-27-neural-ops-in-biology.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/ppo-batch-size/">The Trade-off Between Parallel Environments and Steps in PPO</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/agent-evaluation/">A Hitchhiker's Guide to Agent Evaluation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026-iclr-blogpost-earth4d/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-data.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>