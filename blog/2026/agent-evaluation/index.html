<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Hitchhiker's Guide to Agent Evaluation | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="An introductory guide to LLM-based agents' evaluation. We explore what makes agent evaluation different from traditional LLM benchmarks, how to measure success, safety, and trajectory quality, and highlight open challenges in the field."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026-iclr-blogpost-earth4d/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/agent-evaluation/"> <script src="/2026-iclr-blogpost-earth4d/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/template.v2.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "A Hitchhiker's Guide to Agent Evaluation",
            "description": "An introductory guide to LLM-based agents' evaluation. We explore what makes agent evaluation different from traditional LLM benchmarks, how to measure success, safety, and trajectory quality, and highlight open challenges in the field.",
            "published": "April 28, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026-iclr-blogpost-earth4d/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/" rel="external nofollow noopener" target="_blank"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener" target="_blank">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>A Hitchhiker's Guide to Agent Evaluation</h1> <p>An introductory guide to LLM-based agents' evaluation. We explore what makes agent evaluation different from traditional LLM benchmarks, how to measure success, safety, and trajectory quality, and highlight open challenges in the field.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#how-do-llm-and-agent-evaluation-differ">How Do LLM and Agent Evaluation Differ?</a> </div> <ul> <li> <a href="#single-step-vs-multi-step">Single-step vs. Multi-step</a> </li> <li> <a href="#output-vs-outcome">Output vs. Outcome</a> </li> <li> <a href="#passive-vs-interactive">Passive vs. Interactive</a> </li> </ul> <div> <a href="#what-does-agent-evaluation-actually-measure">What Does Agent Evaluation Actually Measure?</a> </div> <ul> <li> <a href="#model-vs-system-performance">Model vs. System Performance</a> </li> <li> <a href="#primary-metrics">Primary Metrics</a> </li> </ul> <div> <a href="#how-to-evaluate-agent-reliability-and-safety">How to Evaluate Agent Reliability and Safety?</a> </div> <ul> <li> <a href="#consistency-metrics">Consistency Metrics</a> </li> <li> <a href="#policy-adherence">Policy Adherence</a> </li> <li> <a href="#adversarial-safety-tests">Adversarial Safety Tests</a> </li> </ul> <div> <a href="#how-to-evaluate-agent-trajectories">How to Evaluate Agent Trajectories?</a> </div> <ul> <li> <a href="#milestones-and-subgoals">Milestones and Subgoals</a> </li> <li> <a href="#agent-as-a-judge">Agent-as-a-Judge</a> </li> <li> <a href="#tool-call-analysis">Tool-Call Analysis</a> </li> </ul> <div> <a href="#what-are-the-big-open-problems-research-questions-in-agent-evaluation">What Are the Big Open Problems &amp; Research Questions in Agent Evaluation?</a> </div> <ul> <li> <a href="#scalability">Scalability</a> </li> <li> <a href="#cost-efficiency">Cost-Efficiency</a> </li> <li> <a href="#long-term-autonomy">Long-term Autonomy</a> </li> <li> <a href="#generalist-agents">Generalist Agents</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>As Large Language Models (LLMs) evolve from standalone text generators into <strong>autonomous agents</strong> capable of taking actions in the real world, the way we evaluate them must fundamentally change. Traditional benchmarks that measure text quality or accuracy are no longer sufficient. We need evaluation frameworks that assess whether agents can perform multi-step tasks in dynamic environments to achieve goals in a reliable and safe way.</p> <p>This blog provides a hitchhiker’s guide to the emerging field of agent evaluation. It begins by detailing the key distinctions from traditional LLM evaluation, and then describes how these differences affect evaluation solutions. We organize the paper around the main questions that can allow easy entrance to newcomers to the field.</p> <hr> <h2 id="how-do-llm-and-agent-evaluation-differ">How Do LLM and Agent Evaluation Differ?</h2> <p>The shift from LLMs to agents introduces three fundamental changes in evaluation philosophy:</p> <h3 id="single-step-vs-multi-step">Single-step vs. Multi-step</h3> <p>LLM benchmarks mostly assess one-step tasks. Agents handle <strong>long-horizon tasks</strong> requiring planning and multiple steps. For example, the SWE-bench coding tasks require editing multiple functions and files to fix a bug, going far beyond single-line code generation <d-cite key="jimenez2023swebench"></d-cite>. Similarly, τ-bench tasks involve multi-turn dialogues with tools and database queries, which standard LLM evaluation would not capture <d-cite key="yao2024tbench"></d-cite>.</p> <blockquote> To make an analogy, LLM evaluation is like examining the performance of an engine. In contrast, agent evaluation assesses a car’s performance comprehensively, as well as under various driving conditions. </blockquote> <h3 id="output-vs-outcome">Output vs. Outcome</h3> <p>Traditional LLM evaluation focuses on <strong>text-generation quality</strong>, accuracy on a benchmark, likelihood scores, or fluency metrics. Agent evaluation, by contrast, focuses on <strong>task completion</strong>. We care about whether a goal is achieved (e.g., a flight booked, GitHub issue solved), not just the plausibility of generated text <d-cite key="mohammadi2025survey"></d-cite>.</p> <h3 id="passive-vs-interactive">Passive vs. Interactive</h3> <p>Agents operate in <strong>dynamic environments</strong>, interacting with users or APIs. This means evaluation must account for interactivity and adherence to external rules. For instance, τ-bench highlights that an agent must gather user information, call backend APIs, and follow domain-specific policy rules during a conversation. Safety also becomes critical: unlike pure LLM tasks, an agent evaluation must check for policy compliance and avoidance of unsafe actions (e.g. deleting the code base) <d-cite key="levy2024stwebagentbench"></d-cite>.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph LLM["Traditional LLM Evaluation"]
        A[Input Prompt] --&gt; B[Text Output]
        B --&gt; C[Quality Metrics]
    end
    
    subgraph Agent["Agent Evaluation"]
        D[Goal/Task] --&gt; E[Multi-step Actions]
        E --&gt; F[Environment Interaction]
        F --&gt; G[Outcome Assessment]
        G --&gt; H[Safety &amp; Policy Check]
    end
</code></pre> <hr> <h2 id="what-does-agent-evaluation-actually-measure">What Does Agent Evaluation Actually Measure?</h2> <h3 id="model-vs-system-performance">Model vs. System Performance</h3> <p>When evaluating a fixed agent framework, performance reflects the underlying LLM’s capability (e.g., tool-calling accuracy, reasoning). In this case, we are effectively measuring the model’s problem-solving ability on multi-step agentic tasks.</p> <p>By contrast, when comparing different agent architectures or “scaffolds,” the evaluation measures the full agent architecture. Leaderboards like the ones for SWE-bench and AppWorld have more focus on evaluating the different scaffolds and not only the model itself. More broadly, the <strong>Holistic Agent Leaderboard (HAL)</strong> conducts standardized trials across many tasks and frameworks to isolate architectural effects <d-cite key="kapoor2025hal"></d-cite>. For example, HAL ran 21,730 rollouts over 9 LLMs, 9 different agent “scaffolds,” and multiple benchmarks (coding, web navigation, etc.), revealing how agent design (e.g., planning algorithm, memory use) affects success.</p> <h3 id="primary-metrics">Primary Metrics</h3> <p>Most agent benchmarks report <strong>success rates</strong> or <strong>task completion percentages</strong> as the main metric (analogous to accuracy). These metrics test whether the agent was able to achieve the task, but other auxiliary measures are needed for evaluating the multi-step agent actions (trajectory) quality and efficiency of the agent, as pointed out by the AI-Agent that matters paper. Auxiliary measures include:</p> <table> <thead> <tr> <th>Metric Type</th> <th>Examples</th> </tr> </thead> <tbody> <tr> <td>Primary</td> <td>Success rate, task completion %</td> </tr> <tr> <td>Efficiency</td> <td>Latency, token cost, number of steps</td> </tr> <tr> <td>Partial Credit</td> <td>Subtask completion, milestone-based accuracy</td> </tr> <tr> <td>Trajectory Quality</td> <td>Action sequence correctness, tool usage accuracy</td> </tr> </tbody> </table> <p>Other metrics from traditional NLP (perplexity, F1) are rarely appropriate for agents because the “text output” is just one small part of the process. Instead, agent evaluation often includes metrics for action sequences, tool usage, and end-state correctness. This change reflects a higher focus on semantic evaluation than on syntactic one.</p> <hr> <h2 id="how-to-evaluate-agent-reliability-and-safety">How to Evaluate Agent Reliability and Safety?</h2> <p>Beyond raw performance, agents must demonstrate <strong>reliability</strong> and <strong>safety</strong>. This section covers three critical dimensions.</p> <h3 id="consistency-metrics">Consistency Metrics</h3> <p>Because LLM agents are nondeterministic, it is not sufficient to only measure the agent success rate; we also need to measure how reliably an agent performs a task over multiple runs. Common metrics are <strong>pass@k</strong> and <strong>pass^k</strong> rates <d-cite key="yao2024tbench"></d-cite>:</p> \[\text{pass@}k = \text{Success in one of } k \text{ attempts}\] \[\text{pass}^k = \text{Success in all } k \text{ attempts}\] <p>Where:</p> <ul> <li>Success is defined by the task completion metric</li> <li>$k$ is the number of attempts</li> <li> <strong>pass@k</strong> represents the agent’s ability to succeed <em>at least once</em> in $k$ attempts</li> <li> <strong>pass^k</strong> represents the agent’s ability to succeed on <em>every one</em> of $k$ trials</li> </ul> <d-footnote>The pass@k metric is useful for scenarios where you can retry, while pass^k is crucial for production systems where consistent performance is required.</d-footnote> <p>For example, τ-bench explicitly introduced pass^k to quantify agent consistency. In practice, modern agents often have high pass@1 but rapidly falling pass^k. Yao et al. report that <strong>GPT-4’s success on τ-bench drops from ~61% (pass@1) to only ~25% for pass^8</strong>, underscoring that a good agent must not only succeed sometimes, but succeed consistently.</p> <h3 id="policy-adherence">Policy Adherence</h3> <p>In interactive or enterprise settings, agents must obey rules or policies. Benchmarks now include safety constraints as part of the task. For instance, <strong>ST-WebAgentBench</strong> explicitly provides a hierarchy of organizational policies and measures whether the agent completes the task under those policies <d-cite key="levy2024stwebagentbench"></d-cite>.</p> <p>A proposed metric is <strong>Completion under Policy (CuP)</strong>, which gives credit only if no policy is violated. Studies find that state-of-the-art agents often fail on these criteria—for example, many succeed at completing a web task but ignore critical safety rules. These metrics are especially important for high-risk organizational agents.</p> <h3 id="adversarial-safety-tests">Adversarial Safety Tests</h3> <p>Additional evaluations probe harmful or unsafe behaviors by design. For example, the <strong>CoSafe benchmark</strong> feeds agents adversarial prompts (e.g., requests for illicit instructions) and measures the rate of unsafe completions <d-cite key="pan2024cosafe"></d-cite>. Other tests like <strong>AgentHarm</strong> quantify the agent’s tendency to produce disallowed content.</p> <p>In practice, one might report the percentage of trials in which the agent violates a safety rule (akin to a “failure rate” under adversarial stress). These measures complement success metrics, ensuring an agent is not only effective but also aligned with ethical and safety standards.</p> <hr> <h2 id="how-to-evaluate-agent-trajectories">How to Evaluate Agent Trajectories?</h2> <p>Beyond final outcomes, understanding <em>how</em> an agent arrives at its solution is crucial. This section covers trajectory-level evaluation.</p> <h3 id="milestones-and-subgoals">Milestones and Subgoals</h3> <p>Many agent tasks are naturally hierarchical. Benchmarks often define intermediate checkpoints or key subgoals along the trajectory. For instance, <strong>TheAgentCompany</strong> benchmark explicitly designs tasks that require many consecutive steps and provides partial credit for completing subtasks <d-cite key="xu2024theagentcompany"></d-cite>. Likewise, <strong>WebCanvas</strong> measures success rates at “key nodes” in the workflow <d-cite key="zhou2024webcanvas"></d-cite>.</p> <p>By breaking a task into milestones, evaluators can compute metrics like:</p> <ul> <li>Fraction of subtasks achieved</li> <li>Milestone-based accuracy</li> <li>Progress score (even for failed tasks)</li> </ul> <p>This provides a finer-grained view of progress than a single binary outcome.</p> <h3 id="agent-as-a-judge">Agent-as-a-Judge</h3> <p>A recent trend is to use LLMs (or agents) themselves to evaluate trajectories. The <strong>LLM-as-a-Judge</strong> paradigm employs a large model to score or critique an agent’s multi-step output <d-cite key="zhuge2024agentjudge"></d-cite>.</p> <p>For example, Zhuge et al. (2024) propose an <strong>Agent-as-a-Judge</strong> framework: multiple AI agents read an execution trace and vote on success. Such approaches can automatically assess factors like:</p> <ul> <li>Logical consistency</li> <li>Goal alignment</li> <li>Efficiency of the solution path</li> </ul> <p>They remain experimental, but they show promise for scalable, subjective evaluations.</p> <h3 id="tool-call-analysis">Tool-Call Analysis</h3> <p>Since agent interaction with its environment is based on tool calling, evaluating the sequence of tool invocations is critical. Key questions are:</p> <ul> <li>Did the agent call the <strong>right tools</strong>?</li> <li>Were they called in the <strong>right order</strong>?</li> </ul> <pre><code class="language-mermaid">flowchart TD
    A[Agent Action] --&gt; B{Tool Call?}
    B --&gt;|Yes| C[Invocation Accuracy]
    B --&gt;|No| D[Text Response]
    C --&gt; E[Tool Selection Accuracy]
    E --&gt; F[Sequence Analysis]
    F --&gt; G[Graph-based Metrics]
    G --&gt; H[Node F1: Correct tools]
    G --&gt; I[Edge F1: Correct ordering]
    G --&gt; J[Edit Distance: Path similarity]
</code></pre> <p>Metrics include <d-cite key="mohammadi2025survey"></d-cite>:</p> <table> <thead> <tr> <th>Metric</th> <th>What it Measures</th> </tr> </thead> <tbody> <tr> <td>Invocation Accuracy</td> <td>Was a tool call needed at each step?</td> </tr> <tr> <td>Tool Selection Accuracy</td> <td>Was the correct tool chosen?</td> </tr> <tr> <td>MRR/NDCG</td> <td>Ranking quality of tool selection</td> </tr> <tr> <td>Node F1</td> <td>Correct tools chosen (graph-based)</td> </tr> <tr> <td>Edge F1</td> <td>Correct ordering of tools</td> </tr> <tr> <td>Normalized Edit Distance</td> <td>Similarity to reference trajectory</td> </tr> </tbody> </table> <p>In addition, <strong>execution-based evaluation</strong> runs the tool calls to verify they produce the right output. For instance, <strong>GorillaBench</strong> executes each proposed function call to verify it produces the right output <d-cite key="patil2023gorilla"></d-cite>. Similarly, τ-bench applies the tools to measure if the agent achieves the desired database state.</p> <hr> <h2 id="what-are-the-big-open-problems--research-questions-in-agent-evaluation">What Are the Big Open Problems &amp; Research Questions in Agent Evaluation?</h2> <p>Despite rapid progress, several fundamental challenges remain in agent evaluatio<d-cite key="yehudai2025survey"></d-cite>.</p> <h3 id="scalability">Scalability</h3> <p>Current agent evaluations are <strong>resource-intensive</strong>. Running complex tasks with many trials (especially using large models) can cost thousands of dollars. HAL’s evaluation harness reduced wall-clock time, but still required 21,730 agent rollouts across 9 benchmarks at a cost of about <strong>$40,000</strong> <d-cite key="kapoor2025hal"></d-cite>. Moreover, creating evaluation data is time and cost intensive, making it hard to evaluate agents on new domains.</p> <p>Future work must improve:</p> <ul> <li>Efficient evaluation: achieving comparable evaluation signal with fewer resources</li> <li>Automation: automate synthetic benchmark creation with LLM-based pipelines</li> </ul> <h3 id="cost-efficiency">Cost-Efficiency</h3> <p>As model inference is expensive, a key question is how to balance performance against computational cost. HAL, for example, emphasizes <strong>Pareto frontiers of accuracy vs. inference cost</strong> <d-cite key="kapoor2025hal"></d-cite>.</p> <p>Such multi-objective evaluation is still nascent: we need standardized ways to report cost (token usage, latency, cloud bills) alongside success rates. Without this, improvements may hide astronomical costs.</p> <h3 id="long-term-autonomy">Long-term Autonomy</h3> <p>Evaluating agents over extended interactions remains challenging. Most benchmarks cover <strong>minutes-long tasks</strong>; long-term autonomy would involve days or continuous deployment.</p> <p>Some recent efforts study simulated multi-day environments, or tasks with long-horizon goals. However, metrics for tracking sustained goal achievement or adaptation over time are still underdeveloped. How do we measure an agent’s ability to pursue a goal if the task evolves over hours or days? This “life-long” evaluation is an open frontier.</p> <h3 id="generalist-agents">Generalist Agents</h3> <p>Many benchmarks focus on narrow domains, but we aspire to agents that generalize across tasks and environments. Evaluating such <strong>generalist agents</strong> requires broad, heterogeneous test suites.</p> <p>TheAgentCompany attempted this by mixing coding, management, and finance tasks; even so, the best agent solved only <strong>~24% of tasks</strong> <d-cite key="xu2024theagentcompany"></d-cite>, highlighting the difficulty.</p> <p>Open questions include:</p> <ul> <li>How to aggregate performance across diverse tasks (weighted averages, worst-case)?</li> <li>How to design benchmarks that fairly test versatility?</li> <li>How to maintain agent and environment generality when producing a general agent evaluation framework?</li> </ul> <hr> <h2 id="conclusion">Conclusion</h2> <p>Agent evaluation is at an exciting inflection point. As LLMs become autonomous actors in the world, we need evaluation paradigms that go beyond text quality to assess:</p> <ol> <li> <strong>Task completion</strong> over long horizons</li> <li> <strong>Safety and policy compliance</strong> in interactive settings</li> <li> <strong>Consistency and reliability</strong> across multiple runs</li> <li> <strong>Trajectory quality</strong> including tool usage and intermediate steps</li> <li> <strong>Cost-efficiency</strong> and scalability of the evaluation itself</li> </ol> <p>The benchmarks and metrics described here—from SWE-bench and τ-bench to HAL and ST-WebAgentBench—represent important first steps. But as agents become more capable and are deployed in higher-stakes domains, the evaluation frameworks must continue to evolve.</p> <p>For practitioners, the key takeaway is: <strong>don’t just measure if your agent works, measure if it works safely, consistently, and efficiently</strong>. For researchers, the open problems in scalability, long-term autonomy, and generalist evaluation offer rich opportunities for contribution.</p> <p>The hitchhiker’s guide to agent evaluation is still being written—and there’s plenty of galaxy left to explore.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026-iclr-blogpost-earth4d/assets/bibliography/2026-04-28-agent-evaluation.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/ppo-batch-size/">The Trade-off Between Parallel Environments and Steps in PPO</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/witness-problem/">The Witness Problem in Multi-Agent Cooperation</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026-iclr-blogpost-earth4d/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-data.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>