<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Why Did the AI Model Say That? A Methodological Practical Guide for Explaining Multimodal Medical Decisions | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="A methodological guide that summarizes key explainability techniques for understanding multimodal medical AI systems, addressing the critical challenge of understanding what these systems have learned and how they'll behave in real-world clinical settings."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026-iclr-blogpost-earth4d/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/why-did-the-model-say-that/"> <script src="/2026-iclr-blogpost-earth4d/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/template.v2.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Why Did the AI Model Say That? A Methodological Practical Guide for Explaining Multimodal Medical Decisions",
            "description": "A methodological guide that summarizes key explainability techniques for understanding multimodal medical AI systems, addressing the critical challenge of understanding what these systems have learned and how they'll behave in real-world clinical settings.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "#",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026-iclr-blogpost-earth4d/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/" rel="external nofollow noopener" target="_blank"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener" target="_blank">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Why Did the AI Model Say That? A Methodological Practical Guide for Explaining Multimodal Medical Decisions</h1> <p>A methodological guide that summarizes key explainability techniques for understanding multimodal medical AI systems, addressing the critical challenge of understanding what these systems have learned and how they'll behave in real-world clinical settings.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#part-1-how-multimodal-models-build-representations">Part 1 - How Multimodal Models Build Representations</a> </div> <ul> <li> <a href="#the-representation-space">The Representation Space</a> </li> <li> <a href="#multimodal-representation">Multimodal Representation</a> </li> </ul> <div> <a href="#part-2-explainability-techniques-for-probing-multimodal-systems">Part 2 - Explainability Techniques for Probing Multimodal Systems</a> </div> <ul> <li> <a href="#visual-attribution-methods">Visual Attribution Methods</a> </li> <li> <a href="#probing-learned-representations">Probing Learned Representations</a> </li> <li> <a href="#cross-modal-attention-visualization">Cross-Modal Attention Visualization</a> </li> <li> <a href="#model-agnostic-methods">Model-Agnostic Methods</a> </li> <li> <a href="#modality-ablation-studies">Modality Ablation Studies</a> </li> </ul> <div> <a href="#part-3-current-challenges-and-gaps">Part 3 - Current Challenges and Gaps</a> </div> <ul> <li> <a href="#dataset-scarcity">Dataset Scarcity</a> </li> <li> <a href="#missing-data-handling">Missing Data Handling</a> </li> <li> <a href="#general-purpose-encoders">General-Purpose Encoders</a> </li> <li> <a href="#shallow-fusion-strategies">Shallow Fusion Strategies</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Medical AI has emerged as a potential solution to some of healthcare’s most intractable problems, including specialist shortages, treatment backlogs, and limited access to care affecting millions worldwide. These systems promise to automate diagnosis, predict disease progression, and detect abnormalities at scale. The technology has advanced rapidly, with algorithms now achieving impressive accuracy rates on diagnostic tasks. But as these systems scale from research labs into hospitals and clinics, a critical gap has become apparent: we do not fully understand what these models have actually learned or how they will behave in the unpredictable conditions of real-world medical practice. Accuracy, as it turns out, is only part of the equation.</p> <p>Consider what happened when researchers trained AI models to detect COVID-19 from chest X-rays <d-cite key="degrave2021ai"></d-cite>. The models achieved impressive accuracy on paper. But when the researchers dug deeper, they discovered something troubling. The algorithms weren’t learning to recognize actual disease pathology. Instead, they were exploiting shortcuts. To understand what the models were actually seeing, the researchers used a technique called Saliency Mapping. Saliency maps work by measuring how much each pixel in an image contributes to the final prediction. The result is a visualization that highlights the regions the model relied on most. The researchers noticed that the models sometimes focused on the lungs, suggesting they had potentially learned something about COVID-19 pathology. But the models also highlighted regions that had nothing to do with disease, such as stamped dataset markers or the edges of images. The researchers discovered that instead of learning to diagnose COVID-19, the model had learned to distinguish datasets.</p> <p>The COVID-19 detection models operated in a relatively simple world. They looked at chest X-rays and made a call. But the cutting edge of medical AI has moved well beyond single data sources. Modern systems integrate chest X-rays with radiology reports, patient histories, lab results, and vital signs to render diagnoses. This multimodal approach has become the dominant paradigm in medical AI research. In 2018, just three published papers explored multimodal medical diagnostics. By 2024, that number exceeded 150 <d-cite key="schouten2025navigating"></d-cite>. The fifty-fold increase reflects a fundamental insight about how medicine actually works. Doctors do not diagnose from images alone. They synthesize visual findings with clinical context, patient symptoms, and prior medical history. AI systems that mimic this approach consistently outperform single-modality models.</p> <p>But the same complexity that makes multimodal AI powerful also makes it harder to understand. Multimodal models require investigating several interacting systems simultaneously. The transition from research prototype to clinical tool will be happening soon, not in some distant future. Understanding what these systems have learned before they become embedded in medical practice has become critical. These systems operate as what researchers call “black boxes.” A black box model produces accurate outputs without revealing its internal reasoning process. You can observe what goes in and what comes out, but the transformation happening inside remains opaque. This creates a fundamental tension. The architectural sophistication that drives performance gains also deepens this opacity. The ideal would be systems that are both highly accurate and genuinely interpretable. Current multimodal AI achieves the former while struggling with the latter.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/spectrum-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/spectrum-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/spectrum-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/spectrum.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1: The Encoding Process - Transforming Heterogeneous Medical Data into Unified Embeddings </div> <p>This is where Explainable AI (XAI) becomes critical. XAI encompasses techniques designed to make neural network decisions interpretable. Attention visualization reveals which image regions influence predictions. Attribution methods quantify feature importance. Embedding analysis exposes how models internally organize medical knowledge. These tools provide a window into the representations models learn, revealing not just what they predict, but how they structure information about disease.</p> <p>This tutorial explores explainability as a lens for studying multimodal representation learning in medical AI. It’s organized around three interconnected themes. First, we will examine how multimodal models represent medical knowledge internally. When a model processes both a chest X-ray and a clinical report, how does it encode that information? What does the learned representation space look like, and where do different medical concepts sit within it? Understanding these representations is fundamental to interpreting what models have actually learned. Second, we will explore explainability techniques specifically designed for probing multimodal systems. These range from attention visualization methods that reveal cross-modal alignment to attribution techniques that measure how models weight different data sources. We will see how these tools can be used not just to explain individual predictions, but to compare learned representations across different architectures. Finally, we will examine the real-world challenges and research gaps that remain. Missing data, deployment failures, and systematic disparities in how multimodal medical AI is developed all point to fundamental questions about representation learning that the field has yet to answer: Do different architectures converge on similar representations? Are there universal medical concepts that all models encode similarly? Can we predict which models will transfer knowledge effectively?</p> <p>The goal is not just to make multimodal models more transparent. It is to use explainability as a research methodology for understanding what multimodal models learn, and whether that learning reflects genuine medical insight or sophisticated pattern matching.</p> <hr> <h2 id="part-1-how-multimodal-models-build-representations">Part 1: How Multimodal Models Build Representations</h2> <h3 id="the-representation-space">The Representation Space</h3> <p>When a doctor learns to diagnose a disease, the process works like traditional education. Read the textbooks. Memorize the diagnostic criteria. Learn which symptoms point to which conditions, what findings appear on imaging, and when to order which tests. Ask a doctor why they made a diagnosis, and they’ll walk you through it. This is how human doctors learn. Multimodal medical AI systems learn differently. Every input, whether it be a chest X-ray, a clinical note, or lab results, first passes through an encoder, a component that transforms the raw data into a mathematical representation called an embedding. These embeddings are vectors of numbers that capture the essential features of the input.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/learned_representation-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/learned_representation-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/learned_representation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/learned_representation.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Visualizing the Learned Representation Space of Multimodal Medical AI </div> <p>All these embeddings exist in a high-dimensional mathematical space called the representation space (also called latent space or embedding space). In this space, every medical concept occupies its own location based on the patterns the model discovered during training. The model doesn’t store knowledge as rules or criteria. Instead, it organizes information geometrically. Concepts that appear in similar contexts or share statistical patterns in the training data tend to be positioned closer to each other. This often, but not always, corresponds to semantic or clinical similarity.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/shared_representation-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/shared_representation-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/shared_representation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/shared_representation.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Example Shared Representation Space </div> <p>The representation space reveals what the system has learned. If the model has grasped genuine medical insight, its representation space can reflect clinical reality. Bacterial and viral pneumonias should cluster together. Conditions affecting the same organ system should be neighbors. Different severities of the same disease should sit along a spectrum. But if the model has learned superficial shortcuts, like those COVID-19 detectors that memorize dataset markers, the space might look different. The representation space becomes a diagnostic tool in itself. Not for diagnosing patients, but for diagnosing what the AI has learned. We will explore specific techniques for probing and visualizing these representation spaces in Part 2 of this blog.</p> <h3 id="multimodal-representation">Multimodal Representation</h3> <p>But how does a model create a unified representation space when working with fundamentally different data types? A chest X-ray arrives as a 512×512 grid of pixel intensities. A radiology report is a sequence of words. Lab values are structured numbers. These modalities don’t naturally speak the same language. Modern medical AI systems solve this through a two-step process.</p> <h4 id="step-1-encoding-data">Step 1: Encoding Data</h4> <p>First, each modality gets its own specialized encoder. An encoder is a computational structure that transforms raw data into a standardized numerical format called an embedding. For medical images like chest X-rays or pathology slides, the dominant encoder type is a convolutional neural network. CNNs process images through stacked filters that detect patterns at increasing levels of complexity. Early filters might recognize edges or textures. Deeper filters identify anatomical structures like blood vessels or tissue boundaries. By the final stage, the CNN has compressed a high-resolution image into a compact vector of numbers that captures its essential visual features.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/image_encoder-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/image_encoder-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/image_encoder-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/image_encoder.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Image Encoder: CNN </div> <p>Text data requires different machinery. Radiology reports and clinical notes get processed by transformers or recurrent neural networks. These encoders understand sequential information and can capture relationships between words. A transformer can learn that “bilateral infiltrates” and “diffuse opacities” describe similar findings even though they use different terminology. The encoder converts the report into a numerical vector that encodes its medical meaning.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/test_encoder-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/test_encoder-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/test_encoder-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/test_encoder.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5: Text Encoder: Transformer Language Model </div> <p>Structured data like lab values or vital signs typically uses simpler encoders. Multi-layer perceptrons or even handcrafted feature extractors can transform these already-numerical inputs into representations compatible with the other modalities. Some researchers design custom encoders for specialized data types like genetic sequences or protein structures.</p> <h4 id="step-2-fusion">Step 2: Fusion</h4> <p>Fusion is a model architecture decision. Architecture describes the arrangement of computational components within a model and the paths data takes as it moves from input to output. Different architectural choices create different learning capacities. While some structures allow deep interaction between modalities, others keep them largely separate until the final step.</p> <p>Fusion is the process of combining or integrating representations from different modalities so the model can learn cross-modal patterns and relationships. The timing of fusion varies across architectures. Intermediate fusion dominates current research, appearing in 79% of studies <d-cite key="schouten2025navigating"></d-cite>. In this architectural pattern, each modality first passes through its own dedicated encoder, and fusion occurs after encoding. After fusion, the output (whether a single combined vector or interacting representations) then flows through additional neural network layers before reaching the final output, whether that’s a diagnosis classification or risk prediction.</p> <p>Intermediate fusion allows each encoder to specialize in its own data type while still enabling the model to learn cross-modal patterns. Intermediate fusion sits between two extremes. It’s not early fusion, where raw data combines before any encoding. It’s not late fusion, where each modality maintains complete independence until final predictions merge. Common intermediate fusion strategies include concatenation, attention mechanisms, projection into shared embedding spaces, and methods that capture feature interactions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/fusion-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/fusion-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/fusion-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/fusion.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 6: Adapted from Schouten et al. (2025), Fig. 6. Navigating the landscape of multimodal AI in medicine: A scoping review on technical challenges and clinical applications. ScienceDirect. https://doi.org/10.1016/j.artmed.2025.102368 </div> <p>The fusion approaches described in this blog post apply specifically to neural networks, the deep learning systems that dominate modern medical AI. Traditional machine learning approaches like random forests or support vector machines handle multiple data types differently, typically requiring manual feature engineering to combine modalities before training. Neural networks can learn to fuse information automatically through their layered structure.</p> <h5 id="concatenation">Concatenation</h5> <p>Concatenation stacks embeddings into a single unified representation. For example, an X-ray might be encoded into a 512-dimensional vector while a clinical report gets encoded into a 768-dimensional vector. Concatenation places them side by side into a single 1280-dimensional vector that downstream layers learn to interpret. This method dominates intermediate fusion research and appears in roughly 69% of these models <d-cite key="schouten2025navigating"></d-cite>. The encoders don’t explicitly learn to align the modalities in a shared space. Instead, subsequent layers learn to interpret the combined vector and extract cross-modal relationships useful for prediction.</p> <h5 id="attention-based-mechanisms">Attention-based Mechanisms</h5> <p>Attention-based fusion offers another approach used in about 12% of intermediate fusion models <d-cite key="schouten2025navigating"></d-cite>. Here, the model learns to dynamically weight different modalities based on their relevance to each specific prediction. For one patient, the imaging might be most informative. For another, the clinical history carries more weight. Attention mechanisms let the model decide which information source deserves emphasis. Unlike concatenation, attention-based fusion lets representations remain distinct while querying each other.</p> <h5 id="projection-into-shared-embedding-space">Projection into Shared Embedding Space</h5> <p>More sophisticated architectures create a shared embedding space where different modalities can be directly compared. Each encoder still produces its initial embedding in its own modality-specific space. But then a projection step maps all embeddings into a common geometric space where the model learns alignments. For instance, a chest X-ray showing pneumonia and its radiology report describing fluid in both lungs would be positioned close together in this space. Meanwhile, that same X-ray would sit far from a report describing a normal chest. In the shared space, distance encodes semantic similarity, meaning that concepts with related medical meanings occupy nearby positions. Related conditions cluster together. Unrelated findings stay far apart.</p> <h5 id="other-fusion-strategies">Other Fusion Strategies</h5> <p>Other approaches, like outer products or bilinear pooling, create joint representations that capture feature combinations across modalities. Rather than simply placing features side by side, these methods encode pairwise interactions. For example, if one modality detects a hazy pattern on chest imaging and another notes elevated inflammatory markers, the joint representation can encode not just both features independently but their co-occurrence as a meaningful pattern. This captures relationships like “feature A from imaging combined with feature B from labs” that simple concatenation might miss.</p> <h5 id="beyond-intermediate-fusion">Beyond Intermediate Fusion</h5> <p>The intermediate fusion approaches described above dominate in the multimodal medical AI space. But other timing strategies exist. In late fusion, each modality gets its own complete model that makes independent predictions, and the final predictions of each model are combined, often through simple averaging or by training a separate model on top of the individual predictions. This prevents modalities from teaching each other during learning, but makes handling missing data easier since each model trains independently.</p> <p>In early fusion, modalities combine before any encoding happens. This approach faces a fundamental obstacle in that raw data from different sources exists in incompatible formats. A chest X-ray arrives as a grid of pixels, while lab values are structured numbers, and clinical notes are text sequences. Some researchers solve this by embedding clinical variables directly into images or building graph networks that connect different data types from the start.</p> <hr> <h2 id="part-2-explainability-techniques-for-probing-multimodal-systems">Part 2: Explainability Techniques for Probing Multimodal Systems</h2> <p>Encoders and fusion strategies determine how models can integrate information across modalities. But architecture is only half the story. What matters is what the trained model actually learned. Understanding what a model has actually learned requires probing different aspects of how the model processes and integrates information. Researchers have adapted explainability techniques for unimodal models to probe multimodal systems. These approaches fall into several categories, which we explore below.</p> <h3 id="visual-attribution-methods">Visual Attribution Methods</h3> <p>Visual attribution methods answer a basic question: which parts of an image actually drove the model’s prediction? The technique produces heatmaps where bright regions show areas that influenced the output and dark regions show areas the model ignored.</p> <p>Gradient-weighted Class Activation Mapping (Grad-CAM) is a common visual attribution method used in medical AI. Grad-CAM builds on earlier Class Activation Mapping (CAM) methods introduced by Zhou et al. in 2016 <d-cite key="selvaraju2017gradcam"></d-cite>, which highlighted discriminative image regions by weighting feature maps from the final convolutional layer. CAM saw early adoption for tasks like weakly-supervised nodule segmentation and lesion localization, but it required specific architectural constraints, including global pooling followed by a fully connected layer. Grad-CAM, introduced by Selvaraju et al. in 2017 <d-cite key="selvaraju2017gradcam"></d-cite>, removed these restrictions by using gradients of the class score with respect to feature maps instead of requiring weights from a fully connected layer. Most medical image encoders use convolutional neural networks (CNNs), which process images through stacked layers that detect increasingly complex patterns. Grad-CAM exploits this layered structure by flowing gradients backward through these convolutional layers to identify which image regions increased or decreased confidence in the prediction.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/grad_cam-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/grad_cam-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/grad_cam-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2025-10-16-why-did-the-ai-say-that/grad_cam.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 7: Gradient-weighted Class Activation Mapping (Grad-CAM) </div> <p>Because Grad-CAM works with any CNN architecture, it has been widely adopted across medical imaging research. In 2021, researchers studying diabetic retinopathy <d-cite key="huff2021interpretation"></d-cite> used Grad-CAM to verify that models correctly identified hemorrhages and microaneurysms rather than camera artifacts. Some researchers have also applied Grad-CAM analysis to attention modules within their models. One study used this approach to predict microvascular invasion, a condition where cancer cells spread into small blood vessels surrounding a liver tumor. By generating heatmaps from the model’s attention layers, researchers revealed that the model focused on the edges of tumors and the tissue immediately surrounding them.</p> <p>Other gradient-based methods, like integrated gradients, offer complementary approaches. Integrated gradients compute how much each input pixel would need to change to alter the prediction by considering the path from a baseline image to the actual input.</p> <h3 id="probing-learned-representations">Probing Learned Representations</h3> <p>Visual attribution reveals what image encoders learned to recognize. A complementary approach probes the representation space directly by analyzing the embeddings the model generates and the geometric relationships between them. These techniques reveal how embeddings cluster together, separate into distinct groups, and relate to one another in the space.</p> <p><strong>Dimensionality reduction:</strong> Dimensionality reduction provides the most direct window into representation space. Techniques like t-SNE, UMAP, and PCA compress high-dimensional embeddings into two or three dimensions that humans can visualize, preserving relative distances to show structure. They can be used to examine whether diseases form distinct clusters or continuous spectrums, how different conditions relate, and where individual cases sit within this landscape. One study of COVID-19 chest X-ray models <d-cite key="covid_cxr_umap"></d-cite> used UMAP to visualize the learned representations within the neural network. The researchers extracted the 256-dimensional output from the penultimate fully connected layer, which was the layer by which the network had already encoded diagnostically relevant patterns before final classification. Using UMAP for dimensionality reduction, they projected this high-dimensional latent space onto two dimensions, creating a visualization where proximity indicated similarity in the model’s internal representation. The visualization exposed the continuous structure the model had learned, revealing how the entire patient population was distributed across disease stages and how individual trajectories moved through this space during disease progression and recovery.</p> <p><strong>Nearest neighbor analysis:</strong> Nearest neighbor analysis is a crucial technique for understanding what features a model has actually learned by examining its representation space. This technique involves identifying which examples in the training data are closest to a given input when measured by distance metrics (such as Euclidean distance) in this learned representation space. Given a specific medical image, researchers can identify which other images sit closest in the embedding space by computing distances between their learned representations. These neighbors reveal what the model considers similar and provide insight into the features driving the model’s decisions.</p> <p>This approach has been applied as a post hoc explainability method in clinical settings, where similar patient retrieval uses k-nearest neighbor techniques to identify comparable cases from training data. For example, in a recent study on ICU mortality prediction <d-cite key="icu_mortality"></d-cite>, researchers fine-tuned a medical language model and applied k-nearest neighbor retrieval using cosine similarity to find the top-3 most similar patients based on their admission notes. When evaluated by 32 practicing clinicians, this exemplar-based approach demonstrated a greater ability to build trust compared to feature-based methods, as it mirrors how practitioners naturally rely on past experiences in clinical decision-making. Similarly, in medical imaging, another classification model called the Retrieval Augmented Medical Diagnosis System <d-cite key="retrieval_augmented"></d-cite> combines standard predictions with nearest neighbor-based retrieval of similar historical cases for breast ultrasound classification, achieving a 21% improvement in sensitivity while providing clinicians with interpretable reference points for understanding the model’s reasoning.</p> <p><strong>Probing classifiers:</strong> Probing classifiers work like diagnostic tests for models. They reveal what information is hiding inside a model’s internal representations. Researchers freeze a trained model and extract embeddings from one or more layers, then train a simple classifier on top of these embeddings to predict specific attributes or categories. If this simple classifier succeeds, the model’s representation space at that layer must already contain that information. Researchers typically use linear classifiers because they provide the clearest test of whether information is directly accessible in the embeddings, though some studies use slightly more complex probes. Researchers often probe multiple layers systematically to understand how different types of information emerge at different depths in the network. Earlier layers typically encode low-level features, while later layers encode higher-level concepts.</p> <p>Researchers have used probing classifiers to detect when models encode sensitive information. In one study, researchers trained simple classifiers to predict patient race and sex <d-cite key="race_sex_cxr"></d-cite> from chest X-ray embeddings in disease detection models. These classifiers succeeded with high accuracy, confirming that demographic information had been learned and encoded in the model’s representations.</p> <p>However, probing classifiers have an important limitation. While they confirm the presence of information in the embeddings, they cannot determine whether the model actually uses this information for its predictions.</p> <h3 id="cross-modal-attention-visualization">Cross-Modal Attention Visualization</h3> <p>Attention mechanisms have become central to modern multimodal architectures. These components learn to weigh different parts of the input based on relevance. In multimodal medical AI, attention determines how imaging features and text tokens interact. Visualizing these attention patterns reveals how models align information across modalities.</p> <p>Attention visualization works by extracting attention weights from the model’s fusion layers. These weights indicate how strongly the model connected each image region to each text token when making a prediction. The weights can be rendered as matrices or heatmaps. High attention between a lung opacity in an image and the word “consolidation” in a report suggests the model learned a meaningful clinical relationship.</p> <p>Research on breast cancer diagnosis demonstrates attention visualization’s power. One influential study <d-cite key="breast_cancer_attention"></d-cite> examined models that processed mammograms alongside BI-RADS structured reports. BI-RADS is a standardized radiology vocabulary that describes breast findings using specific descriptors like mass shape and margin characteristics. The researchers visualized attention weights between image regions and BI-RADS descriptors in the reports. The results showed clinically coherent alignments. When the text mentioned “irregular mass,” attention weights concentrated on the corresponding mass location in the mammogram. When reports described architectural distortion, attention was highlighted in the distorted tissue regions. The model had learned to ground abstract clinical language in concrete visual features. This alignment suggests the representation space encodes meaningful medical relationships rather than spurious correlations.</p> <h3 id="model-agnostic-methods">Model-Agnostic Methods</h3> <p>Some explainability techniques work regardless of model architecture. These model-agnostic methods treat the AI system as a black box. They only require the ability to query the model with inputs and observe outputs. Shapley Additive explanations provide a unified framework grounded in game theory. SHAP values represent each feature’s contribution to the prediction as the average marginal contribution across all possible feature coalitions. The math comes from Shapley values originally developed for cooperative games. Computing exact SHAP values requires exponentially many model evaluations, so practical implementations use approximations. DeepSHAP adapts the approach for neural networks by leveraging backpropagation. KernelSHAP provides a model-agnostic sampling method. These variants trade computational efficiency for exactness. Medical AI research has adopted SHAP extensively. A study examining a multimodal sepsis prediction model called DeepSEPS <d-cite key="deepseps"></d-cite> used SHAP to measure how lab values and vital signs contributed to predictions. The analysis revealed that for septic shock prediction, the model appropriately prioritized lactate as the most important feature, despite it being a sparse laboratory measurement. However, for general sepsis prediction (without shock), the model relied more heavily on readily available features like oxygen delivery type and heart rate, with lactate playing a secondary role. This demonstrates how SHAP can quantify feature importance and reveal how models weight different data sources depending on the specific prediction task, even within the same clinical domain.</p> <p>It is important to note that SHAP faces a fundamental limitation in multimodal settings. It explains individual features independently but struggles to capture cross-modal interactions. If a prediction emerges from the combination of an imaging finding with contradictory clinical text, feature-level attributions may not reveal this relationship. The method works best for identifying which modality dominates rather than explaining how modalities interact. This limitation means SHAP should complement rather than replace techniques specifically designed for multimodal analysis.</p> <h3 id="modality-ablation-studies">Modality Ablation Studies</h3> <p>Ablation studies offer the most direct way to measure what each modality contributes. The approach follows a simple protocol. Train the model with all modalities. Then systematically remove modalities and measure how performance changes. A large performance drop when removing imaging suggests the model relied heavily on visual information. Minimal change when removing clinical text suggests that modality contributed little.</p> <p>Medical researchers use ablation studies to validate that multimodal models actually benefit from multiple data sources. For example, the PneumoFusion-Net study <d-cite key="pneumofusion"></d-cite> used ablation experiments to quantify each modality’s contribution. When classifying pneumonia from CT images, clinical text, numerical lab data, and imaging reports, researchers found that CT image features contributed approximately 45% to the decision-making process, clinical text 12%, numerical data 33%, and imaging reports contributed to the remaining percentage. Removing numerical data resulted in the largest drop in accuracy, indicating that biomarkers were highly critical for disease classification.</p> <p>Ablation studies also reveal redundancy across modalities. Some research on cardiac risk prediction found that models achieved similar performance with either echocardiogram videos or structured clinical measurements. The modalities contained overlapping information about cardiac function. This suggested that in clinical deployment, either modality alone might suffice when the other is unavailable.</p> <p>The limitation is that ablation studies only measure the overall contribution. They don’t reveal what specific features within each modality the model uses or how modalities interact during fusion. A modality might be essential overall, while the model still ignores important information within that modality. Ablation studies work best when combined with other techniques that provide finer-grained analysis. Together, these methods build a comprehensive picture of what multimodal medical AI systems actually learned and whether that learning reflects genuine medical understanding.</p> <hr> <h2 id="part-3-current-challenges-and-gaps">Part 3: Current Challenges and Gaps</h2> <p>Multimodal medical AI faces several major challenges that limit its progress and real-world usability.</p> <h3 id="dataset-scarcity">Dataset Scarcity</h3> <p>A fundamental obstacle is the lack of large, paired datasets. Collecting multimodal medical data is difficult because each case must include both imaging and text components, such as an X-ray and its radiology report. Hospitals rarely release complete datasets, and many patient records are missing one modality. Because of this scarcity, most research teams rely on pretrained image and text encoders that were originally built for general data rather than medical data. This shortcut saves time but introduces gaps in domain-specific understanding.</p> <h3 id="general-purpose-encoders">General-Purpose Encoders</h3> <p>That reliance on general-purpose encoders creates another issue. Models trained on everyday images and common text sources struggle to interpret the subtle patterns and precise language of medicine. The image encoder may recognize shapes and textures but not specific clinical abnormalities, while the text encoder may misinterpret medical abbreviations or context. As a result, these models can combine data streams effectively but do not truly understand either modality in the way a clinician does.</p> <h3 id="shallow-fusion-strategies">Shallow Fusion Strategies</h3> <p>Fusion presents its own difficulties. Early systems used late fusion, merging outputs only after each modality was processed independently. This design is simple but limits cross-modal learning. Intermediate fusion through concatenation is more common today but still often too shallow to capture complex relationships. Attention-based fusion offers a promising alternative by allowing the model to prioritize the most informative data source for each case, yet these methods are still inconsistent and not fully optimized for clinical reliability.</p> <h3 id="missing-data-handling">Missing Data Handling</h3> <p>Finally, the missing data problem remains one of the toughest barriers. Real-world medical records are messy and incomplete, but most multimodal models can only handle fully paired inputs. In practice, this forces researchers to exclude incomplete cases, reducing the amount of usable data and making deployment unrealistic. Some recent models attempt to adapt dynamically when one modality is missing, but the majority still fail in these situations.</p> <p>Multimodal medical AI remains a promising field with clear potential to enhance clinical reasoning and decision support, but it is not yet ready for standard clinical deployment. Until issues like data scarcity, missing modalities, and fusion reliability are addressed, these systems will remain closer to experimental research tools than dependable components of routine medical care.</p> <hr> <h2 id="conclusion-from-explanation-to-understanding">Conclusion: From Explanation to Understanding</h2> <p>Multimodal AI in medicine has proven its performance value. Recent research shows substantial gains from combining radiology images with clinical text. But performance alone isn’t enough for deployment.</p> <p>Through this tutorial, we’ve seen how explainability techniques from attention visualization to gradient-based methods serve dual purposes: they make individual models trustworthy AND provide tools for studying representation learning across models.</p> <p>The question of whether different multimodal medical AI systems converge on similar representations and what that convergence means remains largely unexplored. But the methods we’ve discussed offer a path forward: using attention visualization, attribution methods, and embedding analysis to empirically study representation similarity across models, architectures, and training paradigms.</p> <p>The fusion layer gives us a natural window into multimodal reasoning. Early fusion architectures make this window larger and clearer. Now it’s time to look through that window not just at individual models, but to compare what different models see and understand what representation convergence tells us about building reliable, trustworthy medical AI systems.</p> <hr> <h2 id="references">References</h2> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026-iclr-blogpost-earth4d/assets/bibliography/2026-04-27-why-did-the-model-say-that.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/ppo-batch-size/">The Trade-off Between Parallel Environments and Steps in PPO</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/agent-evaluation/">A Hitchhiker's Guide to Agent Evaluation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <br> <br> <div id="giscus_thread"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'qhuang62/2026-iclr-blogpost-earth4d',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026-iclr-blogpost-earth4d/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-data.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>