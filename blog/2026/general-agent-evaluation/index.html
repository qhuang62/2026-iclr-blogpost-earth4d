<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ready For General Agents? Let's Test It. | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="General-purpose agents are emerging, promising seamless deployment across domains. However, we currently do not measure their adaptability to diverse, unseen settings—a core requirement for true generality. We outline the key challenges and chart a path toward a unified evaluation framework designed to guide the development of general agents."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026-iclr-blogpost-earth4d/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/general-agent-evaluation/"> <script src="/2026-iclr-blogpost-earth4d/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/template.v2.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Ready For General Agents? Let's Test It.",
            "description": "General-purpose agents are emerging, promising seamless deployment across domains. However, we currently do not measure their adaptability to diverse, unseen settings—a core requirement for true generality. We outline the key challenges and chart a path toward a unified evaluation framework designed to guide the development of general agents.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026-iclr-blogpost-earth4d/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/" rel="external nofollow noopener" target="_blank"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener" target="_blank">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Ready For General Agents? Let's Test It.</h1> <p>General-purpose agents are emerging, promising seamless deployment across domains. However, we currently do not measure their adaptability to diverse, unseen settings—a core requirement for true generality. We outline the key challenges and chart a path toward a unified evaluation framework designed to guide the development of general agents.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#preliminary">Preliminary</a> </div> <div> <a href="#the-shift-to-general-agents">The Shift to General Agents</a> </div> <div> <a href="#the-promise-of-general-agents">The Promise of General Agents</a> </div> <ul> <li> <a href="#case-1-scientific-agents">Case 1 - Scientific Agents</a> </li> <li> <a href="#case-2-swe-agents">Case 2 - SWE Agents</a> </li> <li> <a href="#the-promise">The Promise</a> </li> </ul> <div> <a href="#state-of-general-agent-evaluation">State of General Agent Evaluation</a> </div> <ul> <li> <a href="#level-1-agentic-skills-evaluation">Level 1 - Agentic Skills Evaluation</a> </li> <li> <a href="#level-2-domain-agent-evaluation">Level 2 - Domain-Agent Evaluation</a> </li> <li> <a href="#level-3-agentic-cross-model-evaluation">Level 3 - Agentic Cross-Model Evaluation</a> </li> <li> <a href="#level-4-protocol-centric-agent-evaluation">Level 4 - Protocol-Centric Agent Evaluation</a> </li> <li> <a href="#level-5-general-agent-evaluation">Level 5 - General Agent Evaluation</a> </li> </ul> <div> <a href="#challenges-in-general-agent-evaluation">Challenges in General Agent Evaluation</a> </div> <ul> <li> <a href="#lack-of-standardized-agent-interface">Lack of Standardized Agent Interface</a> </li> <li> <a href="#lack-of-standardized-environment-interfaces">Lack of Standardized Environment Interfaces</a> </li> <li> <a href="#lack-of-a-standardized-researcher-interface">Lack of a Standardized Researcher Interface</a> </li> </ul> <div> <a href="#existing-agent-environment-protocols-insufficiency">Existing Agent-Environment Protocols insufficiency</a> </div> <div> <a href="#general-agent-evaluation-framework">General Agent Evaluation Framework</a> </div> <ul> <li> <a href="#a-meta-protocol-for-general-agent-evaluation">A Meta-Protocol for General Agent Evaluation</a> </li> <li> <a href="#core-characteristics-of-a-general-agent-evaluation-framework">Core Characteristics of a General Agent Evaluation Framework</a> </li> </ul> <div> <a href="#conclusions">Conclusions</a> </div> </nav> </d-contents> <p>Over the past few years, the NLP community has shifted from building domain-specific systems - such as standalone summarization or translation models - toward developing general-purpose language models <d-cite key="brown2020languagemodelsfewshotlearners"></d-cite><d-cite key="bommasani2022opportunitiesrisksfoundationmodels"></d-cite>. Many see this trend as a contemporary example of Richard Sutton’s “bitter lesson” <d-cite key="sutton2019bitter"></d-cite>:</p> <blockquote> <p>“The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.” This transition was not abrupt. It emerged from a long sequence of incremental advances that gradually expanded the scope and capability of models. Along the way, domain-specific solutions and increasingly general methods coexisted, each informing and accelerating the other.</p> </blockquote> <p>In this blog post, we argue that a similar shift is now unfolding in the field of AI agents: the field is moving from domain-specialized agents toward increasingly general-purpose ones. Agents that can address diverse types of multi-step tasks across different target domains and previously unseen environments.</p> <p>This development highlights the need for a unified evaluation framework for general-purpose agents that assesses their abilities across environments and compares different architectures. Such a framework is crucial for tracking progress, identifying gaps, and guiding the development of next-generation general agents. It is also essential for evaluating the core of generality itself: an agent’s ability to integrate into new environments and perform successfully.</p> <p>We begin by introducing a shared terminology for discussing domain and general agents, their evaluation, and the agent-to-environment communication protocol (<a href="#preliminary">Sec.2</a>). Next, we describe how today’s domain agents are evolving toward greater generality and outline the effect we expect it to have on general agents (<a href="#the-shift-to-general-agents">Sec.3</a>). The benefits of general-purpose agents and their advantages through two representative use cases follow in <a href="#the-promise-of-general-agents">Sec.4</a>, motivating the need for evaluation solutions that can assess general agent capabilities. For that end, we survey the current landscape of agent evaluation, presenting a five-level taxonomy (<a href="#state-of-general-agent-evaluation">Sec.5</a>), and detail the limitations that make existing approaches insufficient for easily evaluating general agents (<a href="#challenges-in-general-agent-evaluation">Sec.6</a>). We then assess whether existing agentic protocols can address these limitations (<a href="#existing-agent-environment-protocols">Sec.7</a>). Finally, we outline key requirements that a general agent solution needs to fulfill (<a href="#general-agent-evaluation-framework">Sec.8</a>).</p> <p>We hope this blog will help clarify what general agents are, increase awareness of their emergence from domain-specific agents, and highlight the gaps in their evaluation. More broadly, we aim for it to serve as a call to action that inspires a community-wide effort to develop rigorous, scalable, and actionable evaluation frameworks. We believe that as agents become more general and autonomous, such frameworks are essential to guide their progress.</p> <h2 id="preliminary">Preliminary</h2> <p>AI agents are autonomous, goal-driven systems that perform multi-step tasks by interacting with their environment <d-cite key="bandi2025rise"></d-cite>. The environment is the “world” the agent is situated in. The agent can observe and interact with the environment, obtaining observations according to its internal mechanism <d-cite key="cheng2024exploringlargelanguagemodel"></d-cite>. An agent deployed in the environment can interact with it to achieve its goal. Many of those terms were adopted and adjusted from the domain of reinforcement learning to the field of LLM and AI agents.</p> <p>AI agents are systems composed of interacting algorithmic components for reasoning, planning, memory preservation, code execution, and more. The orchestration of these components, often referred to as the agent’s architecture or scaffold, collectively determines the agent’s behavior. A large language model typically serves as the central computational element, providing core capabilities for perception, reasoning, and generation. The agent can also have access to external capabilities like code execution and search.</p> <p>Another important concept is the agent-to-environment communication protocol. This protocol interface describes the way the agent interacts with its environment. Main examples are web browsing, terminal, MCP, and tool schemas.</p> <p>Most current agents are being developed with a specific domain in mind <d-cite key="Wang_2024"></d-cite><d-cite key="yehudai2025surveyevaluationllmbasedagents"></d-cite>. Common examples are web agents and software engineering agents (SWE agents). In such cases, the agent is restricted to a relevant set of components and tools, and the environment is tailored to the target domain.</p> <p>To evaluate the capabilities of different domain agents, researchers defined domain-specific benchmarks. Such benchmarks require an environment in which the agent can operate, a set of tasks that describe the agent’s goal, and a metric that measures whether the agent has achieved its task. These benchmarks enable the assessment of the efficacy of domain agents and the comparison of different backbone LLMs.</p> <p>In contrast to these types of agents, general-purpose agents are designed to handle a diverse set of tasks across different environments. This requires them to be adaptable to different kinds of previously unseen environments, each with its own tools, requirements, and specific setup.</p> <p>In essence, general agents are defined by their capacity to integrate into new problem spaces, absorb their domain knowledge, refine their behavior through interaction, and ultimately master the tasks they encounter.</p> <h2 id="the-shift-to-general-agents">The Shift to General Agents</h2> <p>Large language models (LLMs) are general-purpose systems; they are designed to handle diverse tasks. Yet they have a fixed static knowledge of the world and can only interact with it by producing text. To overcome this challenge, researchers advise utilizing tools that allow LLMs to interact with the world (e.g., searching the web, running code). To further advance this ability, researchers also develop designed patterns such as ReAct <d-cite key="yao2023reactsynergizingreasoningacting"></d-cite> and CodeAct <d-cite key="wang2024executablecodeactionselicit"></d-cite> that facilitate a loop of interaction between the LLM and the environment. Such simple agents can be deployed in any environment to achieve multi-step tasks, making them early versions of general agents.</p> <p>Although these agents are simple, equipping them with the right tools can provide an effective solution. For example, many providers now recommend loop-based agents or agent frameworks as a standard pattern for application development. Even massively used LLM interfaces for both consumer and developer have quietly evolved from a conversation with an LLM to an interaction with an agent equipped with tools for coding and searching.</p> <p>On the other hand, such agents fall short when compared to more complex and specialized agents on target domains. Such domain agents are utilizing much more structure; they tend to have components for planning, memory, state tracking, tool use, and error handling to support reliable, iterative interaction with the domain environment. They are topping domain-specific leaderboards and providing real-world value. For example, SWE agents are already solving millions of GitHub issues without intervention <d-cite key="PRArena"></d-cite>, and deep research agents are being deployed to millions of users <d-cite key="McKay2025_OpenAIDeepResearch"></d-cite>.</p> <p>While different domain agents are by design different from one another, they share similar components. If we look at different SWE agents, such as Claude Code, Codex-cli, and different deep research agents such as OpenAI and Perplexity ones, they are all sharing similar algorithmic components <d-cite key="bgauryy_open-docs_2025"></d-cite><d-cite key="langchain_ai_open_deep_research_2025"></d-cite>. Moreover, each of those components is not specific to its domain, but is a general component that could be used for any target domain. As a result, such domain agents that rely on general components can be easily adopted to other domains and tasks. For example, recently, Anthropic published: “Over the past several months, Claude Code has become far more than a coding tool. At Anthropic, we’ve been using it for deep research, video creation, and note-taking, among countless other non-coding applications. In fact, it has begun to power almost all of our major agent loops.” Additionally, they released their Claude Agent SDK to serve as building blocks for developing other agents <d-cite key="anthropic_claude_agent_sdk_2025"></d-cite>. This demonstrates that domain-specific agents are becoming more proficient and general, allowing them to target a wider set of tasks. This also shows the new type of general agents, composed of plugable general components that can apply to a diverse set of domains across different environments.</p> <h2 id="the-promise-of-general-agents">The Promise of General Agents</h2> <p>In all machine learning tasks, simple and effective solutions are better than their specialized counterparts. Such solutions generalize better, are more robust, and less prone to overfitting <d-cite key="shalev2014understanding"></d-cite><d-cite key="haussler1990andrzej"></d-cite>. Specifically, unlike domain agents that can be over-specialized and tailored to a specific task or domain, general agents work with different environments, forcing them to generalize. They receive diverse signals from a wide range of environments, requiring them to be robust. As a result, they tend to be more cost-effective.</p> <p>Next, we examine two concrete examples of SWE and scientific agents that demonstrate that even simple versions of general agents have a lower cost of development and are more cost-effective. To quantify these qualities, we measure lines of code (LOC) and agent average cost per task.</p> <h3 id="case-1-scientific-agents">Case 1: Scientific Agents</h3> <p>ASTA Bench <d-cite key="bragg2025astabenchrigorousbenchmarkingai"></d-cite>, an effort towards benchmarking deep scientific research agents, provides a clear test. As shown in Table 1, the specialized ASTA-v0 system scores 55% at $3.40 per task and contains subsystems exceeding 13,000 lines of code (LOC)<sup id="fnref:asta-loc"><a href="#fn:asta-loc" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. Yet the second-best system is a 300-line ReAct general agent scoring 44% at just $0.31. On the literature-understanding subtask, ReAct scores 53%; although still below the Asta agent (62%) it is still outperforming both the specialized ASTA Paper Finder (21%) and OpenAI Deep Research (19%).</p> <hr> <table> <thead> <tr> <th>Agent</th> <th>LLMs used</th> <th>ASTA score</th> <th>Cost per task</th> <th>LOC</th> </tr> </thead> <tbody> <tr> <td>ASTA-v0</td> <td>Claude 4 Sonnet, Gemini 2.5 Flash, O3, GPT 4.1, GPT-4o</td> <td>53%</td> <td>$3.40</td> <td>&gt;13,768<sup id="fnref:asta-loc:1"><a href="#fn:asta-loc" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> </td> </tr> <tr> <td>ReAct</td> <td>GPT-5</td> <td>44%</td> <td>$0.31</td> <td>358<sup id="fnref:react-code"><a href="#fn:react-code" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> </td> </tr> </tbody> </table> <blockquote> <p>Table 1: In scientific tasks, a tiny general agent (ReAct) approaches the performance of a large specialized system (ASTA-v0) at a fraction of the cost and complexity.</p> </blockquote> <hr> <h3 id="case-2-swe-agents">Case 2: SWE Agents</h3> <p>In SWE-Bench <d-cite key="yang2025swesmith"></d-cite>, the specialized SWE-Agent scores 67%, but the tiny, domain-agnostic Mini SWE-Agent scores 65% while being 30 times smaller and about 7 times cheaper per run (Table 2). Across both scientific and SWE settings, small general agents of a few hundred lines consistently achieve 70% to 95% of the performance of systems that are thousands of lines.</p> <hr> <table> <thead> <tr> <th>Agent</th> <th>LLM</th> <th>SWE-Bench score</th> <th>Cost per task</th> <th>LOC</th> </tr> </thead> <tbody> <tr> <td>SWE-Agent</td> <td>Claude 4 Sonnet</td> <td>67%</td> <td>~$2.50</td> <td>4,161</td> </tr> <tr> <td>Mini SWE-Agent</td> <td>Claude 4 Sonnet</td> <td>65%</td> <td>$0.37</td> <td>131</td> </tr> </tbody> </table> <blockquote> <p>Table 2: In software engineering tasks, a minimal general agent nearly matches a specialized SWE agent while being far smaller and cheaper.</p> </blockquote> <hr> <h3 id="the-promise">The Promise</h3> <p>As general agents mature, they will become more complex. Yet they hold great promise. Building a general agent can provide a singular solution applied to a wide range of cases. This can make such an effort much more impactful, similar to how OpenAI was able to surpass many task-specific solutions by providing a general-purpose LLM. It also provides a lower cost of development compared to building many domain-specific ones. Additionally, it reduces the development time required for building domain-specific agents, as they can start from a general agent and further adapt it to their target domain, similar to fine-tuning a general-purpose LLM for a specific task.</p> <h2 id="state-of-general-agent-evaluation">State of General Agent Evaluation</h2> <p>There is a shift in the field of AI agents from domain-specific agents to general-purpose ones, and a general agent can be easier to develop compared to many domain-specific agents, more robust, and more cost-effective. This shift raises the need for evaluation solutions that are suitable for evaluating a general agent. This requires a framework that enables running the same agent across different benchmarks and environments to evaluate adaptability. Yet, there is no such solution.</p> <p>To address this gap, we start by organizing the agent evaluation solutions space into five levels, starting from the specific to the more general. This organization can help us outline the fifth level that describes the missing level for evaluating any general agent across environments.</p> <h3 id="level-1-agentic-skills-evaluation">Level 1: Agentic Skills Evaluation</h3> <p>The first level focuses on evaluating LLMs on agentic skills, such as reasoning, planning, and tool calling, without embedding them in a dynamic environment. Benchmarks in this level provide a textual prompt and expect a textual response. The model’s response is assessed independently of any interaction loop or adaptive environment. Consequently, these evaluations measure whether a model can demonstrate an agentic capability in principle, but they do not assess whether the model can deploy that capability reliably in realistic long-horizon tasks. Nonetheless, they can provide insight into the ability of a model to succeed in certain components.</p> <p>Representative benchmarks of this level include GSM8K <d-cite key="cobbe2021trainingverifierssolvemath"></d-cite>, which evaluates step-by-step mathematical reasoning; HotPotQA <d-cite key="yang2018hotpotqadatasetdiverseexplainable"></d-cite>, which tests multi-hop question answering; and BFCL <d-cite key="patil2023gorilla"></d-cite>, which measures tool-use capabilities. They were the basis for the reasoning and action design pattern, ReAct, on multi-step tasks. Together, they exemplify evaluations that probe fundamental agentic skills but stop short of testing genuine agentic behavior.</p> <h3 id="level-2-domain-agent-evaluation">Level 2: Domain-Agent Evaluation</h3> <p>The second level uses interactive environments, such as web browsers, applications, or terminal interfaces, with a set of tools that allow the agent to interact with them. The agent gets tasks requiring multiple steps that it needs to perform by interacting with the environment. The agent sequence of LLM and tool calls, named the agent trajectory, is then evaluated by an environment-specific metric that assesses whether the agent task was achieved.</p> <p>These benchmarks provide high value for assessing domain-specific agent capabilities. However, each benchmark often defines its own custom setup, leading to agent logic that is tightly coupled to each environment. As a result, some benchmarks are used to assess LLMs in agentic environments, while others require agents tailored to the specific benchmark, making it hard to compare the same agent across benchmarks.</p> <p>Representative examples include Tau-Bench <d-cite key="yao2024tau"></d-cite><d-cite key="barres2025tau2"></d-cite> for customer-service scenarios, AppWorld <d-cite key="trivedi-etal-2024-appworld"></d-cite> for multi-application tasks, WebArena <d-cite key="zhou2024webarenarealisticwebenvironment"></d-cite> for browser interactions, TerminalBench <d-cite key="tbench_2025"></d-cite> for Linux command-line tasks, and SWE-Bench <d-cite key="yang2025swesmith"></d-cite> for solving GitHub issues.</p> <h3 id="level-3-agentic-cross-model-evaluation">Level 3: Agentic Cross-Model Evaluation</h3> <p>The third level focuses on providing a standardized evaluation harness for reproducible agent evaluations across various domain-specific benchmarks. It can be used for comparing LLMs as the backbone of different agents, or to compare domain-specific agents on a single benchmark. Yet, their standardization does not allow running the same agent across different benchmarks.</p> <p>A representative example is HAL <d-cite key="hal"></d-cite> compiled nine benchmarks from the second level. Each environment in HAL comes with its own fixed agent setup, and users can easily change the backbone model of an agent or add support to a new domain-specific agent. Hence, this level still does not support comparing different agent architectures across environments as needed for general agent assessment.</p> <h3 id="level-4-protocol-centric-agent-evaluation">Level 4: Protocol-Centric Agent Evaluation</h3> <p>The fourth level moves beyond fixed agent setups by defining a standardized interaction protocol, such as a unified browser API or terminal interface, that any agent can implement. Instead of each environment defining its own custom agent-to-environment communication protocol, this standardization forces the agent to follow a specific protocol to be consistently evaluated across many environments. This enables comparisons not just across models, but also across different agent architectures.</p> <p>However, protocol-centric frameworks still impose a specific mode of interaction. Agents built around fundamentally different communication protocols, such as those using the Model Context Protocol (MCP), cannot be evaluated in their native form. They must be forced through the protocol’s interface, which can obscure their design and distort performance. For example, Harbor <d-cite key="tbench_2025"></d-cite> evaluates agents through a command-line protocol, preventing MCP-based agents like Claude Code from being tested as intended.</p> <p>Representative examples include BrowserGym <d-cite key="chezelles2025browsergym"></d-cite>, which standardizes browser interaction across diverse web tasks, and Harbor <d-cite key="tbench_2025"></d-cite>, which provides a unified terminal protocol.</p> <h3 id="level-5-general-agent-evaluation">Level 5: General Agent Evaluation</h3> <p>A fifth level, still missing today, would provide a framework for general agent evaluation. It will enable the evaluation of the same agent across environments without being forced to use a specific communication protocol. It would reveal how an agent actually performs in realistic settings, how design choices influence outcomes, and how well agents generalize across diverse tasks and environments. Achieving this level of flexibility and fairness remains an open challenge, and the lack of a unified, protocol-agnostic evaluation paradigm is a central barrier to progress toward genuinely general-purpose agents.</p> <hr> <table> <thead> <tr> <th>Level</th> <th>Cross-model</th> <th>Agentic environment interaction</th> <th>Cross-environment</th> <th>Cross-agent</th> <th>Protocol-agnostic</th> <th>Examples</th> </tr> </thead> <tbody> <tr> <td>1: Agentic model skills</td> <td>Yes</td> <td>No</td> <td>No</td> <td>No</td> <td>No</td> <td>BFCL, GSM8K, HotPotQA</td> </tr> <tr> <td>2: Interactive agentic model</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>No</td> <td>No</td> <td>Tau-Bench, AppWorld, WebArena, TerminalBench</td> </tr> <tr> <td>3: Cross-model harness</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>No</td> <td>HAL</td> </tr> <tr> <td>4: Protocol-centric</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>BrowserGym, Harbor</td> </tr> <tr> <td>5: General agent evaluation</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>(missing)</td> </tr> </tbody> </table> <blockquote> <p>Table 3: Comparison of the five levels of agent evaluation, highlighting cross-model, cross-environment, cross-agent, and protocol coverage.</p> </blockquote> <hr> <h2 id="challenges-in-general-agent-evaluation">Challenges in General Agent Evaluation</h2> <p>Benchmarking general agents in different benchmarking environments is challenging; existing benchmarks were typically designed with a specific agent domain and goal in mind, such as a user-facing conversational agent, a computer-using autonomous coder, or a web-navigation agent. These design choices led to incompatible setups, preventing the development of a single, unified evaluation protocol that any agent can be seamlessly integrated into. Below, we outline key challenges that must be resolved to provide a unified standard for general agent evaluation.</p> <h3 id="lack-of-standardized-agent-interface">Lack of Standardized Agent Interface</h3> <p>Many benchmarks implicitly assume the tested agent possesses certain built-in, domain-specific capabilities. For example:</p> <ul> <li>Tau-Bench <d-cite key="yao2024tau"></d-cite><d-cite key="barres2025tau2"></d-cite> assumes an agent that can inherently message or converse with a user:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BaseAgent</span><span class="p">(</span><span class="n">ABC</span><span class="p">,</span> <span class="n">Generic</span><span class="p">[</span><span class="n">AgentState</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s">Base agent class that defines the common interface for agents.</span><span class="sh">"""</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">generate_next_message</span><span class="p">(...,</span> <span class="n">message</span><span class="p">:</span> <span class="n">UserMessage</span> <span class="o">|</span> <span class="n">ToolMessage</span> <span class="o">|</span> <span class="n">MultiToolMessage</span><span class="p">..)</span> <span class="o">-&gt;</span> <span class="p">...</span> <span class="n">AssistanceMessage</span><span class="p">:</span>
        <span class="bp">...</span>
</code></pre></div></div> <ul> <li>WebArena <d-cite key="zhou2024webarenarealisticwebenvironment"></d-cite> assumes an agent whose entire perceptual and action space is mediated through a browser interface controlled by predefined actions:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Base class for the agent</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">next_action</span><span class="p">(...,</span> <span class="n">trajectory</span><span class="p">:</span> <span class="n">Trajectory</span><span class="p">,...)</span> <span class="o">-&gt;</span> <span class="n">Action</span><span class="p">:</span>
        <span class="c1"># ActionType: NONE, SCROLL, KEY_PRESS, MOUSE_CLICK, KEYBOARD_TYPE, MOUSE_HOVER, CLICK, TYPE, HOVER,
</span>        <span class="c1"># PAGE_FOCUS, NEW_TAB, GO_BACK, GO_FORWARD, GOTO_URL, PAGE_CLOSE, ...
</span>        <span class="bp">...</span>
</code></pre></div></div> <ul> <li>Terminal Bench <d-cite key="tbench_2025"></d-cite> assumes an agent whose interaction interface is a computer with a command line:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BaseAgent</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">run</span><span class="p">(...,</span> <span class="n">environment</span><span class="p">:</span> <span class="n">BaseEnvironment</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">...</span>
<span class="k">class</span> <span class="nc">BaseEnvironment</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">exec</span><span class="p">(...,</span> <span class="n">command</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="p">...)</span> <span class="o">-&gt;</span> <span class="n">ExecResult</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Executes a command in the environment...</span><span class="sh">"""</span>
</code></pre></div></div> <p>These assumptions are mutually incompatible. A web-browsing agent cannot converse with a user, while a chat-oriented agent cannot click on a web element. Such rigid, benchmark-specific communication protocols make cross-environment evaluation impossible without substantial ad hoc engineering.</p> <hr> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <blockquote> <p>Figure 1: Illustration of agents in different benchmarks and their incompatibility with the environment constraints of other benchmarks. Tau-Bench assumes user messaging, which does not align with TerminalBench or WebArena.</p> </blockquote> <hr> <h3 id="lack-of-standardized-environment-interfaces">Lack of Standardized Environment Interfaces</h3> <p>Benchmarks rarely specify, in an agent-agnostic way, what task the agent must perform, what information it should receive, or what actions it can take and how those actions affect the environment. As a result, developers are often left to infer or invent these elements themselves.</p> <p>SWE-Bench <d-cite key="yang2025swesmith"></d-cite> exemplifies this lack of specification. Although the benchmark defines issues the agent should solve, it does not supply standard instructions on how the issues should be solved, how the solution will be validated, or how the submitted solution should be structured. Without an explicit environment interface for agents solving the benchmark, every user must design their own, de-facto creating different setups for different agents, making evaluations difficult to compare.</p> <p>Other benchmarks present the opposite issue: they do communicate environment semantics, but only in a form tailored to a specific agent architecture. Tau-Bench <d-cite key="yao2024tau"></d-cite><d-cite key="barres2025tau2"></d-cite> is a clear example. Instead of offering a standalone, environment-level description of how an agent should behave, key instructions appear only in the reference conversational LLM agent:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">AGENT_INSTRUCTION</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
You are a customer service agent that helps the user according to the &lt;policy&gt; provided below.
In each turn you can either:
- Send a message to the user.
- Make a tool call.
You cannot do both at the same time.

Try to be helpful and always follow the policy. Always make sure you generate valid JSON only.
</span><span class="sh">"""</span>
</code></pre></div></div> <p>While this instruction block expresses important aspects of the task, it mixes them with assumptions specific to a particular agent design-such as turn-by-turn interaction rules, usage of tools and JSON output formatting. These constraints make sense for a conversational LLM but do not generalize to other types of agents, such as code-act agents.</p> <p>Across both cases, the core issue is the same: current benchmarks do not provide a standardized, agent-agnostic interface that cleanly communicates the task, available information, and action space. Without such an interface, researchers are forced to do additional work to interpret or construct these components themselves. This, in turn, creates room for inconsistencies across agent evaluations and introduces integration errors. Finally, the lack of standardization prevents testing from scaling to many environments, which is essential for evaluating an agent’s ability to adapt to many different environments.</p> <h3 id="lack-of-a-standardized-researcher-interface">Lack of a Standardized Researcher Interface</h3> <p>Evaluating general agents across many environments requires more than a common agent-environment integration interface. To support seamless experimentation-without hours spent integrating each new environment-researcher-facing interfaces must also be standardized and simplified. Today, every environment demands its own setup, scripts, and output formats. Researchers repeatedly lose time to these inconsistencies and risk introducing avoidable integration errors.</p> <p>One example of this fragmentation is the process of collecting and interpreting results. Benchmarks report outcomes in different formats, store them in different locations, and follow different conventions. As shown in Table 4, even the basic metrics differ across three representative benchmarks-not only in which quantities are tracked, but also in terminology and aggregation standards used. A simple notion like success appears as a Boolean resolved flag in SWE-bench, a success field in AppWorld, and an implicit reward of 1 in Tau-bench. Interaction cost is reported as agent cost and user cost in Tau-bench but omitted entirely in AppWorld and SWE-bench.</p> <p>Even when benchmarks measure conceptually similar quantities, they adopt incompatible formats, naming conventions, and aggregation methods.</p> <p>These gaps underscore the need for consolidation and standardization to make large-scale evaluation of general agents feasible. This is not just a convenience - given the growing complexity of modern benchmarks, it is the only viable path to scalable general agent research.</p> <hr> <table> <thead> <tr> <th>Metric type</th> <th>Tau-Bench</th> <th>AppWorld</th> <th>SWE-Bench</th> </tr> </thead> <tbody> <tr> <td>Success (bool)</td> <td>Reward = 1</td> <td>success</td> <td>Resolved</td> </tr> <tr> <td>Score (float)</td> <td>Reward</td> <td>Score</td> <td>-</td> </tr> <tr> <td>Termination</td> <td>Termination reason</td> <td>-</td> <td>-</td> </tr> <tr> <td>Duration</td> <td>Duration</td> <td>-</td> <td>-</td> </tr> <tr> <td>Number of interactions</td> <td>Num messages</td> <td>Steps</td> <td>-</td> </tr> <tr> <td>Agent cost</td> <td>Agent cost</td> <td>-</td> <td>-</td> </tr> <tr> <td>Environment cost</td> <td>User cost</td> <td>-</td> <td>-</td> </tr> <tr> <td>Task ID</td> <td>Task ID</td> <td>Task ID</td> <td>Instance ID</td> </tr> <tr> <td>Logs</td> <td>Message log</td> <td>Task logs</td> <td>Test logs</td> </tr> <tr> <td>Success rate (benchmark)</td> <td>Avg reward</td> <td>Task goal completion</td> <td>Resolved counts</td> </tr> <tr> <td>Cost aggregate (benchmark)</td> <td>Avg agent cost</td> <td>-</td> <td>-</td> </tr> </tbody> </table> <blockquote> <p>Table 4: Metrics differ across benchmarks, with incompatible names and formats even for basic success and cost reporting.</p> </blockquote> <hr> <p>These gaps underscore the need for consolidation and standardization to make large-scale evaluation of general agents feasible.</p> <h2 id="existing-agent-environment-protocols-insufficiency">Existing Agent-Environment Protocols Insufficiency</h2> <p>Several protocols have recently emerged to standardize interaction, discovery, and data exchange in agentic systems. Some of these can, in principle, support evaluation. For example, A2A <d-cite key="a2a_protocol"></d-cite> provides a standardized way for agents to discover each other’s capabilities, negotiate interaction modes, and coordinate on collaborative tasks, which could inform how benchmarks deliver tasks to agents. MCP <d-cite key="anthropic_mcp_2024"></d-cite> defines a structured way for agents to access external tools, data resources, and prompt templates. In an evaluation setting, a benchmark could expose its environment via an MCP server.</p> <p>In this section, we ask two questions:</p> <ol> <li>Should the research community adopt a single protocol for general-agent evaluation?</li> <li>Do current protocols satisfy the practical needs of evaluation today?</li> </ol> <p>Regarding the first question, we caution against prematurely standardizing evaluation processes on a single protocol. Lock-in at this stage risks constraining innovation as agent capabilities and demands still evolve. More fundamentally, evaluation must remain capable of assessing everything-including the protocol itself-making early commitment counterproductive.</p> <p>To address the second question, we use MCP as a case study for whether existing protocols can support end-to-end evaluation workflows.</p> <p>MCP defines three core primitives: tools (invocable operations), resources (exposed data/content), and prompts (parameterized templates), along with mechanisms for streaming events and updates. While MCP provides a promising foundation for unifying agent-environment interaction, several gaps prevent it from serving as a complete solution for general-agent evaluation:</p> <ul> <li>Missing Support for Benchmark Task Semantics. Benchmarks center around tasks-the defined goal an agent is supposed to achieve. MCP does not offer a built-in way to represent or communicate such tasks. One could require that tasks always appear in either prompts, resources, or events, but doing so would essentially create a new protocol on top of MCP, showing that MCP by itself is not enough.</li> <li>Missing Support for Evaluation Workflows. Evaluation requires more than interaction; it depends on standardized metrics reporting, aggregation, logging, experiment tracking, and reproducibility. MCP is intentionally agnostic to these needs. While one could build evaluation workflows around MCP sessions, doing so would again amount to defining an additional protocol on top of MCP, rather than using MCP itself as the benchmarking standard.</li> <li>Inconsistent Ecosystem Adoption. Support for MCP remains uneven: many frameworks implement tool calling but not resources or prompts, resulting in inconsistent capabilities and substantial integration overhead (Table 5). This fragmentation makes it difficult for benchmark developers to ensure that agents can reliably interact with their environments, and equally hard for agent developers to obtain consistent baselines across benchmarks.</li> </ul> <hr> <table> <thead> <tr> <th>Agent / MCP Integration</th> <th>Tools</th> <th>Resources</th> <th>Prompts</th> </tr> </thead> <tbody> <tr> <td>Smolagents</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> <tr> <td>Llama Stack</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> <tr> <td>OpenAI Agents SDK</td> <td>Yes</td> <td>No</td> <td>Yes</td> </tr> <tr> <td>Codex CLI</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> <tr> <td>Claude Code</td> <td>Yes</td> <td>Yes</td> <td>No</td> </tr> </tbody> </table> <blockquote> <p>Table 5: MCP integration across common agent frameworks. In many cases, only partial protocol components are implemented.</p> </blockquote> <hr> <p>These limitations indicate that while protocols like MCP and A2A lay important groundwork, they do not yet meet the full requirements of standardized general-agent evaluation.</p> <p>Since no current protocol perfectly fits evaluation needs, the next section explores how a shared evaluation framework can still be built.</p> <h2 id="general-agent-evaluation-framework">General Agent Evaluation Framework</h2> <p>Advancing general-purpose agents requires an evaluation framework that is itself general: capable of supporting different environments, diverse agent architectures, and multiple communication protocols. Yet standardizing such evaluation is intrinsically difficult. Environments vary widely, implicit assumptions fragment the space, and existing protocols address only parts of the challenge. Even promising standards such as MCP provide useful building blocks but remain incomplete. These pressures motivate the need for a unifying layer that can support evaluating any agent on any benchmark without restriction to a specific communication protocol.</p> <h3 id="a-meta-protocol-for-general-agent-evaluation">A Meta-Protocol for General Agent Evaluation</h3> <p>At the core of such a framework is a meta-protocol: an abstract, protocol-agnostic layer that defines the semantics of evaluation independently of any concrete agent-environment protocol. Current benchmarks implicitly bind evaluation to a specific communication protocol, thereby entangling agent performance with protocol-specific setup.</p> <p>The meta-protocol needs to specify how tasks, actions, observations, documentation, and termination conditions are represented while allowing different communication protocols. Such a protocol can allow communication interfaces, such as web browsing, terminal, MCP, and tool schemas, without altering the evaluation semantics.</p> <p>By enabling protocol modularity rather than protocol uniformity, a meta-protocol allows agents to be evaluated in their native interaction modes and makes cross-protocol comparisons meaningful. Under this abstraction, the practical requirements of a general agent evaluation framework can be defined cleanly and without protocol-specific assumptions.</p> <h3 id="core-characteristics-of-a-general-agent-evaluation-framework">Core Characteristics of a General Agent Evaluation Framework</h3> <ul> <li>Environment-agnostic agent interface. The agent-facing interface should function across any environment. Environments expose a minimal, standardized schema for actions, observations, tasks, and documentation, with all assumptions explicit and discoverable rather than embedded in reference agents.</li> <li>Agent-agnostic environment interface. Environments should not assume a specific agent architecture, reasoning style, or protocol. Simple ReAct agents, memory-augmented agents, MCP-based agents, browser agents, and command-line agents should all integrate without modification.</li> <li>Plug-and-play modularity. Models, agent architectures, interaction protocols, and environments should be swappable without altering the evaluation protocol. This enables controlled comparisons across dimensions within a unified setup.</li> <li>Standardized reporting and transparency. The framework should define consistent reporting conventions, including metrics for success, robustness, interaction efficiency, and cost. Observability layer support for logging and full trajectory tracking.</li> </ul> <p>Such a framework would let us measure the core property that makes general agents uniquely valuable: adaptability to new environments with minimal re-engineering.</p> <h2 id="conclusions">Conclusions</h2> <p>Ultimately, an evaluation framework for general agents must measure the core capability that defines general agents: operating effectively in unfamiliar environments and solving tasks without environment-specific manual tuning. This adaptability enables agents to scale across domains and deliver broad practical value. A meta-level evaluation protocol is therefore essential-one that can allow testing any agent in any environment without intervention, ensuring fair and consistent comparison. Without such a framework, the field will have limited ability to reliably measure progress and to guide the development of general-purpose agents.</p> <hr> <figure class="text-figure"> <h3 id="checklist-for-general-agent-evaluation-framework">Checklist for General Agent Evaluation Framework</h3> <ul> <li> <strong>Environment-Agnostic Agent Interface</strong> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Works across any environment.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Standardized actions, observations, tasks, and documentation.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>No hidden assumptions or reference-agent logic.</li> </ul> </li> <li> <strong>Agent-Agnostic Environment Interface</strong> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Compatible with any agent architecture or control loop.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Neutral to reasoning style, memory, tools, and workflow.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Independent of protocol (MCP, browser API, CLI, etc.).</li> </ul> </li> <li> <strong>Strict Environment–Agent Separation</strong> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Environments expose only actions, observations, state semantics, and documentation.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>No strategy hints, behavioral instructions, or scaffolding.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>No environment helpers compensating for agent capabilities.</li> </ul> </li> <li> <strong>Discovery-Oriented Evaluation</strong> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Explicit mechanisms for discovering schema and domain knowledge.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Metrics for discovery efficiency and generalization.</li> </ul> </li> <li> <strong>Standardized Reporting and Transparency</strong> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Unified reporting protocol across environments.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Metrics for success, cost, efficiency, robustness, and discovery quality.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Full trajectory and action logging for reproducibility.</li> </ul> </li> <li> <strong>Cross-Dimensional Comparability</strong> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Supports comparisons across models, agents, environments, and protocols.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Separates model, architecture, protocol, and environment effects.</li> </ul> </li> <li> <strong>Efficiency and Practicality</strong> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Cost-aware evaluation with token and compute tracking.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Scalable execution (parallelism, batching, lightweight environments).</li> </ul> </li> <li> <strong>Extensibility and Future-Proofing</strong> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Modular integration of new environments and task families.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Agnostic to future agent protocols and architectural innovations.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Meta-protocol supports evaluation of emerging standards.</li> </ul> </li> </ul> </figure> <blockquote> <p>Figure 2: Checklist of requirements for a general agent evaluation framework.</p> </blockquote> <hr> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:asta-loc"> <p>Based on the python files in https://github.com/allenai/asta-paper-finder/tree/main/agents/mabool/api/mabool. <a href="#fnref:asta-loc" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:asta-loc:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> <li id="fn:react-code"> <p>Source: https://github.com/allenai/asta-bench/blob/f7e25392f4dda167f4e6d46b8c7c080eeeb4cc35/astabench/solvers/react/basic_agent.py. <a href="#fnref:react-code" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026-iclr-blogpost-earth4d/assets/bibliography/2026-04-27-general-agent-evaluation.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/ppo-batch-size/">The Trade-off Between Parallel Environments and Steps in PPO</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/agent-evaluation/">A Hitchhiker's Guide to Agent Evaluation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026-iclr-blogpost-earth4d/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-data.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>