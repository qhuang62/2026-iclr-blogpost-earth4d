<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Trade-offs in LLM Compute for Reasoning-Intensive Information Retrieval | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="The BRIGHT benchmark (ICLR 2025 Spotlight) revealed that reasoning-intensive information retrieval requires LLM-augmented pipelines, but this raises a critical resource allocation question: where should computational budget be invested for maximum effectiveness? We conduct a systematic study on BRIGHT using the Gemini 2.5 model family, evaluating trade-offs across model strength, inference-time thinking depth, and reranking depth. Our controlled experiments quantify the marginal gains of allocating compute to query expansion versus reranking, providing practical guidance for optimizing LLM-based retrieval systems on reasoning-intensive tasks."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026-iclr-blogpost-earth4d/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/trade-offs-in-llm-compute-for-reasoning-intensive-information-retrieval/"> <script src="/2026-iclr-blogpost-earth4d/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026-iclr-blogpost-earth4d/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/template.v2.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Trade-offs in LLM Compute for Reasoning-Intensive Information Retrieval",
            "description": "The BRIGHT benchmark (ICLR 2025 Spotlight) revealed that reasoning-intensive information retrieval requires LLM-augmented pipelines, but this raises a critical resource allocation question: where should computational budget be invested for maximum effectiveness? We conduct a systematic study on BRIGHT using the Gemini 2.5 model family, evaluating trade-offs across model strength, inference-time thinking depth, and reranking depth. Our controlled experiments quantify the marginal gains of allocating compute to query expansion versus reranking, providing practical guidance for optimizing LLM-based retrieval systems on reasoning-intensive tasks.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026-iclr-blogpost-earth4d/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026-iclr-blogpost-earth4d/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/" rel="external nofollow noopener" target="_blank"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener" target="_blank">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Trade-offs in LLM Compute for Reasoning-Intensive Information Retrieval</h1> <p>The BRIGHT benchmark (ICLR 2025 Spotlight) revealed that reasoning-intensive information retrieval requires LLM-augmented pipelines, but this raises a critical resource allocation question: where should computational budget be invested for maximum effectiveness? We conduct a systematic study on BRIGHT using the Gemini 2.5 model family, evaluating trade-offs across model strength, inference-time thinking depth, and reranking depth. Our controlled experiments quantify the marginal gains of allocating compute to query expansion versus reranking, providing practical guidance for optimizing LLM-based retrieval systems on reasoning-intensive tasks.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#background">Background</a> </div> <ul> <li> <a href="#reasoning-intensive-retrieval-and-bright">Reasoning-Intensive Retrieval and BRIGHT</a> </li> <li> <a href="#the-ir-pipeline-for-riir">The IR Pipeline for RIIR</a> </li> <li> <a href="#the-thinking-dimension">The "Thinking" Dimension</a> </li> </ul> <div> <a href="#experimental-setup">Experimental Setup</a> </div> <ul> <li> <a href="#model-suite">Model Suite</a> </li> <li> <a href="#evaluation-metrics">Evaluation Metrics</a> </li> </ul> <div> <a href="#experiments">Experiments</a> </div> <ul> <li> <a href="#scaling-compute-in-query-expansion-qe">Scaling Compute in Query Expansion (QE)</a> </li> <li> <a href="#scaling-compute-in-reranking-rr">Scaling Compute in Reranking (RR)</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-trade-offs-in-llm-compute-for-reasoning-intensive-information-retrieval/riir-llm-tradeoff-overview.png" alt="LLM compute Trade-offs in RIIR" style="width: 100%; max-width: 600px; display: block; margin: 0 auto;"> </figure> <p>The paradigm of Information Retrieval (IR) is undergoing a fundamental shift. While traditional IR focused on semantic similarity and keyword matching, modern applications increasingly demand Reasoning-Intensive Information Retrieval (RIIR). In these scenarios, as shown by the recent ICLR 2025 Spotlight paper <a href="https://arxiv.org/abs/2407.12883" rel="external nofollow noopener" target="_blank">BRIGHT</a><d-cite key="su2025bright"></d-cite> - a system cannot simply find a document that looks like the query; it must understand complex logic, synthesize constraints, and deduce relationships to identify the correct evidence.</p> <p>The BRIGHT benchmark established that standard retrieval methods (both sparse and dense) struggle significantly with these tasks. However, it also highlighted a promising path forward: the integration of Large Language Models (LLMs) into the retrieval pipeline. Specifically, the paper demonstrated that Query Expansion (QE) and LLM-based Reranking (RR) are critical for boosting performance.</p> <p>Yet, this introduces a complex resource allocation problem. <strong>If we view LLM compute as a finite resource (quantifiable by inference cost or latency), where is it best spent?</strong> Should we use a stronger, more expensive model to formulate a better search query (QE), or should we reserve that compute to carefully check the retrieved documents (during RR)? Furthermore, with the advent of “thinking” models (models capable of dynamic chain-of-thought generation during inference), we now have a third lever: the depth of reasoning per token.</p> <p>In this blogpost, we perform a controlled ablation study to study the tradeoffs of LLM compute in RIIR. Using the Gemini 2.5 family of models, we systematically evaluate the cost-performance trade-offs across three dimensions:</p> <ul> <li> <strong>Model Strength</strong>: From lightweight models (Flash-Lite) to reasoning-heavy models (Pro).</li> <li> <strong>Thinking Depth</strong>: Comparing standard inference against dynamic “thinking” modes.</li> <li> <strong>Reranking Depth</strong>: Analyzing the impact of increasing the reranking pool size (k).</li> </ul> <p>Our goal is to answer a practical question for system designers:</p> <blockquote> <p>In a reasoning-intensive IR pipeline, where does an additional unit of compute yield the highest marginal gain in retrieval accuracy?</p> </blockquote> <h2 id="background">Background</h2> <h3 id="reasoning-intensive-retrieval-and-bright">Reasoning-Intensive Retrieval and BRIGHT</h3> <p>Standard retrieval benchmarks (e.g., BEIR<d-cite key="thakur2021beir"></d-cite>) often rely on lexical overlap or semantic proximity. In contrast, the BRIGHT benchmark consists of queries where the answer requires multi-hop reasoning or domain-specific logic that is not explicitly present in the query text. For example, a query might ask about a specific chemical property that implies a class of materials, requiring the retriever to identify documents discussing those materials without the explicit class name being present.</p> <p>More formally, let $\mathcal{C}$ be a large corpus of documents and $q$ be a user query. In standard IR, relevance is often approximated by lexical overlap or semantic similarity. In RIIR, relevance is determined by a latent logic or reasoning requirement.</p> <p>We define a binary relevance function $Rel(q, d) \in {0, 1}$ provided by the dataset annotations. The objective is to retrieve the subset of relevant documents $\mathcal{D}^* = {d \in \mathcal{C} \mid Rel(q, d) = 1}$ and rank them at the top of the result list.</p> <h3 id="the-ir-pipeline-for-riir">The IR Pipeline for RIIR</h3> <p>To address the complexity of RIIR, BRIGHT paper suggests a multi-stage pipeline. The process generally follows a “retrieve-then-rerank” architecture augmented by LLMs.</p> <h4 id="step-1-query-expansion-qe">Step 1: Query Expansion (QE)</h4> <p>The raw query $q$ is often underspecified or requires domain knowledge to map to relevant terms in $\mathcal{C}$. An LLM is used to generate an expanded query $q_{exp}$:</p> \[q_{exp} = \text{LLM}_{\theta}(q)\] <p>This expansion adds context, uncovers implicit constraints, and generates keywords that are statistically likely to appear in $\mathcal{D}^*$.</p> <h4 id="step-2-retrieval-bm25">Step 2: Retrieval (BM25)</h4> <p>We use the BM25 (Best Matching 25) algorithm for the retrieval stage. When augmented with expanded queries BM25, BRIGHT paper shows that BM25 gets very strong results even outperforming best off-the-shelf dual encoder models. BM25 is a probabilistic retrieval framework based on TF-IDF (Term Frequency-Inverse Document Frequency). It scores documents based on the frequency of query terms in the document relative to their frequency across the entire corpus, with normalization for document length. Given $q_{exp}$, BM25 retrieves an initial candidate list $\mathcal{L}_{init} = {d_1, d_2, …, d_N}$ sorted by lexical relevance.</p> <h4 id="step-3-llm-based-reranking-rr">Step 3: LLM-based Reranking (RR)</h4> <p>The top-$k$ documents from $\mathcal{L}_{init}$ are passed to an LLM for re-ordering. Similar to standard practices for BRIGHT benchmark, we employ a list-wise reranking approach. The LLM receives the query and the concatenated text of the top-$k$ candidates as a single prompt. It is instructed to reason over the set and output the identifiers of the top 10 most relevant documents in descending order:</p> \[\pi_{top10} = \text{LLM}_{\phi}(q, \{d_1, ..., d_k\})\] <p>This approach allows the model to compare candidates directly against one another within its context window.</p> <h3 id="the-thinking-dimension">The “Thinking” Dimension</h3> <p>We leverage the Gemini 2.5 family’s ability to perform inference-time compute scaling. “Thinking” models generate internal Chain-of-Thought (CoT) traces before producing the final output. In the context of QE, this allows the model to plan the search strategy. In RR, it allows the model to explicitly reason through the connection between the query constraints and the candidate document content before assigning a rank.</p> <h2 id="experimental-setup">Experimental Setup</h2> <p>To isolate the impact of compute allocation, we fix our retrieval algorithm to BM25 (using the Pyserini implementation) and vary the LLM components used for Query Expansion and Reranking.</p> <h3 id="model-suite">Model Suite</h3> <p>We utilize the Google Gemini 2.5 family to represent a spectrum of cost and capability. We categorize them as follows:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">Gemini-2.5-Flash-Lite</code>: A highly efficient, low-latency model (No-Thinking mode only).</li> <li> <code class="language-plaintext highlighter-rouge">Gemini-2.5-Flash (No-Think)</code>: A standard mid-sized model (Thinking features disabled).</li> <li> <code class="language-plaintext highlighter-rouge">Gemini-2.5-Flash (Think)</code>: The same mid-sized model with dynamic thinking enabled, allowing for extended reasoning tokens.</li> <li> <code class="language-plaintext highlighter-rouge">Gemini-2.5-Pro</code>: A large, high-capacity model (Thinking mode enabled).</li> </ul> <h3 id="evaluation-metrics">Evaluation Metrics</h3> <ul> <li>Quality: We report NDCG@10 for ranking quality and Recall@100 to assess the retrieval ceiling.</li> <li>Performance: We measure Cost per Sample ($) and Time per Sample (s) to visualize the trade-offs.</li> </ul> <h2 id="experiments">Experiments</h2> <h3 id="scaling-compute-in-query-expansion-qe">Scaling Compute in Query Expansion (QE)</h3> <p>Query expansion is the first point where we can allocate LLM compute in our retrieval pipeline. The fundamental question here is: <strong>does investing in more powerful models for query generation translate to better retrieval performance?</strong> To answer this, we evaluate how model strength affects the quality of the initial candidate set retrieved by BM25.</p> <p><strong>Experimental Protocol:</strong></p> <ul> <li>For every query $q$ in the BRIGHT subsets, we generate $q_{exp}$ using the four model variants (Flash-Lite, Flash-No-Think, Flash-Think, Pro).</li> <li>We then perform BM25 retrieval using $q_{exp}$ and measure both quality (NDCG@10, Recall@100) and cost metrics.</li> <li>Objective: To determine if “smarter” queries (generated by more expensive models) lead to higher Recall@100, providing a better candidate set for downstream reranking.</li> </ul> <h4 id="results-summary">Results Summary</h4> <p>The table below presents our findings across all query expansion strategies. We include a “No QE” baseline that uses raw queries directly with BM25, allowing us to isolate the impact of LLM-based query expansion.</p> <table> <thead> <tr> <th>Query Expansion Model</th> <th>Avg NDCG@10</th> <th>Avg Recall@100</th> <th>Avg Cost per Query ($)</th> </tr> </thead> <tbody> <tr> <td>No QE (BM25 only)</td> <td>14.52</td> <td>33.76</td> <td>0.000</td> </tr> <tr> <td>Flash-Lite</td> <td>28.87</td> <td>57.19</td> <td>0.0018</td> </tr> <tr> <td>Flash (No-Think)</td> <td>29.63</td> <td>58.56</td> <td>0.0093</td> </tr> <tr> <td>Flash (Think)</td> <td>30.23</td> <td>57.73</td> <td>0.0141</td> </tr> <tr> <td>Pro</td> <td>30.01</td> <td>58.01</td> <td>0.0489</td> </tr> </tbody> </table> <h4 id="cost-performance-trade-offs-across-domains">Cost-Performance Trade-offs Across Domains</h4> <p>The interactive visualization below shows the cost vs. NDCG@10 trade-off for different query expansion models aggregated across various BRIGHT subsets. Each point represents the avg ndcg and model combination, color-coded by the model. Hover over points to see detailed metrics.</p> <iframe src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-trade-offs-in-llm-compute-for-reasoning-intensive-information-retrieval/cost_vs_ndcg_model1.html" width="800" height="600px" frameborder="0"></iframe> <h4 id="efficiency-analysis">Efficiency Analysis</h4> <p>To better understand the multi-dimensional trade-offs, we visualize three key metrics in the radar chart below: <strong>Quality Gain (NDCG)</strong>, <strong>Speed Efficiency (measured in latency)</strong>, and <strong>Cost Efficiency ($/query)</strong>. Each metric is normalized to enable comparison across different scales.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-trade-offs-in-llm-compute-for-reasoning-intensive-information-retrieval/qe_tradeoff_radar_chart.png" alt="Query Expansion Trade-offs" style="width: 80%; max-width: 600px; display: block; margin: 0 auto;"> <figcaption style="text-align: center;">Normalized performance across three dimensions: quality, speed, and cost efficiency. Larger areas indicate better overall efficiency.</figcaption> </figure> <h4 id="key-observations">Key Observations</h4> <p>Our experiments reveal several important insights about compute allocation in query expansion:</p> <ol> <li> <p><strong>Diminishing Returns Beyond Mid-Tier Models</strong>: Query expansion provides consistent but modest gains in recall. Moving from Flash-Lite to Flash-No-Think yields meaningful improvements (+1.37 Recall@100), but further investment in Pro delivers minimal additional gains.</p> </li> <li> <p><strong>Cost Scales Faster Than Quality</strong>: The cost increase from Flash-Lite to Pro is substantial (<strong>27x more expensive</strong>, from 0.0018 to 0.0489 per query), while the recall improvement is marginal (~1-2% absolute). This suggests that query expansion hits a quality ceiling relatively quickly.</p> </li> <li> <p><strong>Thinking Mode Shows Mixed Value</strong>: Enabling thinking mode in Flash adds 52% more cost (0.0141 vs 0.0093) but actually decreases Recall@100 slightly (57.73 vs 58.56). This suggests that extended reasoning may not be beneficial for the query generation task, possibly because query expansion is more about keyword coverage than deep logical reasoning.</p> </li> </ol> <h3 id="scaling-compute-in-reranking-rr">Scaling Compute in Reranking (RR)</h3> <p>After query expansion and initial retrieval, reranking represents the second major opportunity to allocate LLM compute in our pipeline. Unlike query expansion, which operates on a single query, reranking must process multiple candidate documents, making it inherently more expensive. The key question becomes: <strong>how should we allocate compute in the reranking stage to maximize quality gains while managing costs?</strong></p> <p>We investigate three specific mechanisms for increasing compute during reranking: (1) expanding the reranking depth (top-k), (2) enabling inference-time “thinking,” and (3) using stronger base models. For all experiments, we use the retrieval outputs from our query expansion phase, allowing us to analyze the interaction between QE and RR strategies.</p> <h4 id="increasing-reranking-depth-top-k">Increasing Reranking Depth (Top-k)</h4> <p>One of the most straightforward ways to invest compute in reranking is to rerank more documents. Intuitively, examining a larger candidate pool should improve the chances of promoting relevant documents to the top positions. However, this comes at a linear cost increase, as each additional document adds to the context window size.</p> <p><strong>Experimental Setup:</strong></p> <ul> <li> <strong>Retrieval Basis</strong>: We use the candidate lists generated by all four QE settings from Phase 1 (Flash-Lite, Flash-No-Think, Flash-Think, Pro).</li> <li> <strong>Reranker</strong>: Fixed to Gemini-2.5-Flash (Think) to isolate the impact of depth.</li> <li> <strong>Variable</strong>: We rerank the top-k documents, where $k \in {10, 20, 50, 100}$.</li> </ul> <p><strong>Understanding the Visualization:</strong></p> <p>The interactive scatter plot below shows the cost vs. NDCG@10 trade-off across all combinations of QE models (indicated by color) and reranking depths (indicated by the topN parameter in the legend). Each point represents a specific pipeline configuration. Use the dropdown to filter by specific QE models or view all configurations simultaneously.</p> <iframe src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-trade-offs-in-llm-compute-for-reasoning-intensive-information-retrieval/cost_vs_ndcg_grouped.html" width="1000" height="600px" frameborder="0"></iframe> <p><strong>Key Insights from Depth Scaling:</strong></p> <ol> <li> <p><strong>Significant Quality Gains with Depth</strong>: Increasing reranking depth from k=10 to k=100 yields substantial improvements in NDCG@10 across all QE strategies. For example, with Flash-Think QE, NDCG@10 improves from ~33 (k=10) to ~40 (k=100), representing a 21% relative gain.</p> </li> <li> <p><strong>Linear Cost Scaling</strong>: As expected, cost scales approximately linearly with k. Doubling the reranking depth from k=50 to k=100 roughly doubles the reranking cost, as the model must process twice as many documents in its context.</p> </li> <li> <p><strong>Reranking Amplifies QE Quality</strong>: Stronger QE models provide better candidate sets, which reranking can then exploit.</p> </li> </ol> <h4 id="impact-of-thinking-in-reranking">Impact of “Thinking” in Reranking</h4> <p>The “thinking” mode allows models to generate internal chain-of-thought reasoning before producing outputs. Unlike query expansion, where thinking showed limited value, reranking is a more complex task requiring careful comparison of multiple documents against nuanced query requirements. Does thinking help here?</p> <p><strong>Experimental Setup:</strong></p> <ul> <li> <strong>Retrieval Basis</strong>: We average results across all four QE settings from Phase 1 to obtain robust estimates.</li> <li> <strong>Reranking Depth</strong>: Fixed at k=100 to give thinking mode the most challenging scenario.</li> <li> <strong>Comparison</strong>: We measure the performance delta between Gemini-2.5-Flash (Think) and Gemini-2.5-Flash (No-Think).</li> </ul> <p><strong>Results:</strong></p> <table> <thead> <tr> <th>Reranker Model</th> <th>Avg NDCG@10</th> <th>Avg Cost per Query ($)</th> <th>Avg Latency per Query (s)</th> </tr> </thead> <tbody> <tr> <td>Flash (No-Think)</td> <td>39.36</td> <td>0.0450</td> <td>0.407</td> </tr> <tr> <td>Flash (Think)</td> <td>39.93</td> <td>0.0544</td> <td>0.567</td> </tr> </tbody> </table> <p><strong>Analysis:</strong></p> <p>Despite our hypothesis that thinking mode would benefit the complex task of reranking, the results show <strong>minimal practical value</strong>:</p> <ul> <li> <strong>Marginal Quality Improvement</strong>: Thinking mode yields only a +1.4% gain in NDCG@10 (39.36 → 39.93). These improvements are statistically modest and may not justify the additional computational cost in production settings.</li> <li> <strong>Poor Cost-Efficiency</strong>: The minimal quality gains come at a significant resource penalty: 21% cost increase ($0.0450 → $0.0544) and 39% latency increase (0.407s → 0.567s). This represents a worse cost-benefit ratio than anticipated.</li> <li> <strong>Why Thinking Doesn’t Help Much in Reranking</strong>: While reranking does involve comparing multiple documents against query constraints, the standard Flash model already possesses sufficient capacity to perform this task effectively. The additional reasoning tokens generated by thinking mode appear to provide diminishing returns, suggesting that the core challenge in reranking is less about extended deliberation and more about the model’s base understanding of relevance. The marginal gains do not compensate for the increased computational overhead.</li> </ul> <h4 id="impact-of-model-strength-in-reranking">Impact of Model Strength in Reranking</h4> <p>Our final experiment examines whether investing in a stronger base model for reranking (Pro vs. Flash variants) delivers better returns than the other compute allocation strategies. Due to computational constraints, we evaluate this on the <strong>economics subset</strong> of BRIGHT, which contains diverse reasoning patterns representative of the full benchmark.</p> <p><strong>Experimental Setup:</strong></p> <ul> <li> <strong>QE Strategy</strong>: Fixed to Gemini-2.5-Flash (No-Think) for all configurations, ensuring consistent candidate quality.</li> <li> <strong>Reranking Depth</strong>: Fixed at k=100 to maximize the challenge for the reranker.</li> <li> <strong>Model Variants</strong>: We compare four reranking models: Flash-Lite, Flash-No-Think, Flash-Think, and Pro.</li> </ul> <p><strong>Results:</strong></p> <table> <thead> <tr> <th style="text-align: left">Configuration</th> <th style="text-align: left">NDCG@10</th> <th style="text-align: left">Recall@100</th> <th style="text-align: left">Cost ($) QE</th> <th style="text-align: left">Cost ($) RR</th> <th style="text-align: left">Cost ($) Total</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">QE (flash) + RR (flash-lite)</td> <td style="text-align: left">32.06</td> <td style="text-align: left">32.77</td> <td style="text-align: left">0.017</td> <td style="text-align: left">0.0082</td> <td style="text-align: left">0.0199</td> </tr> <tr> <td style="text-align: left">QE (flash) + RR (flash no-think)</td> <td style="text-align: left">39.56</td> <td style="text-align: left">39.50</td> <td style="text-align: left">0.017</td> <td style="text-align: left">0.0345</td> <td style="text-align: left">0.0462</td> </tr> <tr> <td style="text-align: left">QE (flash) + RR (flash-think)</td> <td style="text-align: left">40.13</td> <td style="text-align: left">40.87</td> <td style="text-align: left">0.017</td> <td style="text-align: left">0.0440</td> <td style="text-align: left">0.0557</td> </tr> <tr> <td style="text-align: left">QE (flash) + RR (pro)</td> <td style="text-align: left">42.25</td> <td style="text-align: left">42.44</td> <td style="text-align: left">0.017</td> <td style="text-align: left">0.1477</td> <td style="text-align: left">0.1594</td> </tr> </tbody> </table> <p><strong>Observations:</strong></p> <ol> <li> <p><strong>Flash-Lite is Insufficient for Reranking</strong>: The lightweight Flash-Lite model achieves only 32.06 NDCG@10, representing a dramatic 19% drop compared to Flash-No-Think (39.56). This suggests reranking is a more demanding task that requires stronger reasoning capabilities than query expansion.</p> </li> <li> <p><strong>Pro Delivers the Highest Quality</strong>: The Pro model achieves 42.25 NDCG@10, outperforming Flash-Think by 5.3% (40.13 → 42.25). This is a more substantial gap than we observed in query expansion, indicating that reranking benefits more from model capacity.</p> </li> <li> <p><strong>Reranking Cost Dominates Total Cost</strong>: In the Pro configuration, reranking accounts for 93% of total cost (0.1477 out of 0.1594). This makes reranking the primary cost driver in the pipeline, especially for strong models.</p> </li> <li> <p><strong>The Cost-Quality Frontier</strong>: Moving from Flash-Think to Pro costs 2.9x more (0.0557 → 0.1594) for a 5.3% quality gain. Whether this is worthwhile depends on the application’s value function for accuracy improvements.</p> </li> </ol> <h2 id="conclusion">Conclusion</h2> <p>As Reasoning-Intensive Information Retrieval (RIIR) becomes central to modern search applications, system designers face a critical decision: how to allocate finite inference compute to maximize accuracy. Our systematic study on the BRIGHT benchmark using the Gemini 2.5 family provides a clear, quantitative map of this landscape.The data suggests that the intuitive “more compute equals better performance” rule requires nuance. We identified distinct zones of diminishing returns and high-leverage investment:</p> <ol> <li> <strong>Query Expansion is a Low-hanging, High-Efficiency Step</strong>: Investing in Query Expansion yields immediate returns by bridging the vocabulary gap between user queries and documents. However, this utility saturates quickly. Moving from Flash-Lite to Flash (No-Think) provides a necessary recall boost, but throwing “Pro-level” compute or “Thinking” models at query formulation offers negligible gains. For most systems, a mid-tier standard model is the optimal stopping point for QE.</li> <li> <strong>Reranking is the Primary Quality Driver</strong>: The bulk of the “reasoning” in RIIR happens during the validation of candidates, not their retrieval. Our experiments show that reranking depth ($k$) and model strength are the most reliable levers for performance. Increasing the candidate pool from 10 to 100 linearly scales costs but consistently delivers significantly higher NDCG, allowing the system to surface “needle-in-a-haystack” answers that simpler retrievers miss.</li> <li> <strong>The “Thinking” Trap</strong>: Perhaps most surprisingly, inference-time “thinking” (dynamic CoT) proved to be an inefficient allocation of resources for this specific workload. In both QE and Reranking, the marginal quality gains from Thinking modes did not justify their substantial latency and cost penalties.</li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026-iclr-blogpost-earth4d/assets/bibliography/2026-04-27-trade-offs-in-llm-compute-for-reasoning-intensive-information-retrieval.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/ppo-batch-size/">The Trade-off Between Parallel Environments and Steps in PPO</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/agent-evaluation/">A Hitchhiker's Guide to Agent Evaluation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026-iclr-blogpost-earth4d/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026-iclr-blogpost-earth4d/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026-iclr-blogpost-earth4d/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/search-data.js"></script> <script src="/2026-iclr-blogpost-earth4d/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>