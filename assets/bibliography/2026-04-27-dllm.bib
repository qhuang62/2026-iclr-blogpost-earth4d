@article{du2025understanding,
  title={Understanding the Limitations of Diffusion LLMs through a Probabilistic Perspective},
  author={Du, Cunxiao and Yang, Xinyu and Zhang, Fengzhuo and Hou, Yunlong and Yu, Sicheng and Lin, Min and Du, Chao},
  journal={arXiv preprint},
  year={2025},
  note={November 25, 2025},
url={https://www.notion.so/Understanding-the-Limitations-of-Diffusion-LLMs-through-a-Probabilistic-Perspective-2ae0ba07baa88053b838d5bf0b0aad41}
}

@article{sun2025why,
  title={Why mask diffusion does not work},
  author={Sun, Haocheng and Wen, Cynthia Xin and Wang, Edward Hong},
  journal={arXiv preprint, arXiv:2510.03289},
  year={2025},
  url={https://arxiv.org/abs/2510.03289}
}

@article{ni2025diffusion,
  title={Diffusion Language Models are Super Data Learners},
  author={Ni, Jinjie and Liu, Qian and Dou, Longxu and Du, Chao and Wang, Zili and Yan, Hang and Pang, Tianyu and Shieh, Michael Qizhe},
  journal={arXiv preprint, arXiv:2511.03276},
  year={2025},
  url={https://jinjieni.github.io/dlms-are-super-data-learners/resources/pdf/Diffusion_Language_Models_are_Super_Data_Learners.pdf}
}

@article{prabhudesai2025diffusion,
  title = {Diffusion Beats Autoregressive in Dataâ€‘Constrained Settings},
  author = {Prabhudesai, Mihir and Wu, Mengning and Zadeh, Amir and Fragkiadaki, Katerina and Pathak, Deepak},
  journal = {arXiv preprint, arXiv:2507.15857},
  year = {2025},
  url = {https://arxiv.org/abs/2507.15857}
}

@misc{guidelabs2025causal,
  title={Causal Diffusion Language Models},
  author={Guide Labs},
  year={2025},
  howpublished={\url{https://www.guidelabs.ai/post/block-causal-diffusion-language-model/}}
}

@article{hersche2025softmasked,
  title={Soft-Masked Diffusion Language Models},
  author={Hersche, Michael and Moor-Smith, Samuel and Hofmann, Thomas and Rahimi, Abbas},
  journal={arXiv preprint, arXiv:2510.17206},
  year={2025},
  url={https://arxiv.org/abs/2510.17206}
}

@article{he2025ultrallada,
  title={UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models},
  author={He, Guangxin and Nie, Shen and Zhu, Fengqi and Zhao, Yuankang and Bai, Tianyi and Yan, Ran and Fu, Jie and Li, Chongxuan and Yuan, Binhang},
  journal={arXiv preprint, arXiv:2510.10481},
  year={2025},
  url={https://arxiv.org/abs/2510.10481}
}

@misc{chandrasegaran2025rnd1,
  title={RND1: Simple, Scalable AR-to-Diffusion Conversion},
  author={Chandrasegaran, Keshigeyan and Thomas, Armin W. and others},
  year={2025},
  howpublished={\url{https://www.radicalnumerics.ai/blog/rnd1}}
}

@article{cheng2025sdar,
  title={SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation},
  author={Cheng, Shuang and Bian, Yihan and Liu, Dawei and Jiang, Yuhua and Liu, Yihao and Zhang, Linfeng and Wang, Wenhai and Guo, Qipeng and Chen, Kai and Qi, Biqing and Zhou, Bowen},
  journal={arXiv preprint, arXiv:2510.06303},
  year={2025},
  url={https://arxiv.org/abs/2510.06303}
}

@article{zhang2025syntaxguided,
  title={Syntax-Guided Diffusion Language Models with User-Integrated Personalization},
  author={Zhang, Ruqian and Zhang, Yijiao and Shen, Juan and Zhu, Zhongyi and Qu, Annie},
  journal={arXiv preprint, arXiv:2510.01028},
  year={2025},
  url={https://arxiv.org/abs/2510.01028}
}

@article{chen2025coda,
  title={CoDA: Coding LM via Diffusion Adaptation},
  author={Chen, Haolin and Wang, Shiyu and Qin, Can and Pang, Bo and Liu, Zuxin and Qiu, Jielin and Zhang, Jianguo and Zhou, Yingbo and Chen, Zeyuan and Xu, Ran and Heinecke, Shelby and Savarese, Silvio and Xiong, Caiming and Wang, Huan and Yao, Weiran},
  journal={arXiv preprint, arXiv:2510.03270},
  year={2025},
  url={https://arxiv.org/abs/2510.03270}
}

@article{zhu2025lladamoE,
  title={LLaDA-MoE: A Sparse MoE Diffusion Language Model},
  author={Zhu, Fengqi and You, Zebin and Xing, Yipeng and Huang, Zenan and Liu, Lin and Zhuang, Yihong and Lu, Guoshan and Wang, Kangyu and Wang, Xudong and Wei, Lanning and Guo, Hongrui and Hu, Jiaqi and Ye, Wentao and Chen, Tieyuan and Li, Chenchen and Tang, Chengfu and Feng, Haibo and Hu, Jun and Zhou, Jun and Zhang, Xiaolu and Lan, Zhenzhong and Zhao, Junbo and Zheng, Da and Li, Chongxuan and Li, Jianguo and Wen, Ji-Rong},
  journal={arXiv preprint, arXiv:2509.24389},
  year={2025},
  url={https://arxiv.org/abs/2509.24389}
}

@article{ye2025dream7b,
  title={Dream 7B: Diffusion Large Language Models},
  author={Ye, Jiacheng and Xie, Zhihui and Zheng, Lin and Gao, Jiahui and Wu, Zirui and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng},
  journal={arXiv preprint, arXiv:2508.15487},
  year={2025},
  url={https://arxiv.org/abs/2508.15487}
}

@article{gong2025diffucoder,
  title={DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation},
  author={Gong, Shansan and Zhang, Ruixiang and Zheng, Huangjie and Gu, Jiatao and Jaitly, Navdeep and Kong, Lingpeng and Zhang, Yizhe},
  journal={arXiv preprint, arXiv:2506.20639},
  year={2025},
  url={https://arxiv.org/abs/2506.20639}
}

@article{khanna2025mercury,
  title={Mercury: Ultra-Fast Language Models Based on Diffusion},
  author={Inception Labs and Khanna, Samar and Kharbanda, Siddhant and Li, Shufan and Varma, Harshit and Wang, Eric and Birnbaum, Sawyer and Luo, Ziyang and Miraoui, Yanis and Palrecha, Akash and Ermon, Stefano and Grover, Aditya and Kuleshov, Volodymyr},
  journal={arXiv preprint, arXiv:2506.17298},
  year={2025},
  url={https://arxiv.org/abs/2506.17298}
}

@article{zhu2025llada15,
  title={LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models},
  author={Zhu, Fengqi and Wang, Rongzhen and Nie, Shen and Zhang, Xiaolu and Wu, Chunwei and Hu, Jun and Zhou, Jun and Chen, Jianfei and Lin, Yankai and Wen, Ji-Rong and Li, Chongxuan},
  journal={arXiv preprint, arXiv:2505.19223},
  year={2025},
  url={https://arxiv.org/abs/2505.19223}
}

@article{nie2025largelldm,
  title={Large Language Diffusion Models},
  author={Nie, Shen and Zhu, Fengqi and You, Zebin and Zhang, Xiaolu and Ou, Jingyang and Hu, Jun and Zhou, Jun and Lin, Yankai and Wen, Ji-Rong and Li, Chongxuan},
  journal={arXiv preprint, arXiv:2502.09992},
  year={2025},
  url={https://arxiv.org/abs/2502.09992}
}

@article{shen2025osdt,
  title={Beyond Static Cutoffs: One-Shot Dynamic Thresholding for Diffusion Language Models},
  author={Shen, Jucheng and Ro, Yeonju},
  journal={arXiv preprint, arXiv:2511.02077},
  year={2025},
  url={https://arxiv.org/abs/2511.02077}
}

@article{wang2025creditdecoding,
  title={CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits},
  author={Wang, Kangyu and Jiang, Zhiyun and Feng, Haibo and Zhao, Weijia and Liu, Lin and Li, Jianguo and Lan, Zhenzhong and Lin, Weiyao},
  journal={arXiv preprint, arXiv:2510.06133},
  year={2025},
  url={https://arxiv.org/abs/2510.06133}
}

@article{gao2025selfspec,
  title={Self Speculative Decoding for Diffusion Large Language Models},
  author={Gao, Yifeng and Ji, Ziang and Wang, Yuxuan and Qi, Biqing and Xu, Hanlin and Zhang, Linfeng},
  journal={arXiv preprint, arXiv:2510.04147},
  year={2025},
  url={https://arxiv.org/abs/2510.04147}
}

@article{lu2025adablock,
  title={AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size},
  author={Lu, Guanxi and Chen, Hao and Karashima, Yuto and Wang, Zhican and Fujiki, Daichi and Fan, Hongxiang},
  journal={arXiv preprint, arXiv:2509.26432},
  year={2025},
  url={https://arxiv.org/abs/2509.26432}
}

@article{huang2025remedi,
  title={Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models},
  author={Huang, Zemin and Wang, Yuhang and Chen, Zhiyang and Qi, Guo-Jun},
  journal={arXiv preprint, arXiv:2509.23653},
  year={2025},
  url={https://arxiv.org/abs/2509.23653}
}

@article{wang2025timeisafeature,
  title={Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models},
  author={Wang, Wen and Fang, Bozhen and Jing, Chenchen and Shen, Yongliang and Shen, Yangyi and Wang, Qiuyu and Ouyang, Hao and Chen, Hao and Shen, Chunhua},
  journal={arXiv preprint, arXiv:2508.09138},
  year={2025},
  url={https://arxiv.org/abs/2508.09138}
}

@article{li2025beyondfixed,
  title={Beyond Fixed: Training-Free Variable-Length Denoising for Diffusion Large Language Models},
  author={Li, Jinsong and Dong, Xiaoyi and Zang, Yuhang and Cao, Yuhang and Wang, Jiaqi and Lin, Dahua},
  journal={arXiv preprint, arXiv:2508.00819},
  year={2025},
  url={https://arxiv.org/abs/2508.00819}
}

@article{xiong2025controllable,
  title={Unveiling the Potential of Diffusion Large Language Model in Controllable Generation},
  author={Xiong, Zhen and Cai, Yujun and Li, Zhecheng and Wang, Yiwei},
  journal={arXiv preprint, arXiv:2507.04504},
  year={2025},
  url={https://arxiv.org/abs/2507.04504}
}

@article{wang2025remasking,
  title={Remasking Discrete Diffusion Models with Inference-Time Scaling},
  author={Wang, Guanghan and Schiff, Yair and Sahoo, Subham Sekhar and Kuleshov, Volodymyr},
  journal={arXiv preprint, arXiv:2503.00307},
  year={2025},
  url={https://arxiv.org/abs/2503.00307}
}

@article{tseng2025dllmsurvey,
  title={Diffusion-based Large Language Models Survey},
  author={Tseng, Chiung-Yi and Zhang, Danyang and Bi, Ziqian and Song, Junhao},
  journal={TechRxiv preprint},
  year={2025},
  url={https://www.techrxiv.org/users/952417/articles/1321784-diffusion-based-large-language-models-survey}
}

@article{li2025dlmsurvey,
  title={A Survey on Diffusion Language Models},
  author={Li, Tianyi and Chen, Mingda and Guo, Bowei and Shen, Zhiqiang},
  journal={arXiv preprint, arXiv:2508.10875},
  year={2025},
  url={https://arxiv.org/abs/2508.10875}
}

@article{sutton2019bitter,
  title = {The Bitter Lesson},
  author = {Sutton, Richard},
  howpublished = {\url{https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf}},
  year = {2019},
  note = {Accessed 2025-12-08}
}
