<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/feed.xml" rel="self" type="application/atom+xml"/><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-30T07:36:30+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/feed.xml</id><title type="html">ICLR Blogposts 2026</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Budget Alignment: Making Models Reason in the User‚Äôs Language</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/budget-alignment/" rel="alternate" type="text/html" title="Budget Alignment: Making Models Reason in the User‚Äôs Language"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/budget-alignment</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/budget-alignment/"><![CDATA[<h1 id="budget-alignment-making-models-reason-in-the-users-language">Budget Alignment: Making Models Reason in the User‚Äôs Language</h1> <p><em>Please read this as a late-stage work in progress shared in a ‚Äúlab meeting‚Äù spirit to help and motivate parallel research.</em></p> <h2 id="introduction">Introduction</h2> <p>You ask a large language model (LLM) a math question in Japanese. It responds politely in Japanese ‚Äî but behind the scenes, it‚Äôs reasoning in English/Chinese. Variables, steps, and mathematical lemmas often silently switch languages during reasoning. This behavior, where models default to English for chain-of-thought (CoT) reasoning, is more than a curiosity. It breaks instruction-following, confuses human overseers, and undermines the purpose of multilingual evaluation.</p> <p>The goal is clear: we want models to reason about a question in the language they are asked ‚Äî not just to answer in that language. But this turns out to be harder than it sounds. Forcing models to reason in non-English languages usually leads to a drop in accuracy. Previous work shows that instructing models to reason only in the prompt language via prompting or steering improves coherence and grading alignment <d-cite key="zhong2025language"></d-cite>, but often comes at a steep ‚Äúaccuracy tax.‚Äù Even a small amount of multilingual fine-tuning helps, but doesn‚Äôt eliminate the trade-off <d-cite key="qi-etal-2025-models"></d-cite>. Further, models not only prefer to reason in English ‚Äî they reason <em>more effectively</em> in English. When researchers force strict in-language reasoning (e.g., in Swahili or Thai), models often lose accuracy compared to when allowed to reason in English. For higher-resource languages like French or German, this trade-off is smaller ‚Äî models can reason in-language nearly as well as in English. For low-resource languages, strict enforcement harms performance more significantly.</p> <p>Why do models switch to English in the first place? Much of it traces back to training. Most reasoning data are in English. Fine-tuning even strong multilingual models on English CoT data often leads them to adopt English as their ‚Äúinternal language of logic.‚Äù Yong et al. (2025) observe a ‚Äúquote-and-think‚Äù behavior <d-cite key="yong2025crosslingual"></d-cite>, where models copy input phrases in the prompt language, but explain everything in English <d-cite key="kim2025one"></d-cite>. The model understands the question in the non-English language ‚Äî it just prefers to reason in English.</p> <p>Our technical goal is simple: <strong>stop the switching without paying an accuracy tax</strong> ‚Äî ideally, push the Pareto frontier of <em>(Accuracy, Language-consistency)</em>.<br/> And we want this post to serve as a practical guide with lessons learned along the way.</p> <p>Code, data, and checkpoints will be linked in the <strong>camera-ready</strong> version of this post to preserve anonymity during review.</p> <hr/> <h2 id="what-we-try-method-in-two-steps">What we try (Method in two steps)</h2> <p>üîß <strong>Base model.</strong> <code class="language-plaintext highlighter-rouge">deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</code>, a large reasoning model distilled from R1 through supervised fine-tuning on its reasoning traces, exhibiting an English/Chinese-dominant prior.</p> <p><strong>Step 1 ‚Äî Small SFT to teach in-language reasoning.</strong><br/> We fine-tune on <strong>817 curated multilingual reasoning chains</strong> (from LiMO <d-cite key="ye2025limo"></d-cite>). This supervision data contains high-quality reasoning data matching R1 long-form reasoning <em>style</em>. No Reinforcement Learning (RL) here ‚Äî just teach the policy to keep reasoning in the user‚Äôs query language.</p> <p><strong>Step 2 ‚Äî Math-only GRPO to push accuracy while retaining reasoning language.</strong><br/> We run an RLVR-style GRPO with no KL, higher clip of 0.28 vs ‚àí0.2 (DAPO-like <d-cite key="yu2025dapo"></d-cite>), rollout 24, LoRA r = 8, LR = 1e-5, <strong>only on a Math-500 set translated to each language</strong>.<br/> Intuition: let RL optimize hard cases and verification behaviors, while the high clip reduces catastrophic reasoning style collapse back to English.</p> <p>We set the verifiable rewards as <strong>1.0 for accuracy, 0.2 for language consistency of reasoning traces, and 0.2 for answer format</strong> <d-cite key="rastogi2025magistral"></d-cite>.</p> <p>üìä <strong>Evaluation.</strong></p> <p>We tried our approach on three different languages: <strong>Japanese (JA) / French (FR) / Spanish (ES)</strong></p> <p>And tested on multiple datasets: <strong>MMLU College Math (MMLU Math), AIME25, GPQA, MMLU Pro Medicine (MMLU Med)</strong></p> <p>The first two are in-domain: MMLU-Math is similar to the training data in terms of hardness, while AIME25 is harder.<br/> The other two are out-of-domain: GPQA covers hard science questions, and MMLU Pro Medicine is made up of hard questions in the medical domain.</p> <p><strong>Regimes tested:</strong></p> <ul> <li>Base ‚Üí <code class="language-plaintext highlighter-rouge">deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</code> <d-cite key="deepseekai2025deepseekr1distillqwen7b"></d-cite></li> <li>SFT on top of Base</li> <li>GRPO-from-Base</li> <li>GRPO-from-SFT</li> </ul> <p><strong>Metrics:</strong></p> <ul> <li><code class="language-plaintext highlighter-rouge">pass@k(1,5,10)</code> where <code class="language-plaintext highlighter-rouge">n = 32</code> for accuracy</li> <li><code class="language-plaintext highlighter-rouge">Language-consistency %</code> (both reasoning traces <strong>and</strong> final answers must be in the requested language; script-aware checks)</li> </ul> <p><strong>How we score language consistency:</strong><br/> We check the entire CoT span and the final boxed answer.<br/> A sample counts as <code class="language-plaintext highlighter-rouge">Following = 1</code> only if both passages are in the requested language (script tokens, numerals, and markers allowed); otherwise <code class="language-plaintext highlighter-rouge">0</code>.<br/> We report the % across the set.</p> <hr/> <h2 id="-key-contributions">üîë Key contributions</h2> <ol> <li> <p><strong>Small SFT reprograms inner monologue.</strong><br/> With only <strong>817 chains</strong>, language consistency rises near the ceiling in French/Spanish across datasets and substantially in Japanese (Fig. RQ0).</p> </li> <li> <p><strong>Two-step recipe Pareto-improves.</strong><br/> SFT secures language consistency; <strong>GRPO-SFT recovers/boosts accuracy on tough sets</strong> (AIME/GPQA) without reverting to English (Figs. RQ1‚ÄìRQ4).</p> </li> <li><strong>Diagnose regressions and actionable fixes.</strong><br/> Regressions stem from: <ul> <li>Japanese tokenization/numeric friction,</li> <li>Spanish cue misalignment,</li> <li>medicine reward/style mismatch.<br/> Tokenizer-aware normalization, small Japanese/Spanish SFT top-ups, and multi-objective GRPO (with optional model merging) could recover accuracy without sacrificing in-language reasoning.</li> </ul> </li> <li><strong>TL; DR.</strong> You can briefly see our main results from the two figures below:<br/> Starting from an EN/ZH-dominant reasoning prior, small multilingual SFT is the most cost-effective way to ‚Äústeer‚Äù in-language chains of reasoning. Adding math-only GRPO then recovers or improves accuracy on hard sets like AIME and GPQA while mostly preserving SFT‚Äôs language consistency discipline ‚Äî pushing the Accuracy √ó Following frontier in many language‚Äìdataset pairs. The two pain points, Japanese (tokenization/numeric friction) and medicine (reward/style mismatch), are expected from the base prior and training signal, and both have potential straightforward fixes with light domain augmentation. And surprisingly, model merging can be very useful and effective.</li> </ol> <p><strong>Figure 1.a) Performance comparison overall across methods</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/1a-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/1a-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/1a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/1a.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 1.b) Overall language consistency rate comparison across methods</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/1b-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/1b-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/1b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/1b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq0--can-small-sft-reprogram-a-reasoning-models-reasoning-tone">RQ0 ‚Äî Can small SFT reprogram a reasoning model‚Äôs ‚Äúreasoning tone‚Äù?</h2> <p>Models often output the final answer in the same language as the user query. We want the <strong>reasoning process</strong> to match the prompt (user) language, too.</p> <p><strong>Results.</strong><br/> SFT drives the language consistency rate close to the ceiling (<strong>~99‚Äì100%</strong>) in French/Spanish and raises Japanese substantially (<strong>high-80s/90s</strong>).<br/> The language consistency rates averaged across all datasets are shown in Fig. RQ0: bars labeled Japanese/French/Spanish.</p> <p><strong>Interpretation.</strong><br/> A few hundred <strong>high-quality chains</strong> are enough to overwrite the English/Chinese inner-monologue priority to other languages. Japanese remains stubborn ‚Äî see RQ5.</p> <blockquote> <p>Recall that instruction-following does not only mean the answer in the prompt language, but it should also ensure that the language of the reasoning traces is the same as the user‚Äôs preference to enhance their trustworthiness. SFT alone solves most of the language mismatch with limited accuracy improvements, which are yet lower than the accuracy of reasoning in English (i.e., the gray dashes in Figure 1.a above) in most cases. We provide more details in the next section.</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r0-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r0-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r0.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq1--does-sft-help-accuracy-or-only-language-reasoning-style">RQ1 ‚Äî Does SFT help accuracy, or only language reasoning <em>style</em>?</h2> <p>We have shown that <strong>SFT significantly improves language consistency rates</strong>, but how about the accuracy?</p> <p><strong>Design.</strong><br/> Compare the accuracy <strong>Base vs SFT</strong> on <code class="language-plaintext highlighter-rouge">pass@k</code> per dataset‚Äìlanguage<br/> (Fig. RQ1: Œî pass@10 = SFT ‚àí Base).</p> <p><strong>Findings.</strong></p> <ul> <li><strong>MMLU-Math:</strong> substantial improvements when train and test are in the same domain <ul> <li><em>French:</em> ~76 ‚Üí <strong>98</strong></li> <li><em>Spanish:</em> ~80 ‚Üí <strong>99</strong></li> <li><em>Japanese:</em> ~68 ‚Üí <strong>88</strong></li> </ul> </li> <li> <p><strong>AIME:</strong> mixed. Although AIME contains math problems, it is way more difficult than LiMO, making it less likely to be considered as in-domain. As a result, SFT trades accuracy for strict language consistency when reasoning in ES.</p> </li> <li><strong>GPQA / MMLU Pro Medicine:</strong> Accuracy drops in most cases, but language consistency rises after SFT, indicating that it‚Äôs not trivial to generalize the capability of generating the correct answer from the training domain to others.</li> </ul> <p><strong>Takeaway.</strong><br/> SFT reliably improves language consistency <strong>and often increases accuracy on in-domain tasks (Math).</strong><br/> On OOD, SFT can over-narrate or change prior most probable token paths since the models are undertrained to reason in lower-resource languages ‚Äî accuracy may dip unless taking further actions (e.g., reinforced by RL, shown in RQ2 and RQ3).</p> <p><strong>Practical guidance.</strong><br/> If your target is <strong>language consistency/reasoning style + some accuracy</strong>, SFT alone is cost-effective in-domain.<br/> If you also need robustness on hard and/or OOD sets, doing an <strong>RL top-up could be helpful.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r1-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r1-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq2--when-rl-comes-how-does-grpo-help-with-accuracy">RQ2 ‚Äî When RL comes, how does GRPO help with accuracy?</h2> <p><strong>Design.</strong><br/> Train GRPO only on Math-500; evaluate deltas (<strong>GRPO-SFT ‚àí SFT</strong>) across<br/> MMLU-Math / AIME / GPQA / MMLU-Med (Fig. RQ2).</p> <p><strong>In-domain.</strong><br/> SFT helps accuracy, but not always; GRPO brings a boost on top of the base SFT while maintaining language consistency of reasoning traces.</p> <ul> <li><strong>MMLU-Math-FR</strong> pass@10: <strong>76.0 ‚Üí 97.8 ‚Üí 98.0</strong> (Base ‚Üí SFT ‚Üí GRPO-SFT)</li> <li><strong>MMLU-Math-ES</strong> pass@10: <strong>80.5 ‚Üí 98.6 ‚Üí 99.1</strong> (Base ‚Üí SFT ‚Üí GRPO-SFT)</li> <li><strong>MMLU-Math-JA</strong> pass@10: <strong>68.1 ‚Üí 88.0 ‚Üí 91.5</strong> (Base ‚Üí SFT ‚Üí GRPO-SFT)</li> </ul> <p>The improvement in accuracy is consistent but slight due to the fact that MMLU-Math is relatively easy:<br/> The model almost achieves 90‚Äì100% accuracy after SFT, leaving no room for GRPO. Thus, the OOD sets are more informative.</p> <p><strong>Out-of-domain.</strong></p> <p>Positive transfers on <strong>AIME JA/FR/ES and GPQA JA/FR</strong>.<br/> For instance:</p> <ul> <li><strong>GPQA-ES</strong> pass@10: <strong>68.7 ‚Üí 85.2 ‚Üí 85.7</strong> (Base ‚Üí SFT ‚Üí GRPO-SFT)</li> <li><strong>AIME-JA</strong> pass@10: <strong>22.6 ‚Üí 28.5 ‚Üí 34.4</strong> (Base ‚Üí SFT ‚Üí GRPO-SFT; GRPO adds a large JA gain)</li> </ul> <p>More results are shown in the figure below.<br/> Although improvements on AIME-FR/ES and GPQA-ES are marginal, they still indicate a successful transfer of knowledge on the OOD setup after GRPO.</p> <p><strong>Negative transfers on Pro-Medicine.</strong></p> <ul> <li>Accuracy improves on Pro-Medicine-JA but decreases on French and Spanish.</li> </ul> <p><strong>Interpretation.</strong><br/> GRPO learns verification/search habits that generalize: language consistency, math reasoning styles, re-checking numeric steps, and tighter answer boxing.<br/> Those help <strong>GPQA and AIME</strong>.<br/> But medicine needs domain lexicon, evidence phrasing, and calibrated claims ‚Äî <strong>absent in math RL</strong>.<br/> Previous works have shown reasoning-only post-training harms performance on downstream instruction-following and knowledge recall tasks <d-cite key="aggarwal2025optimalthinkingbench"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r2-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r2-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq3--where-should-rl-start-from-base-or-sft">RQ3 ‚Äî Where should RL start from: Base or SFT?</h2> <p><strong>Design.</strong><br/> Compare <strong>GRPO-from-Base vs GRPO-from-SFT</strong> (Fig. RQ3).</p> <p><strong>Patterns.</strong></p> <ul> <li> <p><strong>GRPO-from-SFT is a steadier path.</strong><br/> On MMLU-Math FR, for example, GRPO-SFT sits around <strong>~98 pass@10</strong> while GRPO-Base is closer to <strong>~70</strong>,<br/> i.e., <strong>starting from SFT provides language consistency and still improves accuracy.</strong></p> </li> <li> <p><strong>SFT ‚Üí RL keeps the multilingual policy.</strong><br/> Because SFT already forced the model to reason in Japanese/French/Spanish,<br/> RL on top of that mostly optimizes correctness <strong>without switching back to EN/ZH reasoning</strong> (Fig. 1.b).</p> </li> </ul> <p><strong>Interpretation.</strong><br/> <strong>SFT establishes the multilingual ‚Äúreasoning policy.‚Äù</strong><br/> Starting RL from the SFT model lets GRPO optimize correctness <em>while preserving language consistency</em>.<br/> RL from Base sometimes pushes the model back toward its original reasoning style while still producing answers in the target language.<br/> That can make a few out-of-domain slices look better, but it also increases variance and <strong>style regression</strong> compared to starting from SFT.</p> <p><strong>Practical rule.</strong><br/> If you care about following (see Figure 1.b) <strong>and</strong> better in-domain accuracy, <strong>do GRPO after SFT.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r3-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r3-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq4--can-we-push-the-pareto-frontier-instead-of-trading-accuracy-for-language-consistency">RQ4 ‚Äî Can we push the Pareto frontier instead of trading accuracy for language consistency?</h2> <p><strong>Design.</strong><br/> Plot Accuracy (x-axis) vs Following (y-axis) for each regime (4-panel Pareto figure).<br/> Then, inspect bar/line panels per dataset and language.</p> <h3 id="what-we-see">What we see.</h3> <ul> <li> <p><strong>SFT shifts points up</strong> (Following ‚Üë).<br/> On some hard sets, accuracy dips slightly.</p> </li> <li><strong>GRPO-SFT shifts rightward</strong> (Accuracy ‚Üë) with at most a small upward loss, compared with SFT-only ‚Äî <strong>creating new frontiers on:</strong> <ul> <li><strong>MMLU-Math (JA/FR/ES):</strong> both metrics are high.</li> <li><strong>GPQA-ES:</strong> strong frontier point.</li> </ul> </li> <li><strong>Non-frontier holdouts:</strong> Pro-Med FR/JA and AIME-ES, where domain/reward mismatch persists.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r4-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r4-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r4.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Bottom line.</strong><br/> Read each plot within the same language marker (Japanese ‚ñ≤, French ‚ñ†, Spanish ‚óè) and compare colors:</p> <ul> <li><strong>yellow vs. blue</strong> = GRPO-from-SFT vs. Base</li> <li><strong>green vs. blue</strong> = SFT vs. Base</li> </ul> <p>Under this pairing:</p> <blockquote> <p><strong>GRPO-from-SFT (yellow) strictly Pareto-dominates Base (blue) in 9 of 12 language‚Äìdataset pairs</strong> (higher on both accuracy and following).</p> </blockquote> <p>In the remaining pairs, yellow usually raises following but gives up a little accuracy ‚Äî<br/> i.e., a mixed trade-off rather than a strict Pareto gain.</p> <p>SFT (green) vs. Base (blue) generally shifts points up/right, and <strong>GRPO-from-SFT most often traces the upper-right envelope</strong> when strict dominance does occur.</p> <hr/> <h2 id="rq5--does-model-merging-help">RQ5 ‚Äî Does model merging help?</h2> <p><strong>Motivation.</strong><br/> GRPO+SFT often peaks on math but can regress on knowledge-heavy sets (e.g., Pro Medicine),<br/> and SFT alone doesn‚Äôt consistently stabilize accuracy across Japanese/French/Spanish.</p> <p>Ideally, we want a solution that smooths these trade-offs while <strong>keeping language-consistency strong</strong>.<br/> Previous studies have shown that model merging is a promising approach to combine models‚Äô abilities, albeit with some performance degradation <d-cite key="ustun-etal-2024-aya"></d-cite>.</p> <p>Here, we merged the base model with the other three SFT models using <code class="language-plaintext highlighter-rouge">merge-kit</code> with an equal linear merge.</p> <blockquote> <p>The merged approach is quite promising as a one-stop solution!</p> </blockquote> <h3 id="result-avg-pattern-across-datasets">Result (avg pattern across datasets)</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r5b-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r5b-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r5b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r5b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r5a-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r5a-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r5a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-budget-alignment/r5a.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>MERGE consistently shrinks worst-case losses and raises floor performance</strong>, especially where SFT/GRPO dip.<br/> On Pro Medicine, MERGE recovers large chunks of accuracy for Japanese/French<br/> (e.g., JA pass@10 climbs from SFT/GRPO‚Äôs ~47‚Äì58% to ~70%; FR from ~47‚Äì70% to ~76%),<br/> while staying competitive on AIME/GPQA and within a few points of GRPO+SFT on MMLU-Math.</p> <p>In Spanish, where SFT already leads on Medicine, MERGE lands in the middle of Base vs SFT/GRPO+SFT rather than decreasing performance to Base.</p> <p>Overall, it trades a small slice of peak scores for <strong>lower variance across languages and tasks.</strong></p> <h3 id="interpretation">Interpretation</h3> <p>Parameter-space interpolation acts like an ensemble/regularizer:</p> <ul> <li>MERGE <strong>blends GRPO‚Äôs strong multi-step heuristics</strong> with <strong>SFT‚Äôs alignment priors</strong></li> <li>Dampens overfitting to any single regime</li> <li><strong>Stabilizes cross-lingual behavior</strong></li> </ul> <p>Practically, it expresses a steering effect:</p> <blockquote> <p>‚ÄúYou can dial toward robustness without re-running RL.‚Äù</p> </blockquote> <p>When you need:</p> <ul> <li>the <strong>highest leaderboard peak</strong>, pick <strong>GRPO+SFT</strong></li> <li><strong>reliable, in-language reasoning across JA/FR/ES</strong>, especially on domain-heavy sets, pick <strong>MERGE</strong></li> </ul> <blockquote> <p>MERGE is the safer default when you are data + compute-poor.</p> </blockquote> <hr/> <h2 id="discussion-where-performance-regresses-and-potential-solutions">Discussion: Where performance regresses, and potential solutions</h2> <p><strong>Empirical signal.</strong><br/> After SFT followed by GRPO, Japanese language consistency improves markedly, but accuracy lags French (e.g., AIME-JA pass@1 <strong>4.4 ‚Üí 17.9</strong>, pass@10 <strong>22.6 ‚Üí 34.4</strong>;<br/> AIME-FR pass@1 <strong>22.2 ‚Üí 27.3</strong>, pass@10 <strong>46.3 ‚Üí 48.2</strong>), indicating Japanese-specific friction even with its high increase.</p> <p>Spanish on AIME shows the opposite tension: the <strong>Base</strong> model scores well because it always reasons in English despite Spanish prompts, while <strong>SFT+GRPO enforces Spanish chains and accuracy drops</strong>.</p> <p>In Pro-Medicine, <strong>math-only GRPO from SFT causes regression</strong> (e.g.,<br/> FR pass@10 <strong>70.1 ‚Üí 46.6</strong>, ES <strong>86.6 ‚Üí 76.6</strong>, JA <strong>75.9 ‚Üí 58.3</strong>), whereas GRPO started from Base hurts less.</p> <h3 id="mechanisms">Mechanisms</h3> <ol> <li> <p><strong>Language-prior competition.</strong><br/> The model‚Äôs strongest <em>reasoning prior</em> is in EN/ZH.<br/> Under difficulty, chains drift toward those priors.<br/> SFT+GRPO strengthens language consistency, which <strong>reduces access to English-anchored reasoning traces</strong> that previously helped (e.g., AIME-ES).<br/> ‚Üí evidenced by the huge language-consistency bump.</p> </li> <li> <p><strong>Tokenizer &amp; formatting tax (Japanese &gt; French / Spanish).</strong><br/> Mixed scripts, half/full-width digits, unit variants, and thousand separators inflate perplexity on numeric steps ‚Äî precisely where accuracy is most sensitive.</p> </li> <li> <p><strong>Cue misalignment in Spanish math.</strong><br/> AIME leans on algebra/number-theory ‚Äúrecipes‚Äù the model learned primarily in English<br/> (phrases like ‚Äúlet x be,‚Äù ‚Äúgcd,‚Äù ‚Äúmod‚Äù).<br/> Spanish equivalents (‚Äúsea x,‚Äù ‚Äúmcd,‚Äù ‚Äúm√≥dulo‚Äù) are rarer, longer, more accented <br/> ‚Üí model drifts into slower or incorrect approaches mid-solution.</p> </li> <li> <p><strong>Reward misspecification in medicine.</strong><br/> Math-only RL optimizes numeric correctness, <strong>not</strong> biomedical recall, calibration, or evidence style. The policy over-indexes math heuristics and becomes <strong>over-assertive</strong> on clinical QA.</p> </li> <li> <p><strong>Starting-point effect.</strong><br/> RL from SFT pushes the policy toward SFT‚Äôs language/style anchors and away from neutral reasoning.<br/> On medicine, this causes bigger drops. RL from Base is more neutral; regressions are smaller.</p> </li> </ol> <h3 id="lightweight-fixes-that-may-work-across-cases">Lightweight fixes that may work across cases</h3> <ul> <li> <p><strong>Prompt-level normalization (before more training).</strong></p> <ul> <li> <p><em>Japanese:</em> unify to half-width digits/decimals/exp notation; no thousand separators;<br/> explicit math chain template in Japanese. <br/> Example: <code class="language-plaintext highlighter-rouge">Êï∞Â≠ó„ÅØÂçäËßí‚Ä¶ SI „Çí‰ΩøÁî®„Åó‚Ä¶</code>.</p> </li> <li> <p><em>Spanish:</em> prefer <code class="language-plaintext highlighter-rouge">gcd / lcm / mod</code>, exponent notation, half-width digits;<br/> terse step headers (<code class="language-plaintext highlighter-rouge">Definimos / Sustituimos / Comprobaci√≥n / Respuesta</code>).</p> </li> </ul> </li> <li> <p><strong>Tokenizer-aware formatting.</strong><br/> Consistent spacing around numerals/operators; avoid formatting that fragments tokens.</p> </li> <li> <p><strong>Targeted SFT top-ups.</strong><br/> Small, math-dense Japanese/Spanish datasets using normalized templates to reinforce per-language priors.</p> </li> <li> <p><strong>Reward shaping for GRPO.</strong></p> <ul> <li> <p>For <strong>AIME-ES</strong>: up-weight <em>correctness</em> and make <strong>‚ÄúSpanish-only chain‚Äù</strong> a secondary objective.<br/> ‚Üí nudges reasoning into Spanish <strong>without punishing English-anchored correct answers</strong>.</p> </li> <li> <p>For <strong>Medicine</strong>: add a <strong>tiny medical reward head</strong><br/> (terminology fidelity, claim calibration, evidence cues),<br/> plus a <strong>KL / behavior-cloning regularizer</strong> toward medical SFT to preserve discourse style.</p> </li> <li> <p>Use <strong>mixed-objective batches</strong> (math + clinical QA),<br/> and replay OOD medical exemplars during RL to avoid domain forgetting.</p> </li> </ul> </li> </ul> <h3 id="takeaway">Takeaway</h3> <p>The regressions likely stem from one cause:</p> <blockquote> <p><strong>objective + prior mismatch</strong>.</p> </blockquote> <p>Japanese/Spanish math suffers from tokenization and cue issues; medicine suffers from the absence of domain-specific rewards. Normalizing inputs, adding small language-aware SFT top-ups, and turning ‚Äúmath-only RL‚Äù into multi-objective RL (with correctness-first weighting for AIME-ES and a small medical head for Pro-Medicine) could be promising ways to recover accuracy while keeping outputs in the target language and accurate.</p> <hr/> <h2 id="blog-summary--practical-takeaways">Blog Summary ‚Äî Practical takeaways</h2> <ol> <li> <p><strong>If you can only afford one step, do SFT (a few hundred high-quality SFT data).</strong><br/> You‚Äôll almost certainly fix language-consistency without compromising accuracy;<br/> you might also get accuracy improvements on in-domain tasks.</p> </li> <li> <p><strong>If you can afford two steps, do SFT ‚Üí GRPO-SFT.</strong><br/> Use <strong>high clip / no KL</strong>; keep rollouts moderate; verify you haven‚Äôt regressed following.</p> </li> <li> <p>A practical and computationally efficient approach is <strong>model merging among SFT models</strong>.</p> </li> <li> <p><strong>For medicine or other narrative-dense domains, add a tiny domain reward with in-domain data or a dozens-scale domain SFT.</strong></p> </li> <li> <p><strong>For Japanese (or any non-Latin script), include numeric/style templates</strong><br/> and optionally patch tokenization via formatting.</p> </li> <li> <p><strong>Track Pareto, not single metrics.</strong><br/> Always plot <em>(Accuracy, Following)</em> together; real wins move you <strong>up-and-right</strong>.</p> </li> </ol> <hr/> <h2 id="limitations--threats-to-validity">Limitations &amp; threats to validity</h2> <ul> <li> <p><strong>Dataset scope.</strong><br/> We use four well-known benchmarks; real-world prompts are noisier.</p> </li> <li> <p><strong>Reward misspecification.</strong><br/> Math-only RL can hurt non-math; the suggested fixes mitigate but don‚Äôt prove generality across all medical subspecialities.</p> </li> <li> <p><strong>Model prior.</strong><br/> EN/ZH dominance shapes outcomes. A different base prior (e.g., EU-centric) could change which languages are hardest.</p> </li> <li> <p><strong>Language-consistency metric.</strong><br/> Strong, script-aware, but still an automatic proxy; human raters may be stricter.</p> </li> </ul>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[We explore a two step multilingual alignment recipe for large language models to keep reasoning and answers in the user language while preserving accuracy.]]></summary></entry><entry><title type="html">ChunkTabPFN: Training-free Long Context</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/chunked-tabpfn/" rel="alternate" type="text/html" title="ChunkTabPFN: Training-free Long Context"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/chunked-tabpfn</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/chunked-tabpfn/"><![CDATA[<h2 id="1-introduction">1. Introduction</h2> <p><span id="sec:introduction"></span></p> <p>Large language models leverage <strong>in-context learning (ICL)</strong> by adapting their predictions at inference time based solely on provided examples, without requiring any gradient updates. Building on this idea, recent work on <strong>tabular foundation models</strong>, such as TabPFN, TabICL, Mitra, and Limix, extends the same paradigm to tabular data <d-cite key="hollmann2022tabpfn,hollmann2025accurate,qu2025tabicl,zhang2025mitra,zhang2025limix"></d-cite>. These models are trained once on synthetic tasks drawn from a prior, allowing them to approximate the posterior predictive distribution</p> \[p(y_{*} \mid x_*, D_{\text{train}})\] <p>in a single forward pass by supplying the training set as context, without any dataset-specific fine-tuning, without fine-tuning on each new dataset <d-cite key="hollmann2022tabpfn,hollmann2025accurate"></d-cite>. This approach is compelling because it contrasts with most deep tabular models‚Äîlike TabNet, FT-Transformer, NODE, TabM, or retrieval-style models such as TabR and ModernNCA, which typically require dataset-specific training or fine-tuning <d-cite key="arik2021tabnet,gorishniy2021revisiting,popov2019neural,gorishniy2024tabm,gorishniy2023tabr,ye2024modern"></d-cite>. That dependency undermines the ideal of a true ‚Äúdrop-in foundation model.‚Äù</p> <p>ICL-based tabular models move closer to this ideal. However, they face a major practical limitation: <strong>context length</strong>. Transformer attention scales quadratically with sequence length, and current public TabPFN implementations are constrained to around 3,000 samples in the original work to 10,000<d-footnote>At the time of writing, the new TabPFN v2.5 model has just been released, which is supposed to have pushed the context limit further to 50,000.</d-footnote> in later versions <d-cite key="hollmann2022tabpfn,hollmann2025accurate"></d-cite>. Many real-world tabular datasets far exceed these limits.</p> <p>To address this, researchers have experimented with <strong>shrinking the context</strong>, such as by clustering, partitioning, or retrieving only subsets of the data. Examples include random-forest partitioning <d-cite key="hollmann2025accurate"></d-cite>, the Mixture of In-Context Prompters (MICP) <d-cite key="xu2024mixture"></d-cite>, and KNN-style retrieval <d-cite key="thomas2024retrieval"></d-cite>. Others, like TuneTables <d-cite key="feuer2024tunetables"></d-cite>, compress the data into learned representations.</p> <p>While these methods can be effective, they come with two drawbacks:</p> <ul> <li>They often require <strong>dataset-specific tuning</strong> or even retraining, which contradicts the zero-shot, pure ICL philosophy.</li> <li>They don‚Äôt use the <strong>entire training set</strong>, which is a core assumption of TabPFN‚Äôs Bayesian approximation. Replacing full data with summaries introduces conceptual inaccuracy.</li> </ul> <p>Hence, we ask the following question:</p> <blockquote> <p>Can we fit <strong>all training examples</strong> into the context (no pruning, no KNN) without learnable compression while staying within GPU memory?</p> </blockquote> <p>In this work, we focus specifically on TabPFN, though we believe the conclusions extend to other ICL-based tabular models. Our answer is a resounding <strong>yes</strong>. Indeed, TabPFN‚Äôs native implementation already supports this on some devices via <strong>FlashAttention</strong> <d-cite key="dao2022flashattention,dao2023flashattention,shah2024flashattention"></d-cite>. But as we‚Äôll show in this blogpost, there are important caveats:</p> <ul> <li>FlashAttention and similar efficient mechanisms can <strong>fail</strong> when batch or head sizes exceed 65,535.</li> <li>These optimizations are <strong>unsupported</strong> on older or consumer-grade GPUs.</li> </ul> <p>To resolve this, we introduce a <strong>simple patch</strong>:</p> <ul> <li>For efficient attention, we <strong>chunk inputs</strong> along head or batch dimensions to avoid hitting the 65,536 limit.</li> <li>For older GPUs, we implement a <strong>chunked forward pass</strong> in pure PyTorch using the <strong>incremental log-sum-exp trick</strong>.</li> </ul> <p>This patch yields results <strong>identical to standard attention</strong> (up to floating-point associativity), without any approximations, fine-tuning, or pre-filtering.</p> <p>Empirically, we then test TabPFN out-of-the-box scalability by evaluating it on the full <strong>TabArena</strong> benchmark <d-cite key="tabarena"></d-cite>. We specifically analyze TabPFN performance on datasets with <strong>long contexts</strong> (&gt; 10,000). Key findings include:</p> <ul> <li><strong>Accuracy improves</strong> with more data, often up to 100,000+ rows (measured in AUC for classification and RMSE for regression).</li> <li>On smaller contexts (&lt;10,000), our chunked version <strong>matches the original</strong>‚Äîno hidden degradation.</li> <li>The runtime stays <strong>practical</strong> even on commodity GPUs.</li> </ul> <h2 id="2-methodology">2. Methodology</h2> <p><span id="sec:methodology"></span></p> <p>Let <code class="language-plaintext highlighter-rouge">(X, y)</code> be the input to the TabPFN model. The typical dimensions of the feature tensor are <code class="language-plaintext highlighter-rouge">[B, L, F]</code>, where <code class="language-plaintext highlighter-rouge">B</code> is the number of datasets in the batch, <code class="language-plaintext highlighter-rouge">L</code> is the (padded) sample size, and <code class="language-plaintext highlighter-rouge">F</code> is the number of features. The first thing TabPFN does is group features <code class="language-plaintext highlighter-rouge">X</code> and embed them, which yields the following shape: <code class="language-plaintext highlighter-rouge">[B, L, G, D]</code>, where <code class="language-plaintext highlighter-rouge">G</code> is the number of feature groups and <code class="language-plaintext highlighter-rouge">D</code> is the embedding size. In the rest of the blog, we assume <code class="language-plaintext highlighter-rouge">X</code> already has this post-embedding shape.</p> <p>The labels <code class="language-plaintext highlighter-rouge">y</code> are similarly embedded and then concatenated with the features along the group dimension, producing an input of shape <code class="language-plaintext highlighter-rouge">[B, L, G + 1, D]</code>. A keen reader might notice that <code class="language-plaintext highlighter-rouge">y</code> and <code class="language-plaintext highlighter-rouge">X</code> effectively have different ‚Äúlogical‚Äù lengths: <code class="language-plaintext highlighter-rouge">X</code> includes both train and test samples, while <code class="language-plaintext highlighter-rouge">y</code> is only provided for the training split. This is handled by padding the label embeddings for test samples with a dummy embedding. A variable <code class="language-plaintext highlighter-rouge">single_eval_pos</code> in the original code holds the index where train and test samples are concatenated, and this logic can be seen in the <code class="language-plaintext highlighter-rouge">transformer.py</code> file of the original TabPFN repository.</p> <p>The core of TabPFN is the attention mechanism, whose logic is primarily implemented in <code class="language-plaintext highlighter-rouge">layer.py</code>. TabPFN, like many Transformer-style models, uses attention in two ways: <strong>between samples</strong> and <strong>between features</strong>. The between-sample attention has both self- and cross-attention components: self-attention among training samples and cross-attention from test samples to train samples. Following the TabPFN implementation, we assume attention layers expect input of shape <code class="language-plaintext highlighter-rouge">[batch, seq_len, input_size]</code>. In the code, the leading dimensions before <code class="language-plaintext highlighter-rouge">(seq_len, input_size)</code> are collapsed via <code class="language-plaintext highlighter-rouge">_rearrange_inputs_to_flat_batch</code>. For between-feature attention this yields an effective batch size of <code class="language-plaintext highlighter-rouge">L * B</code>, whereas for between-item (between-sample) attention it yields <code class="language-plaintext highlighter-rouge">(G + 1) * B</code>.</p> <p>Recall that efficient attention implementations in PyTorch (such as the fused CUDA kernels backing <code class="language-plaintext highlighter-rouge">torch.nn.functional.scaled_dot_product_attention</code>) tile work across the <strong>batch</strong> and <strong>head</strong> dimensions. On NVIDIA GPUs of Ampere architecture and below, this effectively limits the product <code class="language-plaintext highlighter-rouge">B * num_heads</code> to at most <code class="language-plaintext highlighter-rouge">65535</code> CUDA blocks; when it reaches <code class="language-plaintext highlighter-rouge">65536</code> the kernel can fail with <code class="language-plaintext highlighter-rouge">CUDA error: invalid configuration argument</code> (see the corresponding <a href="https://github.com/pytorch/pytorch/issues/133976">PyTorch GitHub issue</a> for a minimal example where <code class="language-plaintext highlighter-rouge">65535</code> works but <code class="language-plaintext highlighter-rouge">65536</code> fails). In TabPFN, large sample sizes <code class="language-plaintext highlighter-rouge">L</code> or a large number of feature groups <code class="language-plaintext highlighter-rouge">G</code> can easily push these flattened batch sizes (<code class="language-plaintext highlighter-rouge">L * B</code> or <code class="language-plaintext highlighter-rouge">(G + 1) * B</code>) past this limit.</p> <p>A simple practical fix is to loop over the flattened batch dimension in chunks, so that each call to <code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code> stays within the kernel‚Äôs limits. This keeps the rest of the model unchanged while avoiding the <code class="language-plaintext highlighter-rouge">invalid configuration</code> errors at large <code class="language-plaintext highlighter-rouge">L</code> or <code class="language-plaintext highlighter-rouge">G</code>. Conceptually, this is can be done via the following patch to the attention computation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">q_chunk</span><span class="p">,</span> <span class="n">k_chunk</span><span class="p">,</span> <span class="n">v_chunk</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">k_b</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">v_b</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
<span class="p">):</span>
    <span class="c1"># (B_chunk, Lq, H, D) -&gt; (B_chunk, H, Lq, D)
</span>    <span class="n">Q</span> <span class="o">=</span> <span class="n">q_chunk</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">k_chunk</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">v_chunk</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span>
        <span class="n">Q</span><span class="p">,</span>
        <span class="n">K</span><span class="p">,</span>
        <span class="n">V</span><span class="p">,</span>
        <span class="n">attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">dropout_p</span><span class="o">=</span><span class="n">dropout_p</span> <span class="k">if</span> <span class="n">dropout_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">softmax_scale</span><span class="p">,</span>
    <span class="p">)</span>  <span class="c1"># (B_chunk, H, Lq, D)
</span>
    <span class="c1"># -&gt; (B_chunk, Lq, H, D)
</span>    <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">())</span>

<span class="n">attention_head_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>A different issue is <strong>hardware support</strong> for efficient attention kernels. PyTorch‚Äôs <code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code> can dispatch to several backends on CUDA: FlashAttention, memory-efficient attention, or a plain math implementation in C++. The availability of these specialized kernels varies across GPU generations. For educational purposes, and for those who wish to implement these kernels on older or unsupported devices, we refer to <a href="https://github.com/lucidrains/memory-efficient-attention-pytorch/tree/main">this repository</a>. We provide a brief sketch of how the chunking works to reduce the memory footprint below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">chunked_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">q_chunk</span><span class="p">,</span> <span class="n">kv_chunk</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    q: (..., Lq, D)
    k: (..., Lk, D)
    v: (..., Lk, Dv)
    q_chunk: size of query tiles (l)
    kv_chunk: size of key/value tiles (r)
    </span><span class="sh">"""</span>
    <span class="n">Lq</span><span class="p">,</span> <span class="n">Lk</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">k</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">qs</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Lq</span><span class="p">,</span> <span class="n">q_chunk</span><span class="p">):</span>
        <span class="n">qe</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">qs</span> <span class="o">+</span> <span class="n">q_chunk</span><span class="p">,</span> <span class="n">Lq</span><span class="p">)</span>
        <span class="n">q_tile</span> <span class="o">=</span> <span class="n">q</span><span class="p">[...,</span> <span class="n">qs</span><span class="p">:</span><span class="n">qe</span><span class="p">,</span> <span class="p">:]</span>                            <span class="c1"># (..., l, D)
</span>
        <span class="c1"># running stats per query row
</span>        <span class="n">mu</span> <span class="o">=</span> <span class="n">q_tile</span><span class="p">.</span><span class="nf">new_full</span><span class="p">(</span><span class="n">q_tile</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">inf</span><span class="sh">"</span><span class="p">))</span>  <span class="c1"># (..., l)
</span>        <span class="n">s</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>                               <span class="c1"># (..., l)
</span>        <span class="n">a</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="o">*</span><span class="n">mu</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                         <span class="n">device</span><span class="o">=</span><span class="n">q</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>        <span class="c1"># (..., l, Dv)
</span>
        <span class="k">for</span> <span class="n">ks</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Lk</span><span class="p">,</span> <span class="n">kv_chunk</span><span class="p">):</span>
            <span class="n">ke</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">ks</span> <span class="o">+</span> <span class="n">kv_chunk</span><span class="p">,</span> <span class="n">Lk</span><span class="p">)</span>
            <span class="n">k_tile</span> <span class="o">=</span> <span class="n">k</span><span class="p">[...,</span> <span class="n">ks</span><span class="p">:</span><span class="n">ke</span><span class="p">,</span> <span class="p">:]</span>                           <span class="c1"># (..., r, D)
</span>            <span class="n">v_tile</span> <span class="o">=</span> <span class="n">v</span><span class="p">[...,</span> <span class="n">ks</span><span class="p">:</span><span class="n">ke</span><span class="p">,</span> <span class="p">:]</span>                           <span class="c1"># (..., r, Dv)
</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q_tile</span><span class="p">,</span> <span class="n">k_tile</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale</span>
            <span class="n">local_max</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="n">values</span>               <span class="c1"># (..., l)
</span>            <span class="n">new_mu</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">local_max</span><span class="p">)</span>

            <span class="c1"># rescale old aggregates
</span>            <span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="n">new_mu</span><span class="p">)</span>
            <span class="n">s</span> <span class="o">*=</span> <span class="n">alpha</span>
            <span class="n">a</span> <span class="o">*=</span> <span class="n">alpha</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">]</span>

            <span class="c1"># accumulate current tile
</span>            <span class="n">exp_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">new_mu</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">])</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="n">exp_logits</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                         <span class="c1"># sum_k e^{z_k}
</span>            <span class="n">a</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">exp_logits</span><span class="p">,</span> <span class="n">v_tile</span><span class="p">)</span>               <span class="c1"># sum_k e^{z_k} v_k
</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="n">new_mu</span>

        <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">a</span> <span class="o">/</span> <span class="n">s</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">])</span>                        <span class="c1"># softmax = a / s
</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>                           <span class="c1"># (..., Lq, Dv)
</span></code></pre></div></div> <p>In this implementation, the key components are:</p> <ul> <li>It tiles queries into chunks <code class="language-plaintext highlighter-rouge">q_chunk</code> instead of processing all <code class="language-plaintext highlighter-rouge">Lq</code> at once.</li> <li>It streams over keys/values in chunks <code class="language-plaintext highlighter-rouge">kv_chunk</code>, computing only <code class="language-plaintext highlighter-rouge">l √ó r</code> logits at a time.</li> <li>It maintains per-row running statistics <code class="language-plaintext highlighter-rouge">(mu, s, a)</code> using a numerically stable log-sum-exp merge, so the final output matches full attention as if we had formed the entire <code class="language-plaintext highlighter-rouge">Lq √ó Lk</code> score matrix in one go.</li> </ul> <h2 id="3-experiments">3. Experiments</h2> <p><span id="sec:experiments"></span></p> <p>We evaluate the TabPFN v2 model with chunking enabled on <strong>TabArena</strong> <d-cite key="tabarena"></d-cite>, which includes 51 tabular datasets spanning classification and regression tasks. We report scaling statistics for memory and runtime in Figure 1, and overall performance on TabArena in Figure 2. Note that in the original and subsequent reports of TabPFN, LIMIX, and TabICL on TabArena, the authors have typically imputed values that exceeded the context length for their respective methods. This might have created a distorted view of model capabilities. In Figure 2, we use only directly measured (non-imputed) results.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="attn-figure-caption"> Figure 1. Scaling TabPFN v2 to long contexts. Chunked TabPFN matches baseline accuracy where both fit, and extends inference to 100K+ examples. </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="attn-figure-caption"> Figure 2. Elo and normalized score across TabArena. Striped bars denote prior imputed TabPFN runs (filled with Random Forest fallbacks when OOM); our chunked TabPFN reports direct measurements. </div> <p>Separately, we evaluate TabPFN v2 on the same long-context datasets while varying the context length. Specifically, we sample <code class="language-plaintext highlighter-rouge">num_samples</code> points from each dataset and then report performance, memory, and runtime in Figure 3. To better understand how context length affects TabPFN‚Äôs performance, we perform a <em>scaling study</em> on the 15 ‚Äúlong-context‚Äù datasets from TabArena. For each dataset, we subsample the training set to progressively larger sizes (3,000 ‚Üí 5,000 ‚Üí 10,000 ‚Üí 20,000 ‚Üí 50,000 ‚Üí 100,000) and compare baseline TabPFN v2 against our Chunked TabPFN.</p> <ul> <li>Chunked TabPFN maintains <em>exact equivalence</em> to baseline TabPFN while extending feasible context length by roughly 10√ó.</li> <li>Empirical scaling shows either plateau or monotonic improvement‚Äînever catastrophic degradation.</li> <li>Memory and runtime growth are linear in chunk size, enabling inference on 100 K+ examples with a single GPU.</li> </ul> <p>These findings reinforce that <strong>TabPFN‚Äôs in-context generalization truly extends beyond its training limit</strong>, and that the primary bottleneck was <em>implementation-level memory</em>, not <em>model-level capacity</em>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="attn-figure-caption"> Figure 4. Scaling curves for long-context datasets. Each plot shows RMSE, AUC, wall-clock inference time (s), and peak GPU memory (MB). Chunked TabPFN tracks baseline accuracy exactly up to 10 K examples and continues scaling to 100 K without degradation. </div> <h2 id="4-conclusion">4. Conclusion</h2> <p><span id="sec:conclusion"></span></p> <p>We presented <strong>Chunked TabPFN</strong>, an exact tiling strategy that enables TabPFN to process <em>long-context</em> tabular datasets (100 K+ rows) without retraining, fine-tuning, or any pre-processing such as clustering or compression.</p> <p>Our main results show:</p> <ol> <li> <p><strong>Exactness without approximation.</strong> The chunked attention computation is mathematically identical to the original transformer attention‚Äîonly the evaluation order changes. Predictions match baseline TabPFN bit-for-bit (within floating-point tolerance) for all short-context cases.</p> </li> <li> <p><strong>Memory scalability.</strong> Peak GPU memory scales linearly with tile size instead of quadratically with context length. This removes the practical 10 K-sample ceiling and allows inference on 100 K+ rows using 24‚Äì32 GB GPUs.</p> </li> <li> <p><strong>Training-free generalization.</strong> Chunked TabPFN retains the spirit of in-context learning: no dataset-specific training, no hyperparameter search, no adaptation steps. Despite its simplicity, it matches or surpasses tuned deep tabular models on the long-context slice of TabArena.</p> </li> <li> <p><strong>Empirical insights.</strong> Many datasets continue to improve with larger contexts‚Äîsuggesting that the PFN prior generalizes beyond its nominal pre-training length.</p> </li> </ol>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Tabular foundation models struggle with large datasets due to the quadratic attention. While methods like FlashAttention promise scalability, practical challenges persist in their application to tabular foundation models. Our work resolves these hurdles, enabling efficient attention, and reveals that contrary to the eariler reports, TabPFN's performance improves with larger contexts, highlighting its inherent robustness and minimal fine-tuning needs when scaling to complex, long datasets from the TabArena benchmark.]]></summary></entry><entry><title type="html">Defining and quantifying compositional structure</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/compositionality/" rel="alternate" type="text/html" title="Defining and quantifying compositional structure"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/compositionality</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/compositionality/"><![CDATA[<p>What is compositionality? For those of us working in AI or cognitive neuroscience this question can appear easy at first, but becomes increasingly perplexing the more we think about it. We aren‚Äôt short on intuitions: we know that compositionality has something to do with reuse of parts, combinatorial expressive power, systematic generalization, and that natural language is a paradigmatic case, among other things. We seem to be able to glance at some data and say ‚Äúyes, <em>that</em> is compositional!‚Äù, and we largely seem to agree on these judgments. But what is compositionality <em>really</em>, on a mathematical level, and can we quantify it?</p> <p>Formalisms of intuitive concepts have proven incredibly useful in science, and for good reason. Humanity was long aware of things like gravity and drag, but it is only by understanding their deeper mathematical nature that we were able to build things like airplanes. We‚Äôre familiar with this phenomenon in AI as well. Since the early days of deep learning, people knew that building the structure of a modality and its symmetries into a model would be helpful, and were making some progress largely through intuition alone ‚Äî for instance, designing convolutional architectures that are invariant to image translation <d-cite key="fukushima_neocognitron_1980"></d-cite>. However, it was only through the formalizations of Geometric Deep Learning <d-cite key="bronstein_geometric_2021"></d-cite>, which was built on the foundations of abstract algebra, topology, and group theory, that we were able to do this successfully in more general modalities such as graphs and manifolds. In contrast, when it comes to compositionality, we‚Äôre largely in the dark. We recognize its computational significance and its potential to address longstanding challenges like out-of-distribution generalization or continual learning, but we fumble around trying to make it emerge in our models without the proper theoretical tools needed to detect it, let alone build it in by design.</p> <p>The purpose of this blog post is to <strong>introduce a formal definition of compositionality</strong>. We don‚Äôt promise that it‚Äôs perfect ‚Äî we have no experimental results and none of this has been vetted up to this point by peer review. Nevertheless, we feel like there‚Äôs enough here to warrant putting the ideas out in writing for the rest of the community to digest and criticize. As we hope you‚Äôll find, the formal definition that we‚Äôre going to propose is both simple and wide-reaching, capturing disparate notions of compositionality within a single equation that applies to any form of data (a neural representation, a dataset, a piece of art or music, a physical object, etc.). We‚Äôll also say quite a bit about <strong>what this definition of compositionality is useful for</strong> in AI, since it has deep implications for how we should build neural architectures and, even more importantly, how we should go about training them through the design of data curricula that <em>maximize compositional structure</em>.</p> <p>This blog post can broadly be divided into three sections. In the <a href="#section-1--compositionality-as-the-emergence-of-novel-shared-structure">first part</a>, we‚Äôll introduce the ideas at a purely <em>intuitive</em> level. In the <a href="#section-2--a-formal-definition-for-quantifying-compositionality">second part</a>, we‚Äôll make these ideas formally precise by expressing them through the mathematics of algorithmic information theory and compression, culminating in a single succinct equation that quantifies compositional structure. In the <a href="#section-3--implications-and-use-cases-for-ai">third and final section</a>, we‚Äôll discuss some practical implications of these ideas for AI, touching on topics such as how to model hierarchical structure and construct data curricula from which knowledge can grow compositionally into the future, in a boundless and open-ended way.</p> <hr/> <h1 id="section-1--compositionality-as-the-emergence-of-novel-shared-structure">Section 1 ‚Äî Compositionality as the emergence of novel shared structure</h1> <p>Paradigmatic examples of compositional data are easy to think of: a piece of music that recombines nested motifs and themes, an image dataset in which any given scene is made up of a combination of objects, a program that maximizes code reuse by defining a network of functions and classes, and of course natural language which can express an infinite set of ideas using a relatively small set of words and grammatical rules. Clearly, these examples have a lot in common, namely the notion of ‚Äúparts‚Äù or ‚Äúmodules‚Äù which interact in complex cascades and at multiple scales to form the ‚Äúwhole‚Äù of the object.</p> <p>This sort of multi-scale parts-based structure is what we‚Äôre going to try to quantify. To do so, we want to first drill down on the example of <em>computer programs</em> as a backdrop to our discussion that will extend more or less throughout the blog post. There are a few reasons for this. For one, programs are extremely general: any object we can think of can be described through a set of instructions (i.e., a program). Even more important, programs are paradigmatic cases of compositional objects that are familiar to all computer scientists. Once we‚Äôve introduced all of the ideas in the context of computer programs, we‚Äôll abstract them back out so that they apply to <em>any</em> object, be it a piece of music, a painting, an <em>iid</em> dataset, a nonstationary stream of data, a function, etc. ‚Äî essentially, anything that can be thought of as <em>information</em> expressed in bits.</p> <h2 id="programs-and-libraries">Programs and libraries</h2> <p>We consider some program compositional when it defines and reuses the same structures again and again in novel ways. Crucially, this is a property of the program‚Äôs <em>library</em> ‚Äî the functions, classes, and data structures that it defines in order to optimize code reuse and modularity. By ‚Äúlibrary‚Äù here we want to stress that we do <em>not</em> mean an external package that one might import; we‚Äôre considering a self-contained program that makes no reference to external code, and the ‚Äúlibrary‚Äù refers to the reusable structures defined within the program itself ‚Äî see the example in Figure 1. A compositional program‚Äôs library is rich, defining a number of functions and classes that serve as the ‚Äúparts‚Äù which are recomposed. The structures in the library must also be broadly reused across the entire code base rather than only in local regions, otherwise we‚Äôre better off talking about multiple independent and non-compositional programs rather than a single compositional one. In addition, the libraries of compositional programs are themselves densely networked: functions and classes build hierarchically and laterally on top of existing ones.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig1-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig1-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 1: Signatures of compositional programs.</strong> Programs are compositional in virtue of their *libraries:* the classes, functions, and other reusable code that they define. The libraries of compositional programs involve reuse of shared code broadly across the program and have a densely-networked structure. </div> <p>We don‚Äôt think any of these high-level signatures of compositionality are particularly controversial. Formalizing these intuitions is more difficult, however: how do we move from <em>qualitative</em> and vague statements like the high-level signatures we outlined above to more <em>quantitative</em> ones that can be precisely expressed in mathematical expressions?</p> <p>One of the key challenges is to specify <em>which</em> library we‚Äôre talking about, since any program can be implemented using an infinite number of possible libraries while remaining functionally identical. Is there any way to do this in a non-arbitrary way? For a given program that one might want to implement, is there a notion of the ‚Äúcorrect‚Äù library one should write, or a library that is intrinsic to the program?</p> <p>It turns out that there is, and it can be arrived at through the goal of <em>compression</em>. Think of what any good programmer would try to do: they would write the reusable code like functions and classes in a way that makes the total length of the program <em>shortest</em>. Other than for reasons of clarity, no programmer would insist on writing a function for some piece of code that is only going to be executed a single time in the program, because doing so wouldn‚Äôt help make the program more concise as a whole.</p> <h2 id="growing-and-refactoring-libraries">Growing and refactoring libraries</h2> <p>We now have a way of talking about a program‚Äôs <em>intrinsic</em> library as the one that leads to the <em>best compression</em>. To talk about compositionality, we want to quantify the degree to which this library is <em>modular</em> ‚Äî whether it decomposes into <em>multiple</em> functions and classes that get reused in many places. We also want to quantify the degree to which this modularity is densely <em>networked and hierarchical</em> ‚Äî whether some elements of the library are used to define <em>other</em> elements inside it, like some functions being recombined to define new ones.</p> <p>If we can see the internals of the library and all of its components in detail, it might be possible to define some graph-theoretic metrics to quantify these things, but they risk being heuristic in nature and difficult to justify as truly general or theoretically-grounded. More problematically, while they may work for the specific case of quantifying compositional structure in programs, we‚Äôre primarily using programs as an intuition pump that we‚Äôll later abstract out from. We‚Äôre looking for an approach that can just as easily be applied to arbitrary sorts of data like music, art, or datasets used for machine learning. In these other cases, the ‚Äúlibrary‚Äù that we‚Äôll be talking about is less cleanly delineated into distinct modules and parts; clear boundaries might be fuzzy or non-existent, like the boundary between a solid or a liquid phase of matter, and we need our definition of compositionality to be robust here, too.</p> <p>We‚Äôre therefore going to take another approach to defining the compositionality of a program that asks how the library <em>grows</em> or <em>changes</em> when we consider increasingly large segments of the program. Basically, if you imagine chunking up a program into some <em>parts</em> (i.e., non-overlapping segments of code), compositional structure exists when the best library for compressing those parts is <em>different</em> from the best library for compressing the whole. Why must this be true? If the library of the whole is different from the libraries of the parts (e.g., it has additional functions), it necessarily means that there were <em>new</em> shared pieces of code among those parts that could be placed in the library as new modules to improve compression. Conversely, if the library of the whole is identical to the libraries of the parts, then the whole program necessarily can‚Äôt be more or less compositional than its parts (compositionality is a property of the library, and the libraries are identical); it is just a longer program. It‚Äôs only when the library changes, or is refactored, that new compositional structure necessarily emerges. This definition of compositionality in the case of computer programs is summarized below:</p> <blockquote class="notice--info" style="font-size:1.0em !important"> <p><strong>Compositional program</strong> (in English)</p> <p>A program is compositional with respect to some division into parts if the library that best compresses the whole differs from the libraries that best compress the parts.</p> </blockquote> <p>We want to quickly clarify a few things before moving on. First, even when we consider joining parts to make a whole, we‚Äôre asking whether or not <em>new</em> compositional structure emerges; even if it does not, the parts themselves might already have compositional structure. This brings us to our second point: this definition of compositionality can be recursively applied to the parts themselves in order to investigate compositionality hierarchically <em>at multiple scales</em>. Finally, this recursive property raises the question of <em>which</em> hierarchical decomposition(s) we should consider, which we‚Äôll have more to say about later in <a href="#section-3--implications-and-use-cases-for-ai">section 3</a>.</p> <h2 id="illustrative-examples">Illustrative examples</h2> <p>For a very abstract definition such as this one, there‚Äôs no substitute for concrete examples that illustrate paradigmatic cases of both compositional and non-compositional structure. In some sense this is the entire goal of defining things in the first place ‚Äî to include all positive cases while leaving out all negative ones using a simple expression ‚Äî so lets put this one to the test. As before, we‚Äôll make use of programs and libraries to build these concrete examples.</p> <p><strong>Brief notation</strong></p> <p>To avoid things getting to cumbersome, it‚Äôs time to introduce a tiny bit of notation. We‚Äôll call a program $x$ and its best library (the one that best compresses it) $m_x$. We‚Äôll denote the parts that we decomposed the program into with subscripts. We‚Äôll just consider splitting the program into two parts for the moment, so that gives us $x_a$ and $x_b$ as well as their corresponding best libraries $m_{x_a}$ and $m_{x_b}$. We also said that we‚Äôre quantifying novel compositional structure as the degree to which the library of the whole <em>changes</em> from the libraries of the parts, which implies some sort of distance metric. We‚Äôll call this distance $K(m_x \mid m_{x_a}, m_{x_b})$ for reasons that will become clearer in <a href="#section-2--a-formal-definition-for-quantifying-compositionality">section 2</a>.</p> <p><strong>Novel shared structure: compositional</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig2-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig2-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 2: Example of novel shared structure.</strong> Left block shows a program and right blocks show that program split into parts. To more clearly show shared code and how a program is split into parts, each program is written both with and without its library (bottom and top of a block, respectively). </div> <p>If $x_a$ and $x_b$ share some novel structure ‚Äî for instance, pieces of code that could be turned into a function ‚Äî then it makes sense to say that there‚Äôs additional compositional structure in $x$ that wasn‚Äôt already present in $x_a$ or $x_b$. This is exactly what is happening in the example of Figure 2, where in order to clearly show how a program is split into parts and which parts of the program share structure we have written them both with and without their libraries. In both $x_a$ and $x_b$, there was no sense in adding a <code class="language-plaintext highlighter-rouge">norm()</code> function to their respective libraries because there would have been no reuse; the overall programs would have been slightly longer if we had. Both the part libraries $m_{x_a}$ and $m_{x_b}$ are therefore empty. However, when we consider these parts together as a whole in $x$, suddenly it makes sense to write a <code class="language-plaintext highlighter-rouge">norm()</code> function to shorten the program because it will be used once.</p> <p>The definition we‚Äôve proposed accounts for this: $m_{x_a}$ and $m_{x_b}$ in the example are both empty because there is no code reuse that would benefit from being wrapped in a function, but $m_x$ on the other hand is not empty, making $K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0$. This is the most minimal case of compositionality that we can construct ‚Äî one ‚Äúpart‚Äù that is reused twice ‚Äî and the definition correctly identifies it. Of course, the definition would also pick up on more interesting cases in which $m_{x_a}$ and $m_{x_b}$ might not be empty, and $x_a$ and $x_b$ share more interesting structure (e.g., multiple segments of code reuse that result in multiple new functions). In general, what happens in these sorts of cases is that the library of the whole $m_x$ <em>grows</em> with respect to the libraries of the parts $m_{x_a}$ and $m_{x_b}$, and this is always reflected in $K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0$.</p> <p>It‚Äôs important to clarify once again what we mean by ‚Äúnovel shared structure‚Äù here. Clearly, the code that is shared between $x_a$ and $x_b$ was already present in each individually. However, when looking at those parts individually, the shared code does not show up in their libraries because it would not help us better compress them individually. It is only when we look at the <em>combination</em> $x = [x_a, x_b]$ that the shared code counts as compositional structure because it now helps us <em>better compress the whole</em>. The shared code itself is not what is novel, then, but rather the fact that this shared code now newly gets added to the library of the whole.</p> <p><strong>No shared structure: not compositional</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig3-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig3-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 3: Example of no shared structure.</strong> Left block shows a program and right blocks show that program split into parts. To more clearly show shared code and how a program is split into parts, each program is written both with and without its library (bottom and top of a block, respectively). </div> <p>A clear case that <em>lacks</em> compositional structure is when the best library we can build for $x$ just trivially combines those that were used for $x_a$ and $x_b$, such that $m_x = [m_{x_a}, m_{x_b}]$. This happens when $x_a$ and $x_b$ are unrelated to each other, such as in the example of Figure 3 where the two parts of a program serve entirely different functional purposes ‚Äî in other words, they are <em>algorithmically independent</em>. Despite $m_x$ being larger and having more functions than either $m_{x_a}$ or $m_{x_b}$, it does not make sense to talk about $x$ as being more compositional than its parts $x_a$ and $x_b$ because those functions do not interact in any way. Instead, it makes more sense to talk about $x$ as two separate compositional parts that join together but do not compositionally interact with each other, almost like two separate subprograms that have nothing to do with each other.</p> <p>Our definition accounts for this case because the work involved in constructing $m_x$ from $m_{x_a}$ and $m_{x_b}$, which is quantified in $K(m_x \mid m_{x_a}, m_{x_b})$, is trivially small: the two libraries just need to be concatenated.</p> <p><strong>Fully shared structure: not compositional</strong></p> <p>Another case lacking compositional structure is when the library for $x$ is identical to the one used for either $x_a$ or $x_b$. Again, we need to emphasize here that we‚Äôre talking about <em>additional</em> compositional structure in the combination $x = [x_a, x_b]$; it is entirely possible that $x_a$ or $x_b$ themselves have rich libraries of reusable functions. But if the library for $x$ is identical to that of either $x_a$ or $x_b$, there is no reason to think of $x$ as being <em>more</em> compositional than its parts. This is the case when $x$ is nothing more than a longer program than $x_a$ or $x_b$ that is in fact reusing the same structures as them throughout.</p> <p>Once again, our definition easily accounts for this case: $m_x$ is identical to one of $m_{x_a}$ or $m_{x_b}$, so it is trivially compressible from them.</p> <blockquote class="notice--primary" style="font-size:0.9em !important"> <p><strong>Box: Compositional generalization</strong></p> <p>We want to give another quick example along these same lines, but this time in the realm of computer vision, since it has important connections to machine learning and generalization. While we‚Äôve just spoken about programs up to this point, this brief digression will foreshadow how we‚Äôll soon generalize the ideas up to this point to arbitrary kinds of data. Imagine that $x_a$ is a dataset of (concatenated) scene images and that $x_b$ is one additional image (nothing says the two objects have to be the same size). For the moment, we can think of the ‚Äúlibraries‚Äù in this case as ‚Äúmodels‚Äù or collections of concepts (objects, possible relations between objects, etc.), although we‚Äôll make this much more precise in <a href="#section-2--a-formal-definition-for-quantifying-compositionality">section 2</a>.</p> <p>If $m_x = m_{x_a}$, it means that the new image $x_b$ consists entirely of known concepts, such that there is no additional structure it could provide. The new image $x_b$ contains the same objects, subparts, backgrounds, textures, and all other reusable structures that were already present in $m_{x_a}$. In this case, the best model of data $x_a$ is <em>also</em> the best explanation of $x = [x_a, x_b]$. This provides very general conditions under which we can meaningfully talk about <em>compositional generalization</em> and when it is even possible: a model can only compositionally generalize to new data, without undergoing additional learning, if that new data provides no additional compositional structure that would serve to change the model.</p> </blockquote> <p><strong>Building on top of existing structure: compositional</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig4-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig4-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig4.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 4: Example of building on top of existing structure.</strong> Left block shows a program and right blocks show that program split into parts. To more clearly show shared code and how a program is split into parts, each program is written both with and without its library (bottom and top of a block, respectively). </div> <p>Lets now consider a more interesting example that will showcase the reach of the definition we‚Äôve provided. A powerful benefit of compositionality is that it can allow us to describe some concepts as simple functions of others, whereas describing the concept from scratch might be pretty expensive. For instance, defining the concept of a ‚Äúthrone‚Äù without prior knowledge of concepts such as ‚Äúchair‚Äù and ‚Äúroyalty‚Äù can be quite messy.</p> <p>What does this look like for programs and libraries? Consider the example in Figure 4, where in the first part of a program $x_a$ we were doing a lot of linear algebra, and as such our library $m_{x_a}$ needed to define a function for matrix multiplication <code class="language-plaintext highlighter-rouge">def matmul(a, b): ...</code>. Now, lets say that for the second part of our program $x_b$ we were doing some geometry that benefited from a vector norm function <code class="language-plaintext highlighter-rouge">def norm(v): sqrt(sum([val ** 2 for val in v]))</code>. What will the optimal library $m_x$ look like when we consider $x_a$ and $x_b$ jointly? Crucially, the <code class="language-plaintext highlighter-rouge">norm</code> function <em>will change</em> in the new library because of novel shared structure between $x_a$ and $x_b$. In particular, $m_x$ will now define the function as <code class="language-plaintext highlighter-rouge">def norm(a): sqrt(matmul(a, a))</code> because this is shorter once we already have code for implementing matrix multiplication. We have compositionality here because we get to re-express some concept (the norm of a vector) in terms of other concepts (matrix multiplication and square root). Crucially, in this case, it‚Äôs not that the library $m_x$ grew with respect to $[m_{x_a}, m_{x_b}]$, but that it was <em>refactored</em> from them because novel shared structure resulted in a better strategy for compression.</p> <p>How does our definition account for this notion of compositionality? Effectively, we‚Äôll have that $K(m_x \mid m_{x_a}, m_{x_a}) &gt; 0$ because the implementation of the <code class="language-plaintext highlighter-rouge">norm</code> function in $m_x$ is a new component of the library ‚Äî it was implemented differently in $m_{x_b}$, and has to be rewritten. Granted, in this example $K(m_x \mid m_{x_a}, m_{x_a})$ will be quite small since the new <code class="language-plaintext highlighter-rouge">norm</code> function is very easy to write given the <code class="language-plaintext highlighter-rouge">matmul</code> function in $m_{x_a}$, but the point we‚Äôre making is more general. Whenever novel structures or concepts can be succinctly described in terms of others, our definition correctly identifies this as a case of compositionality.</p> <h2 id="generalization-to-any-object-good-libraries-as-occams-razor-models">Generalization to any object: good libraries as Occam‚Äôs razor models</h2> <p>We hope that the above discussion of programs and libraries has been helpful for building intuition about compositionality, but ultimately we need to generalize outside of this particular case. For this, we‚Äôll be replacing the notion of a program with that of <em>any</em> arbitrary data that can be expressed in bits. This encompasses things like music, images, <em>iid</em> datasets, non-stationary data streams, neural representations, functions, natural language utterances, computer programs ‚Äî basically, anything that is scientifically interesting. The bigger question is: what should we replace the notion of the program‚Äôs <em>library</em> with? The analogous concept turns out to be a <em>model</em> of the data.</p> <p>A model, like a program‚Äôs library, captures <em>structure</em> in the data: patterns of information that repeat in some shape or form, either in a quite literal sense (like a repeating subsequence of bits) or in more abstract ways (like a rule or template with adjustable parameters). Just like in a computer program, the patterns of information captured by a model can be interdependent and hierarchically-defined, like in a deep neural network where representations are built upon each other.</p> <p>What all this means is that a model, like a program‚Äôs library, can <em>compress</em> the data in ways we‚Äôll make more precise in <a href="#section-2--a-formal-definition-for-quantifying-compositionality">section 2</a>. A model can be thought of as a shorter explanation of a more complex object. If we give you a pattern of bits like <code class="language-plaintext highlighter-rouge">01001100011100001111...</code> and ask you what‚Äôs going on, you‚Äôll probably say something like ‚Äúalternate groups of 0‚Äôs and 1‚Äôs and increase the group size by one each time‚Äù ‚Äî that‚Äôs a model of the data (in this case a generative one) that can help us compress the string with far fewer bits because the model is simple and easy to encode. Even if the model isn‚Äôt perfect and has some degree of error (e.g., imagine corrupting the above string with a bit of noise), it can still help with compression because encoding the model along with a few error-correcting bits can be easier than encoding the entire string verbatim.</p> <blockquote class="notice--primary" style="font-size:0.9em !important"> <p><strong>Box: Models of individual objects rather than datasets</strong></p> <p>We want to quickly head off a potential confusion for the machine learning audience. We‚Äôre accustomed to modeling <em>datasets</em> (often <em>iid</em> ones), and so the idea of modeling an individual object or a nonstationary stream of data might seem strange. But rest assured that there is nothing wrong with modeling individual objects, and indeed it is not entirely abnormal to see this done in practice. For instance, Compositional Pattern-Producing Networks (CPPNs) <d-cite key="stanley_compositional_2007"></d-cite>, Neural Radiance Fields (NeRFs) <d-cite key="mildenhall_nerf_2021"></d-cite>, and various compressors <d-cite key="Balle_2025_CVPR"></d-cite><d-cite key="liao2025arcagiwithoutpretraining"></d-cite> all aim to model an individual objet like an image or a scene. Granted, the ordinary paradigm of sampling datapoints from the training set and minimizing the loss through gradient methods sometimes has to be adapted in such cases, but none of this is essential to modeling anyway.</p> </blockquote> <p>What model should we consider, though, given that we could select among an infinite number of models for any given data? Earlier we considered the library that <em>best compresses</em> a program, and we can take the same approach here. Many will have heard of the principle of Occam‚Äôs razor, where we say that the simplest explanation of some data is the best. In machine learning we follow this principle too, whether we realize it or not, when we search for models that achieve low training error (i.e., models that explain the data) but still generalize to the test set (i.e., models that aren‚Äôt more complex than they need to be, which would result in overfitting). The Occam‚Äôs razor model is an ideal. It is the <em>simplest</em> model that helps us <em>best compress</em> some data, and it is a non-arbitrary way to talk about some data‚Äôs ‚Äúintrinsic‚Äù or ‚Äútrue‚Äù model in the same way that the most compact implementation of a program gave us a meaningful notion of its intrinsic library.</p> <p>We‚Äôre now ready to state our definition for novel compositional structure in the general case. Instead of being a program, $x$ now represents <em>any</em> data that can be expressed in information ‚Äî in bits. Instead of talking about the library that best compresses a program, $m_x$ is now the Occam‚Äôs razor model of the data that allows us to <em>best compress</em> $x$. Since we‚Äôve spoken enough about compression up to this point, we‚Äôre also ready to clarify what $K(m_x \mid m_{x_a}, m_{x_b})$ means: it is the cost of trying to compress the model of the whole given the models of the parts ‚Äî a quantity that we‚Äôll later formalize using Kolmogorov complexity. Below is the succinct definition, in English:</p> <blockquote class="notice--info" style="font-size:1.0em !important"> <p><strong>Compositionality</strong> (in English)</p> <p>An object is compositional with respect to some division into parts if the model that best compresses the whole isn‚Äôt easily derived from the models that best compress the parts.</p> </blockquote> <p>Notice that all of the illustrative examples covered earlier for computer programs still straight-forwardly apply in the general case. Let‚Äôs take $x$ to be a pair of images, for instance. Novel shared structure (compositional) might involve the two images $x_a$ and $x_b$ sharing an object that appears in neither image individually. A case of no shared structure (uncompositional) might involve two images with entirely different semantic and structural content, like an image of cells under a microscope and an image of the Rocky Mountains (although the example isn‚Äôt perfect, as there is still shared low-level structure). A case of fully shared structure (uncompositional) might be two different images of cells under a microscope. Finally, we already gave an example of structures building on top of others earlier: an image of a throne on its own might involve a complex model, but when joined together with images whose models include the notions of a chair and of royalty, the right way to model a throne now becomes to define it in terms of those pre-existing concepts. Analogous examples can easily be constructed for other kinds of data as well, reflecting the generality of this definition of compositionality.</p> <hr/> <h1 id="section-2--a-formal-definition-for-quantifying-compositionality">Section 2 ‚Äî A formal definition for quantifying compositionality</h1> <p>In this section, we‚Äôll be formalizing all of the things we‚Äôve said up to this point and making our definition of compositional structure mathematically precise. In particular, we‚Äôll be clarifying the notion of an ‚ÄúOccam‚Äôs razor model‚Äù $m_x$ of data $x$, as well as the distance metric $K(m_x \mid m_{x_a}, m_{x_b})$ that we‚Äôve been using to quantify novel compositional structure. Many might already have a very solid intuitive understanding of our definition at this point without the need for more formalisms, and this is no accident: the definition is built on the foundations of <em>algorithmic information theory</em>, which is one of the most intuitive yet powerful branches of mathematics we‚Äôve encountered. Some parts of this section may feel tedious ‚Äî we‚Äôll be introducing a lot of background and new notation ‚Äî but if you stick with it, we think that you‚Äôll come away with not only a sharper understanding of compositionality, but also a deeper grasp of far-reaching concepts like information, complexity, structure, modeling, Occam‚Äôs razor, and compression.</p> <h2 id="kolmogorov-complexity-and-optimal-compression">Kolmogorov complexity and optimal compression</h2> <p>Kolmogorov complexity <d-cite key="Kolmogorov01011968"></d-cite> ‚Äî the most important concept in algorithmic information theory ‚Äî is a formal way to quantify information. Most people are familiar with the Shannon notion of information, so we‚Äôll briefly start there. Shannon information quantifies the amount of information contained in an object $x$ as the length of a coded message that a speaker would need in order to communicate $x$ to a listener. Assuming that $x$ is drawn from some distribution $p$ that is known to both the speaker and the listener, it turns out that the optimal coding scheme that achieves the minimal message length in expectation encodes $x$ using $-\log_2 p(x)$ bits ‚Äî intuitively, we assign shorter codes to events that are more frequent.</p> <p>Kolmogorov complexity goes one step beyond Shannon information by dropping the assumption that the distribution $p$ is known to both the speaker and listener, and in fact drops the assumption that $x$ is drawn from any distribution at all. In Kolmogorov complexity, we instead only ask one thing: how <em>compressible</em> is $x$? The way that we do this is that we fix a Turing-complete programming language (Python, for instance), and we ask <em>what is the length of the shortest program that I can write which outputs $x$.</em> We denote this quantity $K(x)$.</p> <blockquote class="notice--info" style="font-size:1.0em !important"> <p><strong>Kolmogorov complexity</strong></p> <p>Given some finite string $x$ and a universal Turing machine $U$, the Kolmogorov complexity $K(x)$ is the length $l(r)$ (in bits) of the <strong>shortest</strong> binary program $r$ that prints $x$ and halts:</p> \[K(x) = \min_r \{l(r) : U(r) = x, r \in \{0, 1\}^* \}\] </blockquote> <p>Kolmogorov complexity has many intuitive properties that make it attractive as a measure of information quantity. The smaller and the more structure an object has ‚Äî regularity, patterns, rules, etc. ‚Äî the more easily it can be compressed using a short program and the lower its Kolmogorov complexity. For instance, a sequence with repeating patterns or a dataset that spans a low-dimensional subspace can be significantly compressed relative to its original size, and this results in low Kolmogorov complexity. In contrast, a random string devoid of any structure cannot be compressed at all and must in effect be ‚Äúhard-coded‚Äù, making its Kolmogorov complexity equal to its original size in bits.</p> <p>There‚Äôs also a conditional notion of Kolmogorov complexity that will be useful, denoted $K(y \mid x)$, which is equal to the length of the shortest program <em>which takes $x$ as input</em> and outputs $y$. Intuitively, this measures the amount of leftover information in $y$ given that we already know $x$. Conditional Kolmogorov complexity $K(y \mid x)$ is of course always less than or equal to $K(y)$ given that we have the option of simply ignoring the input $x$, and it can be significantly smaller than $K(y)$ when $x$ and $y$ share a lot of structure.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig5-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig5-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig5.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 5: Kolmogorov complexity and conditional Kolmogorov complexity.</strong> Kolmogorov complexity is the length of the shortest program that outputs an object, and quantifies information through the lens of compression. Conditional Kolmogorov complexity is the length of a shortest program that takes one or more objects as input and outputs another. </div> <h2 id="sophistication-and-occams-razor">Sophistication and Occam‚Äôs razor</h2> <p>While powerful, Kolmogorov complexity isn‚Äôt completely satisfying as a universal measure of information quantity because it makes no distinction between <em>meaningful,</em> <em>structural</em> information and <em>random, unstructured</em> information. This is easiest to explain through examples. Consider a binary string $x$ that consists exclusively of a repeating sequence of $1$‚Äôs: it‚Äôs intuitively quite a simple object, and indeed $K(x)$ is quite low because we can print $x$ using a simple for-loop. Now, consider the opposite case of a binary string $y$ that consists of an entirely random sequence of $0$‚Äôs and $1$‚Äôs: $K(y)$ is maximally large because we have no other choice but to hard-code $y$. In one respect this makes sense ‚Äî $y$ is incompressible, so in that respect it is indeed ‚Äúcomplex‚Äù. But there is also a sense in which $y$ is strikingly simple, and in fact just as simple as a constant string like $x$. In particular, neither $y$ nor $x$ can really be said to have complex <em>structure</em>. They are equally boring. Even though $y$ must be described with a very large program, that program itself doesn‚Äôt contain much interesting logic outside of simply hard-coding bits, and the distribution from which $y$ might have been drawn would just take a few lines of code to define. In contrast, we can easily imagine a string $z$ that is also difficult to compress with high $K(z)$, but because it contains significant structure (i.e., interesting and sophisticated decompression code) rather than arbitrary randomness.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig6-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig6-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig6.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 6: Shortcoming of Kolmogorov complexity in measuring structural information.</strong> Kolmogorov complexity does not distinguish between *structural/meaningful* information and *unstructured/random* information. </div> <p>This distinction between <em>structural</em> and <em>random</em> information was important to Kolmogorov himself, who quantified it through a new metric called <em>sophistication</em>. The idea is that we can consider strategies for compressing an object that work in two stages: first we encode some model of the object, and then second we encode the remaining bits of random information unaccounted for by the model. These compression strategies are called ‚Äútwo-part codes‚Äù. Let‚Äôs consider the model class of computable probability distributions as an example (although this also works with more general model classes). An optimal ‚Äútwo-part code‚Äù for an object $x$ that achieves the best possible compression is one that satisfies <d-cite key="grunwald_algorithmic_2008"></d-cite><d-cite key="vitanyi_meaningful_2006"></d-cite>:</p> \[K(x) = K(p_x) - \log_2 p_x(x)\] <p>Here, $p_x$ is the probability distribution that we are using to model $x$ and $K(p_x)$ is the minimum number of bits that it takes to implement this probability distribution in a concrete computer program. Because $p_x$ might not be a perfect model of $x$, we also have to account for error correction bits, which in this case correspond to the ordinary Shannon information $-\log_2 p_x(x)$ (recall that Shannon information assumes access to the probability distribution from which $x$ is drawn, which we have accounted for in $K(p_x)$). There is always <em>at least</em> one model that satisfies this equality. For instance, $p_x$ can trivially put all its probability mass on $x$, in which case it takes $K(p_x) = K(x)$ bits to encode and achieves an error of $-\log_2p_x(x) = 0$ bits. However, in general, there can be <em>many</em> solutions. An interesting model to prefer above the others, though, is the one that is simplest:</p> \[p_x = \mathop{\arg\,\min}\limits_{p_x'} \{ K(p_x') : K(x) = K(p_x') - \log_2 p_x'(x) \}\] <p>Note that in the above equation and for the rest of the blog post, the distribution $p_x$ does <em>not</em> just represent <em>any</em> model of $x$: it represents the <em>simplest one that best compresses it</em>. This particular model is not arbitrary, but rather is intrinsically defined purely in terms of $x$, which is why we have chosen to denote it $p_x$ where the subscript indicates the dependence on $x$.</p> <blockquote class="notice--primary" style="font-size:0.9em !important"> <p><strong>Box: Distributions as models of individual objects?</strong></p> <p>By this point, for some readers a tension will have emerged. On the one hand, we‚Äôve stated that Kolmogorov complexity is about individual objects and that $x$ represents an <em>instance</em> rather than a random variable. On the other hand, we‚Äôve also been saying that we can use a probability distribution to model $x$, but probability distributions are generally models of distributions and their random variables.</p> <p>We are indeed still talking about individual objects in this section, and we are making no assumptions whatsoever that $x$ has been drawn from a distribution. Nevertheless, for the purposes of compression, it can be convenient to pretend <em>as if</em> $x$ was drawn from a distribution. In particular, $x$ might look complex, but it might be possible to specify a <em>simple</em> distribution under which $x$ looks ‚Äútypical‚Äù (i.e., it has high likelihood under this distribution, and is easily encoded through it).</p> <p>We also want to close this discussion by emphasizing that even though $x$ is as single object, nothing stops us from considering a single object consisting of <em>multiple</em> draws from a distribution. Kolmogorov complexity is defined over strings, and we can easily represent something like an <em>iid</em> dataset by, for instance, concatenating individual datapoints together.</p> </blockquote> <p>There‚Äôs a nice parallel to machine learning theory here, and in particular the principle of Occam‚Äôs razor: we are looking for a simple model with low $K(p_x)$ that nevertheless accurately explains the data with low error $-\log_2 p_x(x)$. The complexity of this model $K(p_x)$ is what Kolmogorov described as the <em>sophistication</em> of the string $x$, and the Occam‚Äôs razor model $p_x$ is sometimes called the <em>algorithmic minimal sufficient statistic</em> <d-cite key="grunwald_algorithmic_2008"></d-cite><d-cite key="vitanyi_meaningful_2006"></d-cite>. Sophistication is precisely the quantity that we are looking for in order to distinguish between structured and unstructured information, where $K(x)$ alone was insufficient. If a string is too simple (e.g., a repeating pattern of $1‚Äôs$), it can be best compressed by a simple model with low $K(p_x)$. On the other hand, if a string is complex because of random noise rather than structure, $K(p_x)$ is <em>still</em> low because random noise distributions are easy to implement in just a few lines of code. It is only when a string has interesting and complex structure that $K(p_x)$ is high, meaning that the string is best compressed by a complex model.</p> <p>We said earlier that sophistication is defined with respect to some class of models, and that computable probability distributions are just one option. For the rest of the post, to remain more general, we‚Äôll therefore switch to the notation $m_x$ instead of $p_x$ to denote the Occam‚Äôs razor model of string $x$.</p> <h2 id="defining-compositionality-through-algorithmic-information-theory">Defining compositionality through algorithmic information theory</h2> <p>We now have all the tools that we need to formally define compositional structure in data, and we can do so essentially just by replacing some of the language in our earlier definition with more precise mathematics. To briefly revisit this intuition, our central argument was that compositionality emerges from <em>novel</em> structure that is <em>shared</em> between an object‚Äôs parts. This is what that looks like in math:</p> <blockquote class="notice--info" style="font-size:1.0em !important"> <p><strong>Compositionality</strong></p> <p>An object $x$ is compositional with respect to some division into parts $x = [x_a, x_b]$ if:</p> \[K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0\] <p>where the Occam‚Äôs razor model of the data $m_x = \mathop{\arg\,\min}\limits_{m_x‚Äô} \{ K(m_x‚Äô) : K(x) = K(m_x‚Äô) + l_{m_x}(x) \}$, and both $m_{x_a}$ and $m_{x_b}$ are defined similarly. The <em>degree</em> of compositional structure is $K(m_x \mid m_{x_a}, m_{x_b})$.</p> <p><em>Note</em>: $l_{m_x}(x)$ represents the unstructured information in $x$ left unspecified by the model $m_x$. For instance, for the model class of computable probability distributions $l_{m_x}(x) = -\log_2 m_x(x)$.</p> </blockquote> <p>Once again, we need to emphasize that this definition only considers novel compositional structure in $x$ that wasn‚Äôt already present in $x_a$ or $x_b$ because it conditions on $m_{x_a}$ and $m_{x_b}$ ‚Äî for instance, novel shared structure between two images, such as objects that appear in both but not in either individually. This definition does <em>not</em> preclude the possibility that the individual substrings $x_a$ and $x_b$ might themselves have compositional structure, and indeed this is an advantage. As we‚Äôll show in <a href="#section-3--implications-and-use-cases-for-ai">section 3</a>, this helps us easily think about hierarchical compositionality at <em>different scales</em>, since we can simply consider further divisions of the substrings $x_a$ and $x_b$ themselves.</p> <hr/> <h1 id="section-3--implications-and-use-cases-for-ai">Section 3 ‚Äî Implications and use-cases for AI</h1> <p>We‚Äôve proposed a definition where compositionality is the emergence of novel structure shared between parts. While mathematically rigorous, is it useful? We believe this definition does more than just quantify; it offers a normative framework for understanding why current AI succeeds, how natural intelligence grows, and how we might architect the next generation of open-ended systems.</p> <h2 id="modeling-hierarchical-structure-in-the-real-world">Modeling hierarchical structure in the real world</h2> <p>Real-world data is rarely flat; it is compositional all the way down. Consider an image: simple edges form shapes, shapes form textures, textures form objects, and objects form scenes.</p> <p>Our definition captures this recursion naturally. If we split an image patch $x$ into smaller patches $x_a$ and $x_b$, we find $K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0$ because the patches share low-level structure (like edges). But when we consider joining two of those larger patches themselves, we can <em>again</em> detect novel compositional structure because the new model will benefit from describing things like simple textures and shapes which reoccur among the two larger patches.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig7-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig7-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig7.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 7: Hierarchical compositional structure.</strong> In real compositional data, novel shared structure at larger scales is built off of other structure at smaller scales. </div> <p>We can keep playing this game of joining larger segments of data to detect compositionality at increasingly large scales. In the compositionality literature, people typically only consider shared structure between individual datapoints (e.g., by modeling ‚Äúobjects‚Äù that get reused among images). We just showed how our definition can account for compositionality at smaller scales, such as <em>within</em> an individual image, but it can also account for compositionality at larger scales that consider <em>sequences</em> of observations in a non-stationary data stream.</p> <p><strong>The alignment with deep learning</strong> This perspective offers a theoretical grounding for the success of deep learning. Neural networks essentially mirror the hierarchical structure of natural data.</p> <ul> <li><strong>Layer-wise isomorphism:</strong> Each layer in a deep net can be viewed as a model of the data at a specific scale.</li> <li><strong>Capacity for novelty:</strong> Successive layers do not merely copy information; they compute novel models of the data built off of interactions between lower-level models.</li> </ul> <p>This suggests that to model the world at increasingly large scales ‚Äî moving from single images to long-form video or life-long streams ‚Äî we may need architectures that can deepen unboundedly, continuously capturing new structure as it emerges.</p> <h2 id="the-time-dimension-open-endedness-and-data-curricula">The time dimension: open-endedness and data curricula</h2> <p>While standard machine learning focuses on <em>iid</em> datasets, humans are generally confronted with data that extends deep in time and grows in complexity. Knowledge in the real world isn‚Äôt just static; it <em>grows</em> compositionally. New concepts (like calculus) are not learned in isolation but are refactored on top of existing ones (like algebra).</p> <p>Our definition provides a rigorous test for whether a data curriculum is truly open-ended. Consider a data stream $x$ decomposed into history $x_a$ and a new observation $x_b$:</p> <ul> <li><strong>Redundancy ($m_x = m_{x_a}$):</strong> If $x_b$ adds no new patterns, learning stalls.</li> <li><strong>Disorder ($m_x = [m_{x_a}, m_{x_b}]$):</strong> If $x_b$ is unrelated to history (random noise or isolated facts about the world), the model effectively resorts to brute-force memorization.</li> <li><strong>Compositional growth ($K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0$):</strong> This occurs only when the new observation $x_b$ shares structure with history $x_a$ in a way that forces the global model $m_x$ to evolve or refactor. Such data allows a world model to grow in a hierarchical way that efficiently leverages past experience, and allows new experiences to shape its understanding of old ones.</li> </ul> <blockquote class="notice--primary" style="font-size:0.9em !important"> <p><strong>Box: Fractured/Entangled vs. Unified/Factored representations</strong></p> <p>This perspective has implications for a fascinating new paper ‚ÄúQuestioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis‚Äù by Kumar et al. <d-cite key="kumar_questioning_2025"></d-cite>, which found that neural representations which arise from open-ended curricula demonstrate significantly more compositional structure than those of models trained only on the final endpoints of these curricula.</p> <p>One conclusion the authors draw is that compression, while important, is not the only thing that a model should strive for ‚Äî after all, the neural network they train on the final endpoint of a curriculum does a good job in terms of compression, but learns poor representations nonetheless. Our theory of compositionality suggests a different conclusion, however: optimal compression is <em>always</em> what we should strive for when modeling, but crucially a <em>curriculum might have significantly more compositional structure than its endpoint, and its optimal compression might therefore capture this compositional structure far better</em>.</p> <p>Take school curricula again, for instance. If our understanding of calculus builds off of prior concepts in algebra, we‚Äôll be better able to jointly compress <em>both</em> through concept reuse. In contrast, if our only goal was to develop a compressed model of calculus in isolation, who knows what it would look like (imagine having learned calculus without knowing algebra first; you would certainly think of it in a very different way). There‚Äôs a crucial difference between compressing an entire curriculum versus compressing its endpoint alone. All of these interpretations of Kumar et al.‚Äôs fascinating findings are testable, and have important consequences for machine learning and the role of open-ended curricula.</p> </blockquote> <h2 id="visualizing-and-defining-intrinsic-structure">Visualizing and defining intrinsic structure</h2> <p>To move beyond abstract equations, we can map these concepts into <strong>compositional structure diagrams</strong>. These are tree-based visualizations where branches represent the merging of parts, and the height of a branch represents the quantity of novel structure $K(m_x \mid m_{x_a}, m_{x_b})$ emerging at that merge.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig8-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig8-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-compositionality/fig8.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 8: Compositional structure diagrams.</strong> A diagram that shows how compositional structure emerges at different scales in an object, according to a recursive decomposition (legend in the top left of the figure). A given node represents a substring of the entire object, and branches represent substrings merging to form a larger string. The height of a branch above a merge point represents the amount of novel compositional structure that emerges from that combination of substrings (heights of branches below a merge point do not represent anything, and just vary to improve spacing in the diagram). Note: in order to keep the diagram clean, we‚Äôre not showing the intermediate binary joins that don‚Äôt generate significant novel structure. </div> <p>This kind of visualization of our definition shows us how larger structures are built off of novel structure shared at smaller scales ‚Äî like how concepts build off of others ‚Äî and it shows us how this happens recursively through the object. There‚Äôs a nice analogy to be made to phylogenetic trees in biology, but whereas phylogenetic trees show a plausible story of how genetic structure diverges over time and splits off into new species, compositional structure diagrams show a plausible story of how structures merge to form novel ones (which, according to a recent theory of evolution called symbiogenesis <d-cite key="arcas_what_2025"></d-cite>, might actually be how we should think of phylogenetic histories in the first place).</p> <p><strong>The ‚Äúmaximal tree‚Äù hypothesis</strong> A central question remains: <em>which</em> hierarchical decomposition is the ‚Äúcorrect‚Äù one? While the answer may depend on the domain, we propose that the <strong>maximal tree</strong> decomposition ‚Äî the one that maximizes the sum of novel structure at every node ‚Äî is the most scientifically interesting candidate for an object‚Äôs intrinsic structure.</p> <p>We favor the maximal tree $T_{max}(x)$ for several reasons:</p> <ul> <li><strong>Intuition and non-triviality:</strong> It inherently groups parts that maximize shared structure, avoiding arbitrary cuts. In contrast, if we were looking at <em>minimal</em> trees, we would be joining objects that are either structurally identical or algorithmically independent.</li> <li><strong>Different from sophistication:</strong> There exists objects with the same sophistication $K(m_x)$ but <em>different</em> sums of compositionality terms under the maximal tree decomposition. This means that when we consider the maximal tree, our measure of compositionality does not simply reduce to sophistication ‚Äî it quantifies a <em>kind</em> of structure rather than just the total amount of structure. The opposite is true for <em>minimal</em> trees: there is always a trivial solution that merges every individual bit of a string in one single merge, resulting in a sum of compositionality terms equal to $K(m_x)$.</li> <li><strong>Consistency:</strong> It has a kind of recursive consistent: compositional substrings retain their structure under the maximal tree when embedded in larger compositional objects.</li> </ul> <h2 id="practical-use-estimation-and-design">Practical use: estimation and design</h2> <p>Finally, we must address the elephant in the room: Kolmogorov complexity is uncomputable. Does this render the definition useless? Absolutely not.</p> <ol> <li><strong>Estimation via compression:</strong> Just as we upper-bound Kolmogorov complexity with concrete compression algorithms like ZIP or PAQ, we can estimate compositionality using standard learning algorithms and compressors.</li> <li><strong>A normative north star:</strong> Even without precise measurement, the definition guides design. It shifts the focus from purely architectural inductive biases toward <strong>data-centric compositionality</strong>. It suggests that the bottleneck to general AI isn‚Äôt just how we learn from data, but ensuring we collect data streams rich enough to satisfy $K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0$ continuously over time.</li> </ol> <hr/> <h1 id="conclusion">Conclusion</h1> <p>Our work on formally defining compositionality did not start out of nowhere: it arose for a particular reason. We were interested in designing models that can flexibly generalize like humans do when we think, dynamically composing concepts in order to adapt to novel situations. The further we got into these projects, however, the more we felt like we no longer understood the original goal, unable to explain what the scientific problems were or why the approaches we were pursuing would work. Eventually, we started to realize that we take compositionality for granted, using it in our vocabulary for talking about AI without having any real clue as to what it means.</p> <p>This work has filled that conceptual gap for us, and given us renewed clarity on our research and scientific interests. We hope that it can serve a similar purpose for others in the field who are reading this blog post and interested in compositionality. However, like all scientific formalisms, we think that the real significance of these ideas lies in the serendipitous directions they may lead future research. For us, having a new way to think and talk about compositionality more precisely has opened the flood gates: we see new research ideas, applications, and connections to other fields everywhere we look. We‚Äôve tried to highlight some of these here, but our biggest hope in writing this blog post is that it has a similar stimulating effect for others, and that research into compositionality benefits from a new pool of creative ideas.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Compositionality is thought to be crucial in human cognition and AI, but we lack a scientific understanding of what it is. What kind of data is compositionally structured? Can we mathematically quantify the amount and character of compositional structure? This blog post introduces a novel approach for doing so, building off of existing tools from algorithmic information theory that formalize notions of complexity and structure. The mathematical definition of compositionality that we'll come to is rigorous, precise, and general, and the hope is that it can inspire novel research directions in AI for uncovering compositional structure in natural data.]]></summary></entry><entry><title type="html">From U-Nets to DiTs: The Architectural Evolution of Text-to-Image Diffusion Models (2021‚Äì2025)</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/diffusion-architecture-evolution/" rel="alternate" type="text/html" title="From U-Nets to DiTs: The Architectural Evolution of Text-to-Image Diffusion Models (2021‚Äì2025)"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/diffusion-architecture-evolution</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/diffusion-architecture-evolution/"><![CDATA[<style>[data-theme="dark"] figcaption.caption{color:white!important}[data-theme="dark"] .key-differences{color:black!important}[data-theme="dark"] .key-differences strong{color:black!important}[data-theme="dark"] .key-differences li{color:black!important}[data-theme="light"] figcaption.caption{color:black!important}[data-theme="light"] .key-differences{color:black!important}[data-theme="light"] .key-differences strong{color:black!important}[data-theme="light"] .key-differences li{color:black!important}[data-theme="light"] .themed-image{content:url("/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/teaser_white.png")}[data-theme="dark"] .themed-image{content:url("/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/teaser_black.png")}.key-differences{border:2px solid #ff9800;background-color:#fff3e0;padding:15px;border-radius:5px;margin:15px 0}.key-differences ul{margin-top:10px;margin-bottom:0}</style> <div class="l-page"> <figure class="themed-figure"> <img class="themed-image" alt="A hero image summarizing the evolution of diffusion model architectures from U-Nets to Transformers." src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/teaser_white.png"/> <figcaption class="caption">Diffusion Image Model Architecture Evolution.</figcaption> </figure> </div> <h2 id="tldr">TL;DR</h2> <p>As diffusion systems scale, the biggest wins tend to come from leveraging compute with broad, general methods rather than hand-crafting ever more specific tricks <d-cite key="richsuttonBitterLesson2019"></d-cite>. At the same time, we should keep sight of the ‚Äúhardware lottery‚Äù: what succeeds can reflect today‚Äôs accelerators and tooling as much as inherent merit <d-cite key="hookerHardwareLottery2021"></d-cite>.</p> <h2 id="preliminaries-diffusion-models-for-image-generation">Preliminaries: Diffusion Models for Image Generation</h2> <p>Diffusion models have emerged as a powerful paradigm for generative modeling by learning to reverse a gradual noise corruption process. The fundamental approach involves two key stages: a <strong>forward diffusion process</strong> that systematically adds noise to data until it becomes pure Gaussian noise, and a <strong>reverse denoising process</strong> where a neural network gradually removes this noise to generate new samples.</p> <p>This framework has demonstrated remarkable success across diverse domains including image generation, audio synthesis, video generation, and even applications in natural language processing and molecular design. The generality of the diffusion framework makes it particularly attractive for complex generative tasks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM.png" width="100%" height="auto" alt="Diagram showing the forward noising process and the reverse denoising process in diffusion models." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The Markov chain for the forward and reverse diffusion processes, which generate a sample by slowly adding (and removing) noise. Image Credit: <d-cite key="wengWhatAreDiffusion2021"></d-cite></figcaption> </figure> <p>For readers seeking a comprehensive introduction to diffusion model fundamentals, we recommend Yang Song‚Äôs excellent exposition on <a href="https://yang-song.net/blog/2021/score/">score-based generative modeling</a> <d-cite key="song2019generative"></d-cite> and Lilian Weng‚Äôs detailed overview of <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">diffusion models</a> <d-cite key="wengWhatAreDiffusion2021"></d-cite>.</p> <h2 id="interactive-timeline">Interactive Timeline</h2> <div class="l-page"> <iframe src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-diffusion-architecture-evolution/timeline.html" frameborder="0" scrolling="yes" height="700px" width="100%"></iframe> </div> <h2 id="the-u-net-era">The U-Net Era</h2> <p>The early pioneering works in diffusion-based image generation predominantly adopted <strong>U-Net architectures</strong> <d-cite key="ronnebergerUNetConvolutionalNetworks2015"></d-cite> as their neural network backbone. This choice was largely influenced by U-Net‚Äôs proven success in various computer vision tasks <d-cite key="linRefineNetMultiPathRefinement2017"></d-cite><d-cite key="salimansPixelCNNImprovingPixelCNN2017"></d-cite>.</p> <p>The foundational models in this era established the core principles of diffusion-based generation. <strong>NCSN</strong> (Noise Conditional Score Network) <d-cite key="song2019generative"></d-cite> pioneered score-based generative modeling using a RefineNet backbone <d-cite key="linRefineNetMultiPathRefinement2017"></d-cite>, while <strong>DDPM</strong> (Denoising Diffusion Probabilistic Models) <d-cite key="hoDenoisingDiffusionProbabilistic2020"></d-cite> established the probabilistic framework using a PixelCNN++ architecture <d-cite key="salimansPixelCNNImprovingPixelCNN2017"></d-cite>. Subsequent refinements including <strong>NCSNv2</strong> <d-cite key="songImprovedTechniquesTraining2020"></d-cite>, <strong>IDDPM</strong> <d-cite key="nicholImprovedDenoisingDiffusion2021"></d-cite>, <strong>ADM</strong> (Ablated Diffusion Model) <d-cite key="dhariwalDiffusionModelsBeat2021"></d-cite>, and <strong>SDE</strong> (Score-based Diffusion via Stochastic Differential Equations) <d-cite key="songScoreBasedGenerativeModeling2021"></d-cite> built upon these foundations with architectural variations similar to DDPM or NCSN. However, these early models focused primarily on unconditional image generation and lacked text-to-image capabilities.</p> <p>The breakthrough for text-to-image generation came with <strong>LDM</strong> (Latent Diffusion Models, also known as Stable Diffusion) <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite>, which introduced a latent U-Net architecture to enable efficient text-conditioned generation. Following this success, several notable U-Net-based text-to-image models emerged, each exploring different architectural innovations within the U-Net paradigm:</p> <table> <thead> <tr> <th>Model</th> <th>Gen. (#Param)</th> <th>Txt. (#Param)</th> <th>Total (#Param)</th> <th>Release Date</th> </tr> </thead> <tbody> <tr> <td>SD v2.1 <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite></td> <td>0.87B</td> <td>0.34B</td> <td>1.29B</td> <td>2022-12-07</td> </tr> <tr> <td>Kandinsky <d-cite key="razzhigaevKandinskyImprovedTexttoImage2023"></d-cite></td> <td>1.23B</td> <td>0.56B</td> <td>1.86B</td> <td>2023-01-01</td> </tr> <tr> <td>UniDiffuser <d-cite key="baoOneTransformerFits2023"></d-cite></td> <td>0.95B</td> <td>0.12B</td> <td>1.25B</td> <td>2023-05-12</td> </tr> <tr> <td>SDXL <d-cite key="podellSDXLImprovingLatent2024"></d-cite></td> <td>2.57B</td> <td>0.82B</td> <td>3.47B</td> <td>2023-06-25</td> </tr> <tr> <td>Kandinsky 3 <d-cite key="arkhipkinKandinsky30Technical2024"></d-cite><d-cite key="arkhipkinKandinsky3TexttoImage2024"></d-cite></td> <td>3.06B</td> <td>8.72B</td> <td>12.05B</td> <td>2023-12-11</td> </tr> <tr> <td>Stable Cascade (W√ºrstchen) <d-cite key="perniasWurstchenEfficientArchitecture2024"></d-cite></td> <td>1.56B</td> <td>0.69B</td> <td>2.28B</td> <td>2024-02-07</td> </tr> </tbody> </table> <p>The standard U-Net architecture for diffusion models typically consists of an <strong>encoder</strong> that progressively downsamples the noisy input, a <strong>bottleneck</strong> middle block that processes compressed representations, and a <strong>decoder</strong> that upsamples back to the original resolution. Crucially, <strong>skip connections</strong> preserve fine-grained spatial information across corresponding encoder and decoder stages.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration.png" width="100%" height="auto" alt="U-Net backbone used in diffusion models with time conditioning injected into residual blocks and skip connections between encoder and decoder." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">A typical U-Net backbone used in diffusion models with time conditioning. Time representation uses sinusoidal positional embeddings or random Fourier features; these time features are injected into residual blocks via simple spatial addition or adaptive group normalization layers. Image Credit: <d-cite key="CVPR2023Tutorial"></d-cite>.</figcaption> </figure> <h2 id="the-dits-era">The DiTs Era</h2> <p>As U-Net‚Äìbased models began to hit a scaling ceiling (e.g., SDXL with ~2.6B parameters <d-cite key="podellSDXLImprovingLatent2024"></d-cite>), naive scaling proved ineffective, motivating a shift towards alternative backbones. The introduction of Diffusion Transformers (DiTs) <d-cite key="Peebles_2023_ICCV"></d-cite> marks a significant paradigm shift by recasting image generation as a patch-sequence modeling problem solved with transformer blocks. This approach offers several key advantages over U-Nets, including superior <strong>scalability</strong> via stacked DiT blocks, the ability to capture <strong>global context</strong> via self-attention for long-range dependencies, and a <strong>unified</strong> architecture that leverages advances in multimodal integration.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/dit-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/dit-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/dit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/dit.png" width="100%" height="auto" alt="DiT Architecture." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The Diffusion Transformer (DiT) architecture. Left: We train conditional latent DiT models. The input latent is decomposed into patches and processed by several DiT blocks. Right: Details of our DiT blocks. We experiment with variants of standard transformer blocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens. Adaptive layer norm works best. Image Credit: <d-cite key="Peebles_2023_ICCV"></d-cite>.</figcaption> </figure> <table> <thead> <tr> <th>Model</th> <th>Gen. (#Param)</th> <th>Txt. (#Param)</th> <th>Total (#Param)</th> <th>Release Date</th> </tr> </thead> <tbody> <tr> <td>PixArt-$\alpha$ <d-cite key="chenPixArtaFastTraining2024"></d-cite></td> <td>0.61B</td> <td>4.76B</td> <td>5.46B</td> <td>2023/10/06</td> </tr> <tr> <td>Lumina-T2I <d-cite key="gaoLuminaT2XScalableFlowbased2025a"></d-cite></td> <td>~4.7B</td> <td>~7B</td> <td>~15B</td> <td>2024/04/01</td> </tr> <tr> <td>PixArt-$\Sigma$ <d-cite key="chenPIXARTSWeaktoStrongTraining2024a"></d-cite></td> <td>0.61B</td> <td>4.76B</td> <td>5.46B</td> <td>2024/04/11</td> </tr> <tr> <td>Lumina-Next-T2I <d-cite key="zhuoLuminaNextMakingLuminaT2X2024a"></d-cite></td> <td>1.75B</td> <td>2.51B</td> <td>4.34B</td> <td>2024/05/12</td> </tr> <tr> <td>Stable Diffusion 3 <d-cite key="esserScalingRectifiedFlow2024"></d-cite></td> <td>2.03B</td> <td>5.58B</td> <td>7.69B</td> <td>2024/06/12</td> </tr> <tr> <td>Flux.1-Dev <d-cite key="blackforestlabsFLUX1"></d-cite></td> <td>11.90B</td> <td>4.88B</td> <td>16.87B</td> <td>2024/08/02</td> </tr> <tr> <td>CogView3-Plus <d-cite key="zhengCogView3FinerFaster2024a"></d-cite></td> <td>2.85B</td> <td>4.76B</td> <td>8.02B</td> <td>2024/10/13</td> </tr> <tr> <td>Hunyuan-DiT <d-cite key="liHunyuanDiTPowerfulMultiResolution2024"></d-cite></td> <td>1.50B</td> <td>2.02B</td> <td>3.61B</td> <td>2024/12/01</td> </tr> <tr> <td>SANA <d-cite key="xieSANAEfficientHighResolution2025"></d-cite></td> <td>0.59B</td> <td>2.61B</td> <td>3.52B</td> <td>2025/01/11</td> </tr> <tr> <td>Lumina-Image 2.0 <d-cite key="qinLuminaImage20Unified2025"></d-cite></td> <td>2.61B</td> <td>2.61B</td> <td>5.31B</td> <td>2025/01/22</td> </tr> <tr> <td>SANA 1.5 <d-cite key="xieSANA15Efficient2025a"></d-cite></td> <td>1.60B</td> <td>2.61B</td> <td>4.53B</td> <td>2025/03/21</td> </tr> <tr> <td>HiDream-I1-Dev <d-cite key="caiHiDreamI1HighEfficientImage2025"></d-cite></td> <td>17.11B</td> <td>5.58B</td> <td>22.77B</td> <td>2025/04/06</td> </tr> <tr> <td>CogView4-6B <d-cite key="zhengCogView3FinerFaster2024a"></d-cite></td> <td>3.50B</td> <td>2.00B</td> <td>6.00B</td> <td>2025/05/03</td> </tr> <tr> <td>Qwen-Image <d-cite key="wuQwenImageTechnicalReport2025"></d-cite></td> <td>20.43B</td> <td>8.29B</td> <td>28.85B</td> <td>2025/08/04</td> </tr> </tbody> </table> <h2 id="latest-advancement-in-u-net-and-dit-architecture-design">Latest Advancement in U-Net and DiT Architecture Design</h2> <p>While the transition from U-Net to DiT architectures represents a major paradigm shift, both architectural families have continued to evolve with innovative refinements. In the U-Net domain, <strong>two-stage cascaded approaches</strong> <d-cite key="hoCascadedDiffusionModels2022"></d-cite><d-cite key="sahariaImageSuperResolution2022"></d-cite> decompose generation into a low-resolution base model and specialized super-resolution upsamplers. <strong>U-ViT</strong> <d-cite key="Bao_2023_CVPR"></d-cite> bridges U-Net and transformer architectures by replacing CNN residual blocks with Vision Transformer blocks. Challenging the dominance of latent-space models, <strong>Simple Diffusion</strong> <d-cite key="hoogeboomSimpleDiffusionEndtoend2023"></d-cite> and <strong>Simpler Diffusion (SiD2)</strong> <d-cite key="hoogeboomSimplerDiffusion152025"></d-cite> demonstrate that end-to-end pixel-space diffusion can achieve state-of-the-art performance (FID 1.48 on ImageNet 512) through optimized noise schedules and simplified architectures.</p> <p>The DiT family has seen rapid advances across multiple dimensions. <strong>Architecture variants</strong> include <strong>SiT</strong> (Scalable Interpolant Transformer) <d-cite key="maSiTExploringFlow2024"></d-cite>, which replaces diffusion with interpolant-based transport, <strong>FiT</strong> (Flexible Vision Transformer) <d-cite key="luFiTFlexibleVision2024"></d-cite> which supports unrestricted resolutions and aspect ratios, and <strong>LiT</strong> (Linear Diffusion Transformer) <d-cite key="wangLiTDelvingSimple2025"></d-cite> and <strong>DiG</strong> (Diffusion GLA) <d-cite key="zhuDiGScalableEfficient2025"></d-cite> which achieve O(n) complexity through linear attention mechanisms. <strong>Training efficiency innovations</strong> such as <strong>MDT/MDTv2</strong> <d-cite key="gaoMaskedDiffusionTransformer2023"></d-cite><d-cite key="gaoMDTv2MaskedDiffusion2024"></d-cite> and <strong>MaskDiT</strong> <d-cite key="zhengFastTrainingDiffusion2024"></d-cite> leverage masked modeling to accelerate learning. Representation-based approaches <strong>REPA</strong> <d-cite key="yuRepresentationAlignmentGeneration2025"></d-cite> and <strong>REG</strong> <d-cite key="wuRepresentationEntanglementGeneration2025"></d-cite> incorporate external pretrained visual representations to dramatically accelerate training (e.g., REG achieves 63√ó faster training than SiT). <strong>U-DiTs</strong> <d-cite key="tianUDiTsDownsampleTokens2024"></d-cite> combine U-Net‚Äôs multiscale efficiency with DiT‚Äôs power. Furthermore, <strong>JiT</strong> (‚ÄúJust image Transformers‚Äù) <d-cite key="liBackBasicsLet2025a"></d-cite> <d-cite key="nguyenImageWorthMore2024a"></d-cite> revisits the basics, showing that plain transformers operating directly on pixels or patches can perform surprisingly well without complex tokenizers.</p> <h2 id="pre-trained-text-to-image-checkpoints">Pre-trained Text-to-Image Checkpoints</h2> <p>The landscape of pre-trained text-to-image models has evolved dramatically since the introduction of Stable Diffusion. These models serve as powerful foundation models that can be adapted for specialized downstream tasks without architectural modifications, simply by fine-tuning on domain-specific datasets.</p> <h2 id="interactive-architecture-explorer">Interactive Architecture Explorer</h2> <div class="l-body"> <iframe id="architecture-explorer-iframe" src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-diffusion-architecture-evolution/model-architecture-explorer.html" frameborder="0" scrolling="no" height="600px" width="100%" style="border: 1px solid #ddd; border-radius: 4px; min-height: 600px;"></iframe> </div> <script>
  // Listen for resize messages from the iframe
  window.addEventListener('message', function(e) {
    if (e.data && e.data.type === 'resize' && e.data.source === 'architecture-explorer') {
      var iframe = document.getElementById('architecture-explorer-iframe');
      if (iframe) {
        iframe.style.height = e.data.height + 'px';
      }
    }
  });
</script> <h3 id="u-net-family">U-Net Family</h3> <p><strong>Stable Diffusion</strong> <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite> represents the pioneering work in latent diffusion models, adopting a U-Net architecture that operates in a compressed latent space rather than pixel space. This design choice dramatically reduces computational costs while maintaining high-quality generation capabilities. The model combines two key components: a pre-trained variational autoencoder (VAE) for efficient image compression and decompression, and a diffusion model that performs the denoising process in this latent space.<d-footnote>In the prioring work of LDM in the paper <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite>, the VAE part is adopted a VQ-GAN style from <d-cite key="esserTamingTransformersHighResolution2021"></d-cite>. When it comes to CompVis Stable Diffusion v1.1-v.1.4 and StabilityAI Stable Diffusion v1.5 and v2.x version, the VAE part is turned to AutoEncoderKL style rather than a VQ style.</d-footnote></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sd-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sd-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sd-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sd.png" width="100%" height="auto" alt="Stable Diffusion 1.x - 2.x architecture." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Stable Diffusion 1.x - 2.x architecture. Image Credit: <d-cite key="esserTamingTransformersHighResolution2021"></d-cite>.</figcaption> </figure> <p><strong>Stable Diffusion XL (SDXL)</strong> <d-cite key="podellSDXLImprovingLatent2024"></d-cite> marked a significant scaling advancement, adopting a two-stage U-Net architecture and increasing the model size from 0.8 billion to 2.6 billion parameters. SDXL remains one of the largest U-Net-based models for image generation and demonstrates improved efficiency and compatibility across diverse domains and tasks. Despite reaching scaling limits, SDXL continues to serve as a foundation for numerous specialized applications.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl.png" width="100%" height="auto" alt="SDXL Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SDXL Architecture. Image Credit: <d-cite key="podellSDXLImprovingLatent2024"></d-cite>.</figcaption> </figure> <p><strong>Kandinsky</strong> <d-cite key="razzhigaevKandinskyImprovedTexttoImage2023"></d-cite> represents a significant advancement in the U-Net era, introducing a novel exploration of latent diffusion architecture that combines image prior models with latent diffusion techniques. The model features a modified MoVQ implementation as the image autoencoder component and achieves a FID score of 8.03 on the COCO-30K dataset, marking it as the top open-source performer in terms of measurable image generation quality. <strong>Kandinsky 3</strong> <d-cite key="arkhipkinKandinsky30Technical2024"></d-cite><d-cite key="arkhipkinKandinsky3TexttoImage2024"></d-cite> continues this series with improved text understanding and domain-specific performance, presenting a multifunctional generative framework supporting text-guided inpainting/outpainting, image fusion, and image-to-video generation.</p> <p><strong>Stable Cascade</strong> (based on W√ºrstchen architecture) <d-cite key="perniasWurstchenEfficientArchitecture2024"></d-cite> introduces an efficient architecture for large-scale text-to-image diffusion models, achieving competitive performance with unprecedented cost-effectiveness. The key innovation is a latent diffusion technique that learns extremely compact semantic image representations, reducing computational requirements significantly‚Äîtraining requires only 24,602 A100-GPU hours compared to Stable Diffusion 2.1‚Äôs 200,000 GPU hours while maintaining state-of-the-art results.</p> <p><strong>UniDiffuser</strong> <d-cite key="baoOneTransformerFits2023"></d-cite> explores transformer-based diffusion models with a unified framework that fits all distributions relevant to multi-modal data in one model. While primarily focused on transformer architectures, this work demonstrates the potential for unified multi-modal generation within the diffusion framework.</p> <h3 id="pixart-alpha-20231006">Pixart-$\alpha$ (2023/10/06)</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost.png" width="100%" height="auto" alt="Cost Comparison" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Comparisons of CO2 emissions and training cost among T2I generators. PIXART-Œ± achieves an exceptionally low training cost of $28,400. Compared to RAPHAEL <d-cite key="xueRAPHAELTexttoImageGeneration2023"></d-cite>, our CO2 emissions and training costs are merely 1.2% and 0.91%, respectively. Image Credit: <d-cite key="chenPixArtaFastTraining2024"></d-cite>.</figcaption> </figure> <p>PixArt-$\alpha$ is motivated by the rising compute and environmental costs of text-to-image systems, seeking near-commercial quality with a much smaller training budget <d-cite key="chenPixArtaFastTraining2024"></d-cite>. In contrast to SD 1.5/2.1, it adopts a large-language-model text encoder (T5) <d-cite key="raffelExploringLimitsTransfer2020"></d-cite>, making it the first open-source diffusion T2I model to use an LLM-based text encoder while keeping the overall design streamlined.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha.png" width="100%" height="auto" alt="Pixart-Œ± Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Model architecture of PIXART-Œ±. A cross-attention module is integrated into each block to inject textual conditions. To optimize efficiency, all blocks share the same adaLN-single parameters for time conditions. Image Credit: <d-cite key="chenPixArtaFastTraining2024"></d-cite>.</figcaption> </figure> <p>Architecturally, PixArt-$\alpha$ is a latent Diffusion Transformer (DiT): VAE latents are patchified into a token sequence processed by stacked Transformer blocks; each block applies cross-attention to text tokens, and timestep conditioning is injected via a shared adaLN-single, simplifying parameters and conditioning pathways <d-cite key="chenPixArtaFastTraining2024"></d-cite>.</p> <div class="key-differences"> <strong>Key differences vs SD 1.5/2.1</strong> <ul> <li>Transformer sequence-of-patches backbone (no encoder‚Äìdecoder or skip connections)</li> <li>Shared adaLN for time and unified per-block cross-attention (vs U-Net residual blocks with per-block time MLP/spatial injections)</li> <li>T5 text encoder (LLM) rather than CLIP/OpenCLIP</li> </ul> </div> <h3 id="lumina-t2i-20240401">Lumina-T2I (2024/04/01)</h3> <p>Lumina-T2I is the first entry in the Lumina series from Shanghai AI Lab, aiming for a simple, scalable framework that supports flexible resolutions while maintaining photorealism. Building on the Sora insight that scaling Diffusion Transformers enables generation across arbitrary aspect ratios and durations yet lacks concrete implementation details, Lumina-T2I adopts flow matching to stabilize and accelerate training <d-cite key="gaoLuminaT2XScalableFlowbased2025a"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x.png" width="100%" height="auto" alt="Lumina-T2I Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Lumina-T2I architecture featuring Flag-DiT backbone. Image Credit: <d-cite key="gaoLuminaT2XScalableFlowbased2025a"></d-cite>.</figcaption> </figure> <p>Architecturally, Lumina-T2I uses a Flow-based Large Diffusion Transformer (Flag-DiT) with zero-initialized attention, RoPE <d-cite key="suRoFormerEnhancedTransformer2024"></d-cite>, and KQ-Norm <d-cite key="henryQueryKeyNormalizationTransformers2020"></d-cite>. Latent features are tokenized and processed by Transformer blocks; learnable placeholders such as the [nextline] token and layerwise relative position injection enable robust resolution extrapolation without retraining for each size.</p> <div class="key-differences"> <strong>Key differences vs PixArt-Œ±</strong> <ul> <li>Robust resolution generalization across 512¬≤‚Äì1792¬≤</li> <li>Uses one-dimensional RoPE, [nextline] token, and layerwise relative position injection</li> <li>PixArt-Œ± uses absolute positional embeddings limited to the initial layer, degrading at out-of-distribution scales</li> </ul> </div> <h3 id="lumina-next-t2i-20240512">Lumina-Next-T2I (2024/05/12)</h3> <p>Lumina-Next-T2I <d-cite key="zhuoLuminaNextMakingLuminaT2X2024a"></d-cite> targets the core limitations observed in Lumina-T2X‚Äîtraining instability, slow inference, and resolution extrapolation artifacts‚Äîby delivering stronger quality and faster sampling while improving zero-shot multilingual understanding. Unlike prior T2I works that rely on CLIP or T5 encoders <d-cite key="raffelExploringLimitsTransfer2020"></d-cite>, the Lumina series adopts decoder-only LLMs as text encoders: Lumina-T2X uses LLaMA-2 7B <d-cite key="touvronLlama2Open2023"></d-cite>, whereas Lumina-Next employs the lighter Gemma-2B to reduce memory and increase throughput. In practice, Lumina-Next shows clear gains on multilingual prompts (vs. CLIP/T5 setups) and further improves text-image alignment with alternative LLMs like Qwen-1.8B and InternLM-7B.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next.png" width="100%" height="auto" alt="Lumina-Next-T2I Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Lumina-Next-T2I Next-DiT architecture. Image Credit: <d-cite key="zhuoLuminaNextMakingLuminaT2X2024a"></d-cite>.</figcaption> </figure> <p>Architecturally, Lumina-Next introduces the Next-DiT backbone with 3D RoPE and Frequency- and Time-Aware Scaled RoPE for robust resolution extrapolation <d-cite key="suRoFormerEnhancedTransformer2024"></d-cite>. It adds sandwich normalizations to stabilize training (cf. normalization strategies such as KQ-Norm <d-cite key="henryQueryKeyNormalizationTransformers2020"></d-cite>), a sigmoid time discretization schedule to reduce Flow-ODE sampling steps, and a Context Drop mechanism that merges redundant visual tokens to accelerate inference‚Äîall while retaining the flow-based DiT formulation of the Lumina family.</p> <div class="key-differences"> <strong>Key differences vs Lumina-T2I</strong> <ul> <li>Next-DiT with 3D RoPE + frequency/time-aware scaling for stronger resolution extrapolation</li> <li>Sandwich normalizations improve stability; sigmoid time schedule reduces sampling steps</li> <li>Context Drop merges redundant tokens for faster inference throughput</li> <li>Decoder-only LLM text encoders (Gemma-2B by default; Qwen-1.8B/InternLM-7B optional) boost zero-shot multilingual alignment vs CLIP/T5</li> </ul> </div> <h3 id="stable-diffusion-3-20240612">Stable Diffusion 3 (2024/06/12)</h3> <p>Stable Diffusion 3 aims to improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales, demonstrating superior performance compared to established diffusion formulations for high-resolution text-to-image synthesis <d-cite key="esserScalingRectifiedFlow2024"></d-cite>. This work presents the first comprehensive scaling study for text-to-image DiTs, establishing predictable scaling trends and correlating lower validation loss to improved synthesis quality across various metrics and human evaluations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sd3-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sd3-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sd3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sd3.png" width="100%" height="auto" alt="SD3 Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Stable Diffusion 3 Architecture. Image Credit: <d-cite key="esserScalingRectifiedFlow2024"></d-cite>.</figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35.png" width="100%" height="auto" alt="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Simplied Architecture Illustration of Stable Diffusion 3.5 MM-DiT block. Image Source: Stability AI Blog.</figcaption> </figure> <p>Architecturally, SD3 transitions from DiT‚Äôs cross-attention blocks to MMDiT (Multimodal Diffusion Transformer) with double-stream blocks that use separate weights for the two modalities, enabling bidirectional flow of information between image and text tokens for improved text comprehension and typography. Unlike SDXL which relies primarily on CLIP encoders, SD3 incorporates both CLIP (L/14 and OpenCLIP bigG/14) and T5-XXL encoders <d-cite key="raffelExploringLimitsTransfer2020"></d-cite>, concatenating pooled outputs and hidden representations to create comprehensive text conditioning with enhanced understanding capabilities.</p> <div class="key-differences"> <strong>Key differences vs SDXL and PixArt-Œ±</strong> <ul> <li>MMDiT double-stream architecture with separate weights per modality and bidirectional information flow (vs single-stream cross-attention)</li> <li>Integrated rectified flow training with perceptually-biased noise sampling (vs standard diffusion formulation)</li> <li>Combined CLIP + T5-XXL text encoding for enhanced text comprehension and typography</li> <li>First comprehensive scaling study demonstrating predictable trends for text-to-image DiTs</li> </ul> </div> <h3 id="flux1-dev-20240802">Flux.1-Dev (2024/08/02)</h3> <p>Flux.1-Dev, developed by former Stability AI core members, aims to scale beyond previous models and achieve superior image quality with more accurate text-to-image synthesis <d-cite key="blackforestlabsFLUX1"></d-cite>. Representing a significant scaling effort, the model features a massive 12 billion parameter generator combined with a 4.7 billion parameter text encoder, marking substantial growth compared to predecessors and establishing new benchmarks in AI-driven image generation capabilities.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit.png" width="100%" height="auto" alt="Flux.1-Dev Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Flux.1-Dev MMDiT architecture. Image Credit: <d-cite key="labsFLUX1KontextFlow2025"></d-cite>.</figcaption> </figure> <p>Architecturally, Flux.1-Dev advances beyond SD3‚Äôs MMDiT by implementing a hybrid architecture that combines both single-stream and double-stream Multi-Modal Diffusion Transformers, enhancing the model‚Äôs ability to process complex visual-textual relationships. Like SD3, it incorporates T5 text encoding <d-cite key="raffelExploringLimitsTransfer2020"></d-cite> and integrates rectified flow techniques for more stable and efficient training, while conducting a comprehensive scaling study that optimizes performance across the substantially larger parameter space.</p> <div class="key-differences"> <strong>Key differences vs SD3</strong> <ul> <li>Hybrid single-stream + double-stream MMDiT architecture (vs purely double-stream MMDiT)</li> <li>Massive scaling to 12B generator + 4.7B text encoder parameters (vs smaller SD3 variants)</li> <li>Enhanced rectified flow implementation optimized for larger scale training</li> <li>Comprehensive scaling study specifically designed for multi-billion parameter DiTs</li> </ul> </div> <h3 id="cogview3--cogview3-plus-20241013">CogView3 &amp; CogView3-Plus (2024/10/13)</h3> <p><strong>CogView3</strong> <d-cite key="zhengCogView3FinerFaster2024a"></d-cite> introduces a <strong>relay diffusion approach</strong> <d-cite key="tengRelayDiffusionUnifying2024"></d-cite> that generates low-resolution images first, then refines them through super-resolution to achieve 2048√ó2048 outputs. This multi-stage process reduces computational costs while improving quality‚ÄîCogView3 outperformed SDXL by 77% in human evaluations while using only one-tenth the inference time. The model employs a text-expansion language model to rewrite user prompts, with a base stage generating 512√ó512 images followed by relaying super-resolution in the latent space.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3.png" width="100%" height="auto" alt="CogView3 Architecture." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">(left) The pipeline of CogView3. User prompts are rewritten by a text-expansion language model. The base stage model generates 512 √ó 512 images, and the second stage subsequently performs relaying super-resolution. (right) Formulation of relaying super-resolution in the latent space. Image Credit: <d-cite key="zhengCogView3FinerFaster2024a"></d-cite>.</figcaption> </figure> <p><strong>CogView3-Plus</strong> upgrades to DiT architecture with Zero-SNR scheduling and joint text-image attention for further efficiency gains. This architectural evolution represents a significant step in the CogView series, transitioning from traditional approaches to transformer-based diffusion models while maintaining the efficiency advantages of the relay diffusion framework.</p> <h3 id="hunyuan-dit-20241201">Hunyuan-DiT (2024/12/01)</h3> <p>Hunyuan-DiT, developed by Tencent‚Äôs Hunyuan team, aims to create a powerful multi-resolution diffusion transformer capable of fine-grained understanding of both English and Chinese languages, addressing the need for state-of-the-art Chinese-to-image generation with culturally relevant and multilingual capabilities <d-cite key="liHunyuanDiTPowerfulMultiResolution2024"></d-cite>. The model establishes a comprehensive data pipeline with iterative optimization, employing a Multimodal Large Language Model to refine image captions and enhance alignment between textual descriptions and generated images, particularly for intricate Chinese characters and cultural nuances.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit.png" width="100%" height="auto" alt="Hunyuan-DiT Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Hunyuan-DiT multi-resolution architecture. Image Credit: <d-cite key="liHunyuanDiTPowerfulMultiResolution2024"></d-cite>.</figcaption> </figure> <p>Architecturally, Hunyuan-DiT builds upon PixArt-$\alpha$ by incorporating both single-stream and double-stream Multi-Modal Diffusion Transformer (MM-DiT) blocks similar to SD3, enabling efficient handling of complex image generation tasks across multiple resolutions. The model integrates dual text encoders‚ÄîCLIP for understanding overall semantic content and T5 <d-cite key="raffelExploringLimitsTransfer2020"></d-cite> for nuanced language comprehension including complex sentence structures‚Äîcombined with enhanced positional encoding to maintain spatial information across different resolutions, facilitating robust multi-resolution generation capabilities.</p> <div class="key-differences"> <strong>Key differences vs PixArt-Œ±</strong> <ul> <li>Single-stream + double-stream MM-DiT blocks for enhanced multi-modal processing (vs single-stream cross-attention)</li> <li>Dual text encoders (CLIP + T5) for semantic and nuanced language understanding (vs T5 only)</li> <li>Multi-resolution diffusion transformer with enhanced positional encoding for robust resolution handling</li> <li>Multimodal LLM-refined captions with fine-grained bilingual (English + Chinese) understanding</li> </ul> </div> <h3 id="sana-20250111">SANA (2025/01/11)</h3> <p>SANA, developed by NVIDIA, aims to enable efficient high-resolution image synthesis up to 4096√ó4096 pixels while maintaining deployment feasibility on consumer hardware, generating 1024√ó1024 images in under a second on a 16GB laptop GPU <d-cite key="xieSANAEfficientHighResolution2025"></d-cite>. The model introduces innovations to reduce computational requirements dramatically: DC-AE (deep compression autoencoder) achieves 32√ó image compression reducing latent tokens significantly, efficient caption labeling and selection accelerate convergence, and Flow-DPM-Solver reduces sampling steps for faster generation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sana-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sana-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sana-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sana.png" width="100%" height="auto" alt="SANA Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SANA Linear DiT architecture for efficient high-resolution generation. Image Credit: <d-cite key="xieSANAEfficientHighResolution2025"></d-cite>.</figcaption> </figure> <p>Architecturally, SANA advances beyond PixArt-$\Sigma$ by replacing traditional self-attention mechanisms with Linear Diffusion Transformer (Linear DiT) blocks, enhancing computational efficiency at high resolutions without compromising quality. The model adopts a decoder-only small language model as the text encoder, employing complex human instructions with in-context learning to improve text-image alignment compared to conventional CLIP or T5 encoders. The compact 0.6B parameter model achieves competitive performance with substantially larger models like Flux-12B while being 20 times smaller and over 100 times faster in throughput.</p> <div class="key-differences"> <strong>Key differences vs PixArt-Œ£</strong> <ul> <li>Linear DiT replacing traditional self-attention for O(n) complexity vs O(n¬≤) at high resolutions</li> <li>DC-AE with 32√ó compression reducing latent tokens and memory requirements dramatically</li> <li>Decoder-only language model as text encoder with in-context learning (vs T5)</li> <li>0.6B parameters achieving competitive quality with 12B models while 100√ó faster throughput</li> </ul> </div> <h3 id="lumina-image-20-20250122">Lumina-Image 2.0 (2025/01/22)</h3> <p>Lumina-Image 2.0 aims to provide a unified and efficient image generative framework that excels in generating high-quality images with strong text-image alignment across diverse generation and editing tasks <d-cite key="qinLuminaImage20Unified2025"></d-cite>. Building upon the Lumina series‚Äô foundation, the model consolidates multiple generation tasks into a cohesive framework, optimizing performance and efficiency to cater to a wide range of image generation applications while achieving competitive scores across multiple benchmarks including FID and CLIP metrics.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2.png" width="100%" height="auto" alt="Lumina-Image 2.0 Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Lumina-Image 2.0 Unified Next-DiT architecture. Image Credit: <d-cite key="qinLuminaImage20Unified2025"></d-cite>.</figcaption> </figure> <p>Architecturally, Lumina-Image 2.0 advances beyond Lumina-Next-T2I by introducing a unified Next-DiT architecture that seamlessly integrates text-to-image generation and image editing capabilities within a shared framework. The model maintains the Lumina series‚Äô architectural strengths including 3D RoPE <d-cite key="suRoFormerEnhancedTransformer2024"></d-cite>, frequency-aware scaling, and flow-based formulation, while enhancing the framework to support both generation and editing operations efficiently. This unified approach enables the model to leverage shared representations and training strategies across different image generation modalities.</p> <div class="key-differences"> <strong>Key differences vs Lumina-Next-T2I</strong> <ul> <li>Unified Next-DiT framework seamlessly integrating generation and editing (vs generation-only focus)</li> <li>Enhanced multi-task architecture supporting diverse image generation applications within single model</li> <li>Optimized training paradigm leveraging shared representations across generation modalities</li> <li>Competitive performance across FID and CLIP benchmarks with improved efficiency</li> </ul> </div> <h3 id="sana-15-20250321">SANA 1.5 (2025/03/21)</h3> <p>SANA 1.5 aims to push the boundaries of efficient high-resolution image synthesis established by SANA, offering improved performance and scalability through larger model sizes and advanced inference scaling techniques <d-cite key="xieSANA15Efficient2025a"></d-cite>. The model introduces inference scaling via VISA (a specialized NVILA-2B model) that scores and selects top images from large candidate sets, significantly boosting GenEval performance scores‚Äîfor instance, improving SANA-1.5-4.8B from 81 to 96. This approach demonstrates that post-generation selection can dramatically enhance quality metrics without architectural changes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5.png" width="100%" height="auto" alt="SANA 1.5 Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SANA 1.5 improved Linear DiT architecture. Image Credit: <d-cite key="xieSANA15Efficient2025a"></d-cite>.</figcaption> </figure> <p>Architecturally, SANA 1.5 builds upon the original SANA by incorporating an enhanced DC-AE (deep compression autoencoder) to handle higher resolutions and more complex generation tasks, along with advanced Linear DiT blocks featuring more sophisticated linear attention mechanisms to boost efficiency and quality in high-resolution synthesis. The model scales to 4.8B parameters compared to SANA‚Äôs 0.6B, providing a robust solution for generating high-quality images with strong text-image alignment suitable for diverse professional applications requiring both quality and computational efficiency.</p> <div class="key-differences"> <strong>Key differences vs SANA</strong> <ul> <li>Inference scaling with VISA model for candidate selection dramatically improving GenEval scores (81‚Üí96)</li> <li>Enhanced DC-AE handling higher resolutions and more complex generation tasks</li> <li>Advanced Linear DiT with more sophisticated linear attention mechanisms</li> <li>Scaled to 4.8B parameters providing improved quality while maintaining efficiency advantages</li> </ul> </div> <h3 id="hidream-i1-dev-20250406">HiDream-I1-Dev (2025/04/06)</h3> <p>HiDream-I1, developed by HiDream.ai, addresses the critical trade-off between quality improvements and computational complexity in image generative foundation models, aiming to achieve state-of-the-art image generation quality within seconds while maintaining high efficiency <d-cite key="caiHiDreamI1HighEfficientImage2025"></d-cite>. With 17 billion parameters, the model introduces a sparse Diffusion Transformer structure that enables efficient inference suitable for professional-grade design needs, supporting 4K ultra-high-definition image generation with advanced text comprehension, multi-style adaptation, and precise detail control while optimizing computational requirements through sparsity.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/hidream-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/hidream-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/hidream-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/hidream.png" width="100%" height="auto" alt="HiDream-I1-Dev Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">HiDream-I1-Dev Sparse DiT architecture. Image Credit: <d-cite key="caiHiDreamI1HighEfficientImage2025"></d-cite>.</figcaption> </figure> <p>Architecturally, HiDream-I1 advances beyond Flux.1-Dev and Qwen-Image by implementing a novel sparse DiT structure where only subsets of transformer blocks are activated for each forward pass, dramatically reducing computational costs while maintaining generation quality. The sparse architecture enables the massive 17B parameter model to achieve practical inference speeds comparable to smaller dense models, with efficient diffusion mechanisms supporting multimodal input and providing fine-grained control over generation. This sparse approach represents a paradigm shift in scaling DiT models, demonstrating that architectural efficiency through sparsity can rival quality of substantially denser models.</p> <div class="key-differences"> <strong>Key differences vs Flux.1-Dev and other large DiTs</strong> <ul> <li>Sparse DiT structure activating only subsets of blocks per forward pass for efficient 17B parameter model</li> <li>4K ultra-high-definition generation support with optimized inference speed despite massive scale</li> <li>Advanced sparse attention mechanisms maintaining quality while dramatically reducing computational costs</li> <li>Multimodal input support and fine-grained control optimized for professional-grade design applications</li> </ul> </div> <h3 id="cogview4-6b-20250503">CogView4-6B (2025/05/03)</h3> <p><strong>CogView4-6B</strong> <d-cite key="zhengCogView3FinerFaster2024a"></d-cite> represents the latest advancement in the CogView series, featuring a sophisticated <strong>CogView4Transformer2DModel</strong> architecture that excels in Chinese text rendering and multilingual image generation. The model demonstrates exceptional performance in text accuracy evaluation, achieving precision of 0.6969, recall of 0.5532, and F1 score of 0.6168 on Chinese text benchmarks.</p> <p>CogView4-6B leverages GLM-based text encoding and advanced transformer blocks with RoPE (Rotary Position Embedding) for enhanced spatial understanding and text-image alignment. This architectural sophistication enables the model to achieve superior text rendering capabilities, particularly for complex Chinese characters and multilingual content, setting new standards for text-to-image generation in non-Latin scripts. Available on <a href="https://huggingface.co/zai-org/CogView4-6B">Hugging Face</a> under Apache 2.0 license.</p> <h3 id="qwen-image-20250804">Qwen-Image (2025/08/04)</h3> <p>Qwen-Image represents a monumental scaling achievement in text-to-image synthesis, establishing a new state-of-the-art with its massive 28.85 billion parameter architecture <d-cite key="wuQwenImageTechnicalReport2025"></d-cite>. Developed by Alibaba‚Äôs Qwen team, this flagship model aims to push the boundaries of generation quality, text-image alignment, and multimodal understanding through unprecedented scale. The model excels at generating highly detailed, photorealistic images that accurately reflect complex textual prompts, setting new benchmarks for fidelity and coherence in the field.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image.png" width="100%" height="auto" alt="Qwen-Image Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Qwen-Image massively scaled MMDiT architecture. Image Credit: <d-cite key="wuQwenImageTechnicalReport2025"></d-cite>.</figcaption> </figure> <p>Architecturally, Qwen-Image employs a massively scaled Multi-Modal Diffusion Transformer (MMDiT) that builds upon the hybrid single- and double-stream designs seen in models like Flux.1-Dev. The generator model alone comprises over 20 billion parameters, combined with a powerful 8.29 billion parameter text encoder for unparalleled language comprehension. This dual-stream approach allows for sophisticated interaction between text and image modalities, enabling precise control over generated content. The model integrates advanced training techniques, including rectified flow and large-scale data curation, to ensure stable and efficient convergence despite its enormous size.</p> <div class="key-differences"> <strong>Key differences vs HiDream-I1-Dev</strong> <ul> <li>Massive dense scaling to 28.85B parameters (vs HiDream's 17B sparse architecture)</li> <li>Focus on state-of-the-art quality through sheer scale (vs HiDream's focus on efficiency via sparsity)</li> <li>Extremely large 8.29B text encoder for superior text-image alignment</li> <li>Represents the pinnacle of the dense DiT scaling paradigm before potential shifts to new architectures</li> </ul> </div> <h2 id="experiments-and-case-studies">Experiments and Case Studies</h2> <p>To comprehensively evaluate the capabilities of different text-to-image diffusion models, we propose a systematic evaluation framework spanning tasks of varying complexity. This section will present case studies of text-to-image generation visualizations using existing checkpoints, assessing their performance across a spectrum of increasingly challenging tasks.</p> <p><strong>Implementation Details<d-footnote>For commercial model, we use ChatGPT webui GPT-5-Instant with the same prompt for each case study for image generation with a default image size as 1024 √ó 1024</d-footnote>:</strong></p> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Precision</td> <td>bfloat16</td> </tr> <tr> <td>Scheduler</td> <td>default</td> </tr> <tr> <td>Steps</td> <td>50</td> </tr> <tr> <td>Guidance Scale</td> <td>7.5</td> </tr> <tr> <td>Resolution</td> <td>512√ó512</td> </tr> </tbody> </table> <div class="l-page"> <iframe src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-diffusion-architecture-evolution/case-studies.html" width="100%" height="3000" frameborder="0" style="border: none;"></iframe> </div> <script>
  window.addEventListener('message', function(e) {
    if (e.data.type === 'resize' && e.data.source === 'case-studies') {
      const iframe = document.querySelector('iframe[src*="case-studies.html"]');
      if (iframe) {
        iframe.style.height = e.data.height + 'px';
      }
    }
  });
</script> <div class="key-differences"> <strong>Summary of Results</strong> <ul> <li>There is no strong correlation between image model size and image aesthetics (See case study 4).</li> <li>There is no strong correlation between text model size and prompt following (See case study 5).</li> <li>Large models generally work better but always the case.</li> <li>U-Nets based model perform comparativaly worse than DiTs in the similar model size, for instance, SDXL to SANA, Kandinsky-3 to CogView4.</li> <li>StaleDiffusion 3.x continously trained on higher resolution (e.g., 1024px) tends to generate croped results.</li> <li>Not all models are capable to dealing with multilingual prompt (see case study 2).</li> <li>Commercial model such as GPT-Image model works extremely well in aesthetics, prompt following, counting, text rendering and spatial reasoning.</li> </ul> </div> <h2 id="why-scaling-favors-attention">Why Scaling Favors Attention</h2> <p>As diffusion models scaled in data and compute, the active bottleneck shifted from <strong>local fidelity</strong> to <strong>global semantic alignment</strong>, and the community moved accordingly: from U-Nets that hard-wire translation equivariance via convolution to Diffusion Transformers that <strong>learn</strong> equivariances through self-attention. Let \(\mathcal{C}^{\mathrm{conv}}_{G}\) be the class of <strong>translation-equivariant, finite-support Toeplitz operators</strong> (U-Net convolutional kernels) and \(\mathcal{A}^{\mathrm{attn}}\) the class of <strong>self-attention kernels with relative positional structure</strong> (DiTs). Write \(\sqsubseteq^{\mathrm{bias}}\) as ‚Äúis a constrained instance of (via inductive-bias constraints)‚Äù<d-cite key="hornTranslationalEquivarianceKernelizable2021"></d-cite><d-cite key="taiMathematicalExplanationUNet2024"></d-cite>.</p> \[\boxed{ \mathcal{C}^{\mathrm{conv}}_{G}\ \sqsubseteq^{\mathrm{bias}}\ \mathcal{A}^{\mathrm{attn}} }\] <p>In plain terms, <strong>convolution is a simplified, efficient expression of attention</strong> obtained by enforcing fixed translation symmetry, parameter tying, and locality<d-cite key="ramachandranStandAloneSelfAttentionVision2019"></d-cite><d-cite key="cordonnierRelationshipSelfAttentionConvolutional2020"></d-cite><d-cite key="changConvolutionsSelfAttentionReinterpreting2021"></d-cite><d-cite key="choiGraphConvolutionsEnrich2024"></d-cite><d-cite key="joshiTransformersAreGraph2025"></d-cite>; removing these constraints yields attention <strong>without a hard-coded translation prior</strong>, allowing DiTs to <em>learn</em> which symmetries and long-range relations matter at scale. This inclusion explains the empirical shift under modern hardware and datasets: attention strictly generalizes convolution while retaining it as an efficient special case, delivering smoother scaling laws and higher semantic ‚Äúbandwidth‚Äù per denoising step. In practice, this is also a story of hardware path dependence: attention‚Äôs dense-matrix primitives align with contemporary accelerators and compiler stacks, effectively ‚Äúwinning‚Äù the hardware lottery <d-cite key="hookerHardwareLottery2021"></d-cite>. And, echoing the Bitter Lesson<d-cite key="richsuttonBitterLesson2019"></d-cite>, as data and compute grow, general methods with fewer hand-engineered priors dominate‚Äîmaking attention‚Äôs strict generalization of convolution the natural backbone at scale.</p> <h2 id="further-discussion">Further Discussion</h2> <h3 id="from-text-to-image-generation-to-real-world-applications">From Text-to-Image Generation to Real-World Applications</h3> <p>Text-to-image is now genuinely strong; the next wave is about <strong>conditioning existing pixels</strong> rather than generating from scratch‚Äîturning models into reliable editors that honor what must stay and change only what‚Äôs asked. This means prioritizing downstream tasks like image editing, inpainting/outpainting, image-to-image restyling, and structure- or reference-guided synthesis (edges, depth, layout, style, identity). The practical focus shifts from unconstrained novelty to controllable, faithful rewrites with tight mask adherence, robust subject/style preservation, and interactive latencies, so these systems plug cleanly into real creative, design, and industrial workflows.</p> <h3 id="diffusion-models-vs-auto-regressive-models">Diffusion Models vs. Auto-regressive Models</h3> <p>Diffusion models and autoregressive (AR) models represent two fundamentally different approaches to image generation, with the key distinction being that <strong>autoregressive models operate on discrete image tokens</strong> while <strong>diffusion models work with continuous representations</strong>. Autoregressive models like DALL-E <d-cite key="rameshZeroShotTexttoImageGeneration2021"></d-cite>, CogView <d-cite key="dingCogViewMasteringTexttoImage2021"></d-cite>, and CogView2 <d-cite key="dingCogView2FasterBetter2022"></d-cite> treat image generation as a sequence modeling problem, encoding images into discrete tokens using VQ-VAE <d-cite key="esserTamingTransformersHighResolution2021"></d-cite> or similar vector quantization methods, then autoregressively predicting the next token given previous tokens. This approach offers sequential generation with precise control and natural language integration, but suffers from slow generation, error accumulation, and discrete representation loss. In contrast, diffusion models operate directly on continuous pixel or latent representations, learning to reverse a gradual noise corruption process, which enables parallel generation, high-quality outputs, and flexible conditioning, though at the cost of computational overhead and less direct control. Recent advances have significantly improved autoregressive approaches: VAR <d-cite key="tianVisualAutoregressiveModeling2024"></d-cite> redefines autoregressive learning as coarse-to-fine ‚Äúnext-scale prediction‚Äù and achieves superior performance compared to diffusion transformers, while Infinity <d-cite key="hanInfinityScalingBitwise2025"></d-cite> demonstrates effective scaling of bitwise autoregressive modeling for high-resolution synthesis. Additionally, MAR <d-cite key="liAutoregressiveImageGeneration2024a"></d-cite> bridges the gap between paradigms by adopting diffusion loss for autoregressive models, enabling continuous-valued autoregressive generation without vector quantization. Recent work has also explored hybrid approaches that combine both paradigms: HunyuanImage 3.0 <d-cite key="caoHunyuanImage30Technical2025"></d-cite> and BLIP3-o <d-cite key="chenBLIP3oFamilyFully2025"></d-cite> demonstrate unified multimodal models within autoregressive frameworks while incorporating diffusion-inspired techniques, while OmniGen <d-cite key="xiaoOmniGenUnifiedImage2024"></d-cite> and OmniGen2 <d-cite key="wuOmniGen2ExplorationAdvanced2025"></d-cite> use diffusion models as backbones for unified generation capabilities.</p> <hr/> <p><strong>Note:</strong> This blog post‚Äôs knowledge cutoff is November 17, 2025. For the latest developments, we refer readers to recent works including DiP <d-cite key="chenDiPTamingDiffusion2025"></d-cite>, DeCo <d-cite key="maDeCoFrequencyDecoupledPixel2025"></d-cite>, PixelDiT <d-cite key="yuPixelDiTPixelDiffusion2025"></d-cite>, Z-Image <d-cite key="z-image-2025"></d-cite>, and FLUX.2 <d-cite key="flux-2-2025"></d-cite>.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[A comprehensive analysis of how diffusion model architectures evolved from U-Net backbones to Diffusion Transformers, transforming text-to-image generation capabilities.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/distill-example</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/iclr-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/iclr-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/iclr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-27-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/9-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/9-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/8-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/8-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/8.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/10-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/10-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/10.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/11-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/11-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/11.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/12-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/12-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/12.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here‚Äôs how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that‚Äôs required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository‚Äôs root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
    <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-distill-example/plotly_demo_1.html' | relative_url }}"</span>
    <span class="na">frameborder=</span><span class="s">"0"</span>
    <span class="na">scrolling=</span><span class="s">"no"</span>
    <span class="na">height=</span><span class="s">"600px"</span>
    <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well‚Äâ‚Äî‚Äâthe authors are human and it‚Äôs nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid.js</a> directly. Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a> syntax.</p> <p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post‚Äôs front matter:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div> <pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. ‚ÄîAnais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you‚Äôll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item</li> </ol> <ul> <li>Unordered sub-list.</li> </ul> <ol> <li>Actual numbers don‚Äôt matter, just that it‚Äôs a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we‚Äôll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li> <p>Unordered lists can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I‚Äôm an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I‚Äôm an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I‚Äôm a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I‚Äôm a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here‚Äôs our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don‚Äôt need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let‚Äôs keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here‚Äôs a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but‚Ä¶ This line is only separated by a single newline, so it‚Äôs a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Earth4D: Production-Ready Space-Time Positional Encoding for World Models</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/earth4d-world-models/" rel="alternate" type="text/html" title="Earth4D: Production-Ready Space-Time Positional Encoding for World Models"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/earth4d-world-models</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/earth4d-world-models/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>World models‚Äîsystems that learn to simulate and predict complex spatiotemporal dynamics‚Äîhave emerged as a promising paradigm for understanding our planet. From climate forecasting to disaster response, these models need to process Earth observation data spanning vast spatial and temporal scales: from sub-meter satellite imagery to continental weather patterns, from sub-second events to century-long climate trends.</p> <p>At the heart of these models lies a fundamental challenge: <strong>how do we encode continuous space-time coordinates into learnable representations?</strong></p> <p>While transformers have revolutionized deep learning through positional encodings, existing approaches face severe limitations when applied to planetary-scale spatiotemporal data. Sinusoidal encodings lack the expressiveness for complex Earth phenomena. Learned embeddings require discretizing continuous space-time, losing resolution. And while 3D hash encoding<d-cite key="muller2022instant"></d-cite> revolutionized neural graphics, it doesn‚Äôt capture temporal dynamics.</p> <p>We present <strong>Earth4D</strong>, a production-ready 4D space-time positional encoder that extends multi-resolution hash encoding to four dimensions. Earth4D achieves remarkable results:</p> <ul> <li><strong>Matches state-of-the-art foundation models</strong> using only (x,y,z,t) coordinates‚Äîno satellite imagery, weather data, or topography required</li> <li><strong>99% parameter reduction</strong> (724M ‚Üí 5M) with 4√ó training speedup while maintaining strong performance</li> <li><strong>Planetary coverage</strong> from sub-meter to continental scale, with temporal precision from sub-second to centuries</li> </ul> <p>More importantly, Earth4D represents a shift in how we think about world models: rather than requiring massive multimodal pretraining, we can achieve competitive performance by learning rich spatiotemporal representations from coordinates alone.</p> <hr/> <h2 id="the-positional-encoding-challenge">The Positional Encoding Challenge</h2> <h3 id="why-earth-observation-needs-4d-encoding">Why Earth Observation Needs 4D Encoding</h3> <p>Earth observation data presents unique challenges that distinguish it from typical deep learning tasks:</p> <p><strong>Continuous spatiotemporal coordinates</strong>: Unlike images with discrete pixel positions or text with sequential tokens, Earth data exists in continuous 4D space-time. A wildfire measurement at (37.77¬∞N, -122.42¬∞W, 50m elevation, March 23 2023 8:58 AM) needs precise encoding.</p> <p><strong>Extreme scale variation</strong>: We need to reason about phenomena at vastly different scales simultaneously‚Äîa climate model might track both individual storms (10-100km) and global circulation patterns (10,000km+). Temporal scales range from seconds (lightning strikes) to decades (climate change).</p> <p><strong>Planetary coverage</strong>: Unlike 3D graphics which operate in local coordinate frames, Earth observation requires global consistency. The same encoding must work for data from San Francisco, Sydney, and the South Pole.</p> <p><strong>Memory constraints</strong>: Processing planet-scale data quickly exhausts GPU memory. A naive grid representation at 1-meter resolution would require \(10^{18}\) grid cells globally‚Äîcompletely infeasible.</p> <h3 id="limitations-of-existing-approaches">Limitations of Existing Approaches</h3> <p><strong>Sinusoidal positional encodings</strong><d-cite key="vaswani2017attention"></d-cite>, while elegant for transformers, provide fixed basis functions that cannot adapt to complex spatiotemporal patterns in Earth data. They work well for sequential data but struggle with the intricate multi-scale structure of geospatial phenomena.</p> <p><strong>Learned embeddings</strong> require discretizing continuous coordinates. While Vision Transformers<d-cite key="dosovitskiy2021image"></d-cite> discretize images into patches, Earth observation data doesn‚Äôt have natural ‚Äúpatch‚Äù boundaries. Discretization either loses fine-grained resolution or creates memory-prohibitive lookup tables.</p> <p><strong>3D multi-resolution hash encoding</strong><d-cite key="muller2022instant"></d-cite> (InstantNGP) solved many of these problems for neural graphics by using hash tables to achieve memory-efficient multi-resolution encoding. However, it was designed for static 3D scenes. Earth observation fundamentally requires modeling <strong>how</strong> spatial patterns evolve <strong>over time</strong>‚Äîa 4D problem.</p> <p>Extending to 4D isn‚Äôt trivial. A naive 4D grid would explode memory requirements. Simply treating time as another spatial dimension loses the distinct characteristics of temporal dynamics (irreversibility, causality, different resolution requirements).</p> <p>What we need is a 4D encoding that:</p> <ol> <li>Handles continuous coordinates without discretization</li> <li>Scales efficiently to planetary coverage</li> <li>Captures multi-resolution structure in both space and time</li> <li>Adapts to data through learning</li> <li>Fits in GPU memory</li> </ol> <p>Earth4D addresses all of these requirements.</p> <hr/> <h2 id="earth4d-architecture">Earth4D Architecture</h2> <h3 id="decomposed-spatiotemporal-representation">Decomposed Spatiotemporal Representation</h3> <p>Earth4D‚Äôs core innovation is a <strong>decomposed 4D encoding</strong> that separates spatial and temporal structure while capturing their interactions. Rather than a single 4D hash grid (which would be memory-prohibitive), Earth4D uses four 3D grids:</p> <ol> <li><strong>XYZ Grid</strong>: Pure spatial encoding in Earth-Centered Earth-Fixed (ECEF) coordinates</li> <li><strong>XYT Grid</strong>: Equatorial plane + time</li> <li><strong>YZT Grid</strong>: 90¬∞E meridian plane + time</li> <li><strong>XZT Grid</strong>: Prime meridian plane + time</li> </ol> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/earth4d_architecture.png" alt="Earth4D Architecture" style="width: 100%;"/> <figcaption>Earth4D decomposes 4D space-time into four 3D hash-encoded grids. Each grid operates at multiple resolution levels (coarse to fine), with features concatenated to form a 192D spatiotemporal embedding.</figcaption> </figure> <p>This decomposition is inspired by Grid4D<d-cite key="xu2024grid4d"></d-cite> but optimized for planetary scale. By using three orthogonal spatiotemporal projections, we capture how spatial patterns evolve over time from different perspectives. The XYZ grid provides pure spatial context, while XYT, YZT, and XZT encode temporal dynamics in complementary subspaces.</p> <p><strong>Why ECEF coordinates?</strong> We use Earth-Centered Earth-Fixed (ECEF) coordinates internally rather than latitude/longitude because:</p> <ul> <li>ECEF provides uniform spatial hashing globally (no polar singularities)</li> <li>Distances are Euclidean, making interpolation consistent</li> <li>3D Cartesian coordinates align naturally with hash encoding</li> </ul> <p>Input coordinates (latitude, longitude, elevation, time) are automatically converted to ECEF and normalized to \([-1, 1]\).</p> <h3 id="multi-resolution-hierarchy">Multi-Resolution Hierarchy</h3> <p>Each of the four grids operates at multiple resolution levels simultaneously. This multi-resolution structure is crucial for capturing both local details and global patterns.</p> <p>At level \(L\), the grid resolution is:</p> \[r_L = b \cdot g^L\] <p>where \(b\) is the base resolution (typically 32) and \(g\) is the growth factor (typically \(\sqrt{2}\)). With 24 levels (default configuration), spatial resolution ranges from:</p> <ul> <li><strong>Level 1</strong>: 398.2 km/cell (continental scale)</li> <li><strong>Level 12</strong>: 194.4 m/cell (city scale)</li> <li><strong>Level 24</strong>: 4.75 cm/cell (sub-meter precision)</li> </ul> <p>Temporal resolution similarly spans from years to sub-second precision.</p> <h3 id="hash-encoding-mechanics">Hash Encoding Mechanics</h3> <p>At each resolution level \(L\), we map continuous coordinates to discrete grid positions, then hash them to a fixed-size lookup table:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Grid position: pos_grid = floor(coordinate √ó resolution_L)
2. Hash index:
   if grid_size ‚â§ hashmap_size:
       index = pos_grid.x + pos_grid.y √ó stride_y + pos_grid.z √ó stride_z
   else:
       index = hash(pos_grid) mod hashmap_size
3. Interpolate: trilinear interpolation of 8 corner features
</code></pre></div></div> <p>The hash function uses XOR with large primes for mixing:</p> \[\text{hash}(\mathbf{p}) = \bigoplus_{d=1}^{D} p_d \cdot \pi_d \pmod{T}\] <p>where \(\mathbf{p}\) is the grid position, \(\pi_d\) are large primes (2654435761, 805459861, ‚Ä¶), and \(T\) is the hash table size.</p> <p>Coarse levels (small grid size) use direct indexing with no collisions. Fine levels (large grid size) use hashing, which introduces collisions but saves massive memory.</p> <p><strong>Smoothstep interpolation</strong> \((S(t) = 3t^2 - 2t^3)\) provides C¬π continuous gradients, better than linear interpolation for smooth Earth phenomena like temperature fields or elevation gradients.</p> <p>The final output concatenates features from all grids and levels:</p> <ul> <li>4 grids √ó 24 levels √ó 2 features = <strong>192D embedding</strong> per (x,y,z,t) coordinate</li> </ul> <p>This entire process runs on GPU via custom CUDA kernels, enabling <strong>massively parallel encoding</strong> of millions of coordinates simultaneously.</p> <hr/> <h2 id="the-hash-collision-problem">The Hash Collision Problem</h2> <h3 id="understanding-hash-collisions">Understanding Hash Collisions</h3> <p>Hash encoding‚Äôs memory efficiency comes with a tradeoff: <strong>hash collisions</strong>. When the grid size exceeds the hash table size, multiple different spatial positions can map to the same hash index.</p> <p>For example, with a hash table size of \(2^{22}\) (4 million entries) and level 24 grid resolution of \(2^{28}\) cells, only 1 in 64 grid cells gets a unique hash entry. The other 63 collide.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/collision_heatmap.png" alt="Collision Rate Analysis" style="width: 100%;"/> <figcaption>Hash collision rates across resolution levels for different data distributions. Fine levels (high resolution) show expected 2-4% collision rates, except for power-of-2 artifacts at level 23.</figcaption> </figure> <p><strong>Are collisions bad?</strong> Not necessarily. The hash encoding literature<d-cite key="muller2022instant"></d-cite> shows that downstream networks (MLPs) can learn to disambiguate collisions when they‚Äôre relatively rare. The hash function‚Äôs randomness actually acts as a form of regularization.</p> <p>However, <strong>catastrophic collision patterns</strong> destroy information. If all temporal variations at a location map to the same index, we lose the ability to model temporal dynamics.</p> <h3 id="the-uint32-overflow-discovery">The uint32 Overflow Discovery</h3> <p>During development, we discovered bizarre collision patterns in temporal grids:</p> <ul> <li><strong>Level 8</strong>: 100% collision rate (only ~978 unique indices for 41,261 coordinates)</li> <li><strong>Levels 13-19</strong>: 99.9% collision rate (all coordinates with same spatial position but different times mapped identically)</li> </ul> <p>This violated the expected monotonic decrease in collisions as resolution increases. Something was fundamentally broken.</p> <p>After extensive debugging, we discovered a <strong>critical integer overflow bug</strong> in the CUDA kernel:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// BUGGY CODE</span>
<span class="kt">uint32_t</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">;</span> <span class="n">d</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">index</span> <span class="o">+=</span> <span class="n">pos_grid</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride</span><span class="p">;</span>
    <span class="n">stride</span> <span class="o">*=</span> <span class="n">resolution</span><span class="p">[</span><span class="n">d</span><span class="p">];</span>  <span class="c1">// OVERFLOW!</span>
<span class="p">}</span>
</code></pre></div></div> <p>At level 8 with resolution 2048 per dimension:</p> <ul> <li>After processing first two dimensions: <code class="language-plaintext highlighter-rouge">stride = 2048 √ó 2048 = 4,194,304</code></li> <li>Next multiplication: <code class="language-plaintext highlighter-rouge">4,194,304 √ó 2048 = 8,589,934,592</code></li> <li><strong>This overflows uint32 (max 4,294,967,295) and wraps to 0!</strong></li> </ul> <p>When stride became 0, the temporal dimension contributed nothing to the hash index. All temporal variation was lost.</p> <p><strong>The fix</strong> was simple but critical‚Äîuse 64-bit arithmetic for intermediate calculations:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// FIXED CODE</span>
<span class="kt">uint64_t</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// Prevents overflow</span>
</code></pre></div></div> <p>After this fix:</p> <ul> <li>Level 8: 100% ‚Üí <strong>40.5%</strong> collision rate</li> <li>Level 13: 99.9% ‚Üí <strong>2.4%</strong> collision rate</li> <li>Level 19: 99.9% ‚Üí <strong>2.1%</strong> collision rate</li> </ul> <p>This bug hunt revealed an important lesson: <strong>subtle integer overflow can catastrophically corrupt spatiotemporal encodings</strong>. The bug only manifested at specific resolution/hash-table size combinations, making it nearly invisible without careful analysis.</p> <h3 id="collision-patterns-across-scales">Collision Patterns Across Scales</h3> <p>Even with the overflow bug fixed, hash collisions are inherent to memory-efficient encoding. We profiled collision rates across 10 different spatiotemporal data distributions:</p> <ol> <li><strong>Uniform Random</strong>: Global Earth surface sampling</li> <li><strong>Continental Sparse</strong>: Sparse coverage of North America</li> <li><strong>City-Scale Cluster</strong>: 10km √ó 10km dense sampling</li> <li><strong>Building-Scale</strong>: Single 10m √ó 10m area over time</li> <li><strong>Time Series</strong>: Fixed locations sampled repeatedly over time</li> </ol> <p>Results showed collision rates ranging from 0% (coarse levels) to 2-4% (fine levels), with expected power-of-2 artifacts at level 23 (67M grid cells, 4M hash entries = exact 16√ó ratio).</p> <p>While 2-4% collision rate is acceptable for many applications, we wanted to push further: <strong>Can we reduce hash table size even more while maintaining quality?</strong></p> <p>This led us to learned hash probing.</p> <hr/> <h2 id="learned-hash-probing">Learned Hash Probing</h2> <h3 id="how-learned-probing-works">How Learned Probing Works</h3> <p>Learned hash probing<d-cite key="takikawa2023compact"></d-cite> is a technique that learns to resolve hash collisions intelligently. Instead of a single hash function, we use <strong>dual hashing with learned offsets</strong>:</p> \[\text{index} = N_p \times h_1(\mathbf{x}) + \mathcal{D}_c[h_2(\mathbf{x})]\] <p>where:</p> <ul> <li>\(h_1(\mathbf{x})\): Primary hash function (coarse spatial localization)</li> <li>\(h_2(\mathbf{x})\): Secondary hash with different primes (decorrelated)</li> <li>\(\mathcal{D}_c\): Learned codebook of probe offsets</li> <li>\(N_p\): Probing range (typically 4, 8, or 16)</li> </ul> <p>The codebook \(\mathcal{D}_c\) is learned during training via gradients. Initially, probes are uniformly distributed across the \(N_p\) candidates. As training progresses, the model learns which probe indices minimize collisions for the specific data distribution.</p> <p><strong>Backward pass</strong> uses a straight-through estimator: during forward pass, we select a discrete probe index via argmax. During backward pass, we treat the discrete selection as differentiable by distributing gradients across all \(N_p\) candidates weighted by their softmax probabilities.</p> <p>This allows the model to learn <strong>data-adaptive collision resolution</strong> rather than relying on random hash functions alone.</p> <h3 id="extreme-compression-results">Extreme Compression Results</h3> <p>Learned hash probing enables dramatic parameter reduction. On the Globe-LFMC 2.0 benchmark<d-cite key="yebra2024globelfmc"></d-cite>:</p> <table> <thead> <tr> <th>Configuration</th> <th>Parameters</th> <th>GPU Memory</th> <th>Speed</th> <th>MAE</th> <th>R¬≤</th> <th>vs Baseline</th> </tr> </thead> <tbody> <tr> <td><strong>Baseline</strong> (\(2^{22}\) hash)</td> <td>724M</td> <td>12GB+</td> <td>1√ó</td> <td>16.6 pp</td> <td>0.582</td> <td>‚Äî</td> </tr> <tr> <td><strong>Learned Probing</strong> (\(2^{22}\))</td> <td>724M</td> <td>12GB+</td> <td>1.7√ó</td> <td><strong>12.4 pp</strong></td> <td><strong>0.745</strong></td> <td>+28% R¬≤</td> </tr> <tr> <td><strong>Compressed</strong> (\(2^{14}\) hash)</td> <td><strong>5.1M</strong></td> <td><strong>850MB</strong></td> <td><strong>4√ó</strong></td> <td><strong>15.0 pp</strong></td> <td><strong>0.668</strong></td> <td>+14.7% R¬≤</td> </tr> </tbody> </table> <p>The compressed configuration achieves:</p> <ul> <li><strong>99.3% parameter reduction</strong> (724M ‚Üí 5.1M)</li> <li><strong>93% memory reduction</strong> (12GB ‚Üí 850MB)</li> <li><strong>4√ó training speedup</strong></li> <li><strong>Still outperforms baseline</strong> by 14.7% R¬≤</li> </ul> <p>This is remarkable: by shrinking the hash table by \(256√ó\) (\(2^{22}\) ‚Üí \(2^{14}\)) and adding learned probing, we maintain‚Äîand even improve‚Äîperformance while fitting on edge devices.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/compression_tradeoff.png" alt="Compression Tradeoff" style="width: 90%;"/> <figcaption>Performance vs parameter count. Learned hash probing (orange) enables 99% compression with minimal quality loss. The compressed 5.1M model outperforms the 724M baseline.</figcaption> </figure> <p><strong>Why does compression improve performance?</strong> We hypothesize that extreme compression acts as regularization. The forced sharing of hash table entries encourages the model to learn more generalizable spatiotemporal features rather than memorizing training locations. This is similar to how dropout or weight decay can improve generalization.</p> <hr/> <h2 id="experimental-validation">Experimental Validation</h2> <p>We evaluate Earth4D through three research questions, each designed to test a specific capability required for world models.</p> <h3 id="q1-matching-foundation-models-with-coordinates-alone">Q1: Matching Foundation Models with Coordinates Alone</h3> <p><strong>Question</strong>: Can Earth4D achieve state-of-the-art performance using only spatiotemporal coordinates, without satellite imagery, weather data, or other multimodal inputs?</p> <p><strong>Dataset</strong>: Globe-LFMC 2.0<d-cite key="yebra2024globelfmc"></d-cite>, a global benchmark for predicting Live Fuel Moisture Content (LFMC)‚Äîthe percentage of water in vegetation relative to dry weight. LFMC is critical for wildfire risk assessment.</p> <ul> <li>89,764 field measurements across diverse plant species, geographic regions, and temporal periods (2000-2023)</li> <li>Train/test split: 76,467 / 13,297 (official AI2 split for fair comparison)</li> </ul> <p><strong>Baseline</strong>: Galileo<d-cite key="tseng2025galileo"></d-cite>, a Vision Transformer (5.3M parameters) pre-trained by Allen Institute for AI on:</p> <ul> <li>Sentinel-2 optical imagery (10m resolution, 13 spectral bands)</li> <li>Sentinel-1 SAR (radar, cloud-penetrating)</li> <li>ERA-5 weather reanalysis (temperature, precipitation, etc.)</li> <li>TerraClimate soil moisture and climate data</li> <li>SRTM topography (elevation, slope, aspect)</li> <li>(x,y,z,t) coordinates and species type</li> </ul> <p><strong>Earth4D Architecture</strong>:</p> <ul> <li>Earth4D encodes (x,y,z,t) into 192D embeddings</li> <li>Concatenated with learnable species embedding (initialized randomly)</li> <li>MLP predicts LFMC percentage</li> </ul> <p><strong>Results</strong>:</p> <table> <thead> <tr> <th>Model</th> <th>Data Inputs</th> <th>MAE</th> <th>R¬≤</th> </tr> </thead> <tbody> <tr> <td><strong>Galileo</strong> (pretrained)</td> <td>Coordinates + Species + <strong>Multimodal Remote Sensing</strong></td> <td>12.6 pp</td> <td>0.72</td> </tr> <tr> <td><strong>Earth4D</strong> (learned probing)</td> <td><strong>Coordinates + Species only</strong></td> <td><strong>12.4 pp</strong></td> <td><strong>0.745</strong></td> </tr> </tbody> </table> <p>Earth4D <strong>surpasses the pretrained foundation model</strong> (12.4 vs 12.6 MAE, 0.745 vs 0.72 R¬≤) using only coordinates and species embeddings. No satellite imagery. No weather data. No topography.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/lfmc_results.png" alt="LFMC Prediction Results" style="width: 100%;"/> <figcaption><b>Top</b>: Distribution of absolute errors across 13,297 test samples (median 7.1pp). <b>Left</b>: Geographic error distribution shows low error in well-sampled regions. <b>Right</b>: Temporal predictions track ground truth LFMC across seasons (2017-2023).</figcaption> </figure> <p><strong>What‚Äôs happening?</strong> Earth4D learns that certain spatiotemporal coordinates correlate with LFMC patterns:</p> <ul> <li><strong>Spatial</strong>: Coastal California (37¬∞N, -122¬∞W, low elevation) tends toward high LFMC in winter</li> <li><strong>Temporal</strong>: Summer months (June-August) show lower LFMC across most species</li> <li><strong>Elevation</strong>: Higher elevations retain moisture longer</li> <li><strong>Interactions</strong>: Spatial patterns shift with seasons (spatiotemporal coupling)</li> </ul> <p>The 4D hash encoding captures these multi-scale correlations across 24 resolution levels. Coarse levels encode climate zones (Mediterranean, desert, temperate). Fine levels encode microclimate variations.</p> <p>Crucially, the <strong>species embedding provides botanical context</strong> that combines with spatiotemporal features. The model learns that Species A in Location X at Time T has different moisture dynamics than Species B at the same location and time.</p> <p>This result challenges a common assumption: <strong>multimodal pretraining isn‚Äôt always necessary if you have rich positional encodings and the right inductive biases</strong>.</p> <h3 id="q2-can-we-achieve-99-parameter-reduction">Q2: Can We Achieve 99% Parameter Reduction?</h3> <p><strong>Question</strong>: Does the extreme compression result (99% parameter reduction, 4√ó speedup) shown earlier generalize across different configurations?</p> <p><strong>Experiment</strong>: We systematically vary hash table size and probing range across the LFMC benchmark:</p> <table> <thead> <tr> <th>Hash Size</th> <th>Probing</th> <th>Parameters</th> <th>Speed</th> <th>MAE</th> <th>R¬≤</th> <th>Memory</th> </tr> </thead> <tbody> <tr> <td>\(2^{22}\)</td> <td>Disabled</td> <td>724M</td> <td>1.0√ó</td> <td>16.6</td> <td>0.582</td> <td>12GB+</td> </tr> <tr> <td>\(2^{22}\)</td> <td>\(N_p=4\)</td> <td>724M</td> <td>1.5√ó</td> <td>13.2</td> <td>0.698</td> <td>12GB+</td> </tr> <tr> <td>\(2^{22}\)</td> <td>\(N_p=32\)</td> <td>724M</td> <td>1.7√ó</td> <td><strong>12.4</strong></td> <td><strong>0.745</strong></td> <td>12GB+</td> </tr> <tr> <td>\(2^{18}\)</td> <td>\(N_p=32\)</td> <td>45M</td> <td>1.8√ó</td> <td>13.8</td> <td>0.672</td> <td>1.5GB</td> </tr> <tr> <td>\(2^{14}\)</td> <td>\(N_p=32\)</td> <td><strong>5.1M</strong></td> <td><strong>4.0√ó</strong></td> <td>15.0</td> <td>0.668</td> <td>850MB</td> </tr> </tbody> </table> <p><strong>Key findings</strong>:</p> <ol> <li><strong>Learned probing consistently improves performance</strong> even at full hash table size (16.6 ‚Üí 12.4 MAE)</li> <li><strong>Larger probing range (\(N_p\)) improves quality</strong> but adds training overhead</li> <li><strong>Sweet spot: \(2^{18}\) hash + \(N_p=32\)</strong> balances quality and efficiency (93.8% reduction, strong performance)</li> <li><strong>Extreme compression (\(2^{14}\)) remains viable</strong> for edge deployment</li> </ol> <p>The 99% reduction result is robust across multiple trials and random seeds. The key enabler is learned probing‚Äôs ability to adaptively resolve collisions based on data distribution.</p> <h3 id="q3-rgb-reconstruction-from-elevation">Q3: RGB Reconstruction from Elevation</h3> <p><strong>Question</strong>: Can Earth4D learn to infer RGB pixel values from (x,y,z,t) coordinates alone?</p> <p>This tests a different capability: <strong>pure spatiotemporal function approximation</strong> without any auxiliary labels (like species type in LFMC).</p> <p><strong>Dataset</strong>: 5.8M coordinate-color pairs from Houston coastal wetlands:</p> <ul> <li><strong>Input</strong>: USGS 3DEP LiDAR elevation (x,y,z in ECEF, t = acquisition date)</li> <li><strong>Target</strong>: USDA NAIP RGB imagery (R,G,B values at corresponding location/time)</li> </ul> <p>The objective is \((x,y,z,t) \rightarrow (r,g,b)\): given only coordinates, predict the RGB color.</p> <p><strong>Architecture</strong>: Earth4D (192D) ‚Üí MLP (3 hidden layers, 128 units) ‚Üí RGB (3 channels)</p> <p><strong>Results</strong>:</p> <table> <thead> <tr> <th>Configuration</th> <th>Validation Loss</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>Baseline (no probing)</td> <td>0.0847</td> <td>‚Äî</td> </tr> <tr> <td>Learned Probing (\(N_p=32\))</td> <td><strong>0.0694</strong></td> <td><strong>-18%</strong></td> </tr> </tbody> </table> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/rgb_reconstruction.png" alt="RGB Reconstruction" style="width: 100%;"/> <figcaption>RGB reconstruction from LiDAR elevation in Houston wetlands (2018). <b>Left to right</b>: LiDAR height, ground truth RGB, baseline reconstruction, learned probing reconstruction (18% lower loss).</figcaption> </figure> <p>The model learns complex correlations:</p> <ul> <li><strong>Water bodies</strong> (low elevation, flat) ‚Üí blue/green hues</li> <li><strong>Vegetation</strong> (moderate elevation, rough terrain) ‚Üí green</li> <li><strong>Urban areas</strong> (high elevation variance) ‚Üí gray/brown</li> <li><strong>Coastal transitions</strong> (elevation gradients) ‚Üí color gradients</li> </ul> <p>Learned probing significantly improves reconstruction quality, especially in fine-detail regions like coastline boundaries and vegetation patches.</p> <p>This experiment demonstrates Earth4D‚Äôs ability to capture <strong>implicit spatiotemporal functions</strong> that generalize across diverse phenomena‚Äînot just LFMC prediction, but any function of (x,y,z,t).</p> <hr/> <h2 id="implications-for-world-models">Implications for World Models</h2> <p>Earth4D‚Äôs results suggest several important implications for building world models of Earth observation data:</p> <h3 id="1-positional-encodings-as-first-class-features">1. Positional Encodings as First-Class Features</h3> <p>Traditional approaches treat positional information as auxiliary context for satellite imagery or sensor data. Earth4D flips this: <strong>positional encodings can be primary features</strong> that capture rich spatiotemporal patterns.</p> <p>This is analogous to how CLIP<d-cite key="radford2021learning"></d-cite> showed that text alone (without pixel-level annotations) could guide image understanding through contrastive learning. Here, coordinates alone (without multimodal data) can achieve competitive performance through expressive encoding.</p> <p>For world models, this means we can:</p> <ul> <li><strong>Bootstrap</strong> from coordinate-only data when multimodal inputs are unavailable</li> <li><strong>Reduce dependency</strong> on expensive satellite imagery or weather reanalysis</li> <li><strong>Generalize</strong> to regions/times with sparse observational coverage</li> </ul> <h3 id="2-extreme-efficiency-enables-edge-deployment">2. Extreme Efficiency Enables Edge Deployment</h3> <p>The 99% parameter reduction (724M ‚Üí 5M) makes Earth4D viable for edge deployment:</p> <ul> <li><strong>Satellite onboard processing</strong>: Run Earth4D on satellite GPUs for real-time wildfire detection</li> <li><strong>Mobile disaster response</strong>: Deploy on tablets/phones for field teams</li> <li><strong>IoT sensor networks</strong>: Embed in low-power environmental monitoring stations</li> </ul> <p>This shifts world models from datacenter-scale to <strong>ubiquitous deployment</strong>, enabling real-time decision-making where it matters most.</p> <h3 id="3-learned-probing-as-universal-compression">3. Learned Probing as Universal Compression</h3> <p>Learned hash probing isn‚Äôt specific to Earth observation‚Äîit‚Äôs a <strong>general technique for compressing hash-based encodings</strong>. Applications include:</p> <ul> <li>Neural radiance fields (NeRF) for 3D reconstruction</li> <li>Implicit neural representations for any spatiotemporal data</li> <li>Memory-efficient transformers with positional embeddings</li> </ul> <p>The key insight: <strong>let the model learn to resolve collisions</strong> rather than sizing hash tables conservatively.</p> <h3 id="4-downstream-task-agnostic">4. Downstream Task Agnostic</h3> <p>Earth4D produces a 192D embedding per (x,y,z,t) coordinate. This embedding can feed into:</p> <ul> <li><strong>Classification</strong>: Crop type, land cover, disaster detection</li> <li><strong>Regression</strong>: Temperature, precipitation, soil moisture</li> <li><strong>Segmentation</strong>: Flood extent, deforestation boundaries</li> <li><strong>Generation</strong>: Synthesizing satellite imagery from coordinates</li> <li><strong>Forecasting</strong>: Predicting future states from current embeddings</li> </ul> <p>By separating the positional encoder from task-specific heads, we enable <strong>transfer learning</strong> across Earth observation tasks. Pretrain Earth4D on one task (LFMC), then fine-tune on another (crop yield), reusing the spatiotemporal representations.</p> <h3 id="5-foundation-for-multimodal-world-models">5. Foundation for Multimodal World Models</h3> <p>While Earth4D succeeds with coordinates alone, it‚Äôs designed for <strong>fusion with multimodal encoders</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Satellite Image] ‚Üí Vision Encoder ‚Üí 512D
[Weather Data]    ‚Üí Time Series Enc ‚Üí 256D
[(x,y,z,t)]       ‚Üí Earth4D         ‚Üí 192D
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Concatenate       ‚Üí Transformer     ‚Üí Predictions
</code></pre></div></div> <p>The 4D positional embedding provides <strong>spatiotemporal grounding</strong> for other modalities. An image patch at (lat, lon) gets enriched with Earth4D‚Äôs multi-resolution features encoding its geospatial context.</p> <p>This mirrors how language models use positional encodings‚Äînot as replacements for tokens, but as essential context that enables attention mechanisms to reason about structure.</p> <hr/> <h2 id="limitations-and-future-work">Limitations and Future Work</h2> <p>While Earth4D demonstrates strong performance, several limitations and open questions remain:</p> <h3 id="power-of-2-collision-artifacts">Power-of-2 Collision Artifacts</h3> <p>At level 23 (resolution \(2^{26}\), hash table \(2^{22}\)), collision rate jumps to 3.8% due to exact power-of-2 ratio. This creates periodic artifacts in hash distribution.</p> <p><strong>Mitigation</strong>: Use non-power-of-2 hash table sizes (large primes) or increase hash capacity. However, power-of-2 sizes align with GPU memory boundaries and enable bitwise optimizations.</p> <h3 id="hyperparameter-sensitivity">Hyperparameter Sensitivity</h3> <p>Earth4D has several hyperparameters:</p> <ul> <li>Number of resolution levels (default 24)</li> <li>Hash table size per grid (\(2^{14}\) to \(2^{22}\))</li> <li>Probing range \(N_p\) (2, 4, 8, 16, 32)</li> <li>Codebook size \(N_c\) (512 to 4096)</li> </ul> <p>While we provide reasonable defaults, <strong>optimal settings vary by task</strong>. Automated hyperparameter search (e.g., using validation loss) would improve usability.</p> <h3 id="learning-rate-tuning-for-probing">Learning Rate Tuning for Probing</h3> <p>Index logits gradients are 5-7 orders of magnitude smaller than embedding gradients (inherent to straight-through estimators). We recommend 100√ó higher learning rate for index logits:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span>
    <span class="p">{</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">:</span> <span class="n">encoder</span><span class="p">.</span><span class="n">embeddings</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">},</span>
    <span class="p">{</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">:</span> <span class="n">encoder</span><span class="p">.</span><span class="n">index_logits</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1e-1</span><span class="p">}</span>
<span class="p">])</span>
</code></pre></div></div> <p>This dual learning rate requirement adds complexity. <strong>Automatic gradient rescaling</strong> could simplify training.</p> <h3 id="global-vs-regional-tradeoffs">Global vs Regional Tradeoffs</h3> <p>Earth4D uses a single hash table for the entire planet. This is memory-efficient but treats all regions equally. Some regions (densely sampled urban areas) may benefit from finer-grained encoding than sparse regions (oceans, deserts).</p> <p><strong>Future work</strong>: Adaptive hash allocation based on data density. Allocate more hash capacity to high-information regions, less to homogeneous areas.</p> <h3 id="temporal-resolution-assumptions">Temporal Resolution Assumptions</h3> <p>Our experiments normalize time to \([0, 1]\) over the dataset‚Äôs temporal range. For applications spanning centuries (climate modeling), we may need explicit multi-scale temporal encoding (years, months, days, hours) similar to spatial multi-resolution.</p> <h3 id="interpretability">Interpretability</h3> <p>While Earth4D learns effective representations, understanding <strong>what</strong> it learns remains challenging. Visualization of hash table features could reveal:</p> <ul> <li>Which spatiotemporal patterns activate specific hash entries?</li> <li>How do features at different resolution levels specialize?</li> <li>Can we interpret learned probe offsets?</li> </ul> <p>Techniques from mechanistic interpretability<d-cite key="olah2020zoom"></d-cite> could shed light on Earth4D‚Äôs internal representations.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>We presented Earth4D, a production-ready 4D space-time positional encoder that achieves state-of-the-art performance on ecological forecasting while using only spatiotemporal coordinates. Through decomposed hash encoding and learned hash probing, Earth4D demonstrates:</p> <ol> <li><strong>Matching foundation models</strong> pretrained on multimodal Earth observation data, using coordinates alone</li> <li><strong>99% parameter reduction</strong> with maintained or improved performance</li> <li><strong>Planetary-scale coverage</strong> from sub-meter to continental resolution</li> <li><strong>4√ó training speedup</strong> enabling practical deployment</li> </ol> <p>These results challenge the assumption that world models require massive multimodal pretraining. Rich spatiotemporal representations, learned through 4D hash encoding, can capture complex Earth dynamics with remarkable efficiency.</p> <p>Earth4D is fully open source and ready for integration into world models:</p> <p><strong>GitHub</strong>: <a href="https://github.com/legel/deepearth">https://github.com/legel/deepearth</a></p> <p>As we build AI systems to understand and simulate our planet‚Äîfor climate forecasting, disaster response, agricultural planning, and beyond‚Äî<strong>positional encoding matters</strong>. Earth4D provides a foundation for world models that is memory-efficient, expressive, and ready for production deployment.</p> <p>The future of world models may not require encoding everything about Earth. Perhaps we just need to encode Earth‚Äôs space-time structure effectively‚Äîand let the model discover the rest.</p> <hr/> <h2 id="acknowledgments">Acknowledgments</h2> <p>We thank the Allen Institute for AI for releasing the Globe-LFMC 2.0 dataset and Galileo baseline. We thank NVIDIA for open-sourcing InstantNGP, which inspired Earth4D‚Äôs architecture. We thank the USGS 3DEP and USDA NAIP programs for providing public LiDAR and imagery data.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[How decomposed 4D hash encoding with learned probing enables planetary-scale deep learning with 99% parameter reduction while matching foundation model performance]]></summary></entry><entry><title type="html">Elastic Weight Consolidation (EWC): Nuts and Bolts</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/elastic-weight-consolidation-nuts-bolts/" rel="alternate" type="text/html" title="Elastic Weight Consolidation (EWC): Nuts and Bolts"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/elastic-weight-consolidation-nuts-bolts</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/elastic-weight-consolidation-nuts-bolts/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>In this blogpost, we present a theoretical support of the continual learning method <strong>Elastic Weight Consolidation</strong>, introduced in the paper titled ‚ÄòOvercoming catastrophic forgetting in neural networks‚Äô <d-cite key="kirkpatrick2017overcoming"></d-cite>. Being one of the most cited papers in regularized methods for continual learning, this blogpost disentangles the underlying concept of the proposed objective function. We assume that the reader is aware of the basic terminologies of continual learning.</p> <h2 id="introduction">Introduction</h2> <p>Following are the notations used throughout this blogpost. Vectors and matrices are denoted in bold lowercase and bold uppercase, respectively. Superscript $^{\top}$ denotes matrix transpose. $\mathbb{E}[\cdot]$ denotes the expectation operator. An optimum value of a variable is denoted by adding a superscript $^{\star}$.</p> <p>Continual learning is a much desired attribute for neural networks. For example, if we train a model to distinguish between images of a cat and a dog (task 1), and subsequently train it again to distinguish between images of chair and table (task 2), the model should be able to retain its knowledge on task 1 even after learning task 2. In simple terms, our network model should be able to perform equally well on all seen tasks, even after learning new ones. Any degradation of performance on the previous tasks after learning new ones is fittingly termed as <em>catastrophic forgetting</em>. This sub-research area has seen an insurgence in works in recent times <d-cite key="kirkpatrick2017overcoming,zenke2017continual,li2017learning,aljundi2018memory"></d-cite>. Briefly, the continual learning scenarios can be categorized into following <d-cite key="van2019three"></d-cite>:</p> <ul> <li><strong>Task-Incremental Learning</strong>: For the given set of tasks, the task identity is known during testing.</li> <li><strong>Domain-Incremental Learning</strong>: For the given set of tasks, task identity is not provided during testing, but need not infer the same.</li> <li><strong>Class-Incremental Learning</strong>: For the given set of tasks, task identity is not provided during testing, but has to infer the same.</li> </ul> <p>We highly recommend <d-cite key="van2019three,wiewel2019localizing"></d-cite> for a good overview of different methodologies to alleviate catastrophic forgetting as well as continual learning in general. The next Section describes the well studied regularization method of continual learning: Elastic Weight Consolidation. It presents a solution to the continual learning problem by making task-specific synaptic (<em>read</em> network parameters) consolidation. Based on the theory of plasticity of post-synaptic dendritic spines in the brain, this method presents a paradigm that marks how important is a network parameter to the previous tasks and penalizes any change made to it depending upon the importance, while learning new tasks.</p> <h2 id="elastic-weight-consolidation">Elastic Weight Consolidation</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig1-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig1-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig1.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" title="Possible configurations of Œ∏*_A" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <strong>Possible configurations of</strong> $\boldsymbol{\theta}^\star_{\mathcal{A}}$. The shaded region represents a space of optimum $\boldsymbol{\theta}_{\mathcal{A}}$ with acceptable errors w.r.t. $\boldsymbol{\theta}^\star_{\mathcal{A}}$ for task $\mathcal{A}$. </div> <p>Denote parameters of layers of a deep neural network (DNN) with $\boldsymbol{\theta}$. Training DNNs generates a mapping between the input distribution space and target distribution space. This is done by finding out an optimum</p> <p>\begin{equation} \boldsymbol{\theta} = \boldsymbol{\theta}^\star \end{equation}</p> <p>which results in the least error in the training objective. It has been shown in earlier works <d-cite key="sussmann1992uniqueness"></d-cite> that such a mapping can be obtained with many configurations of $\boldsymbol{\theta}^\star$, represented in the figure above. The term <em>many configurations</em> can be interpreted as a solution space around the most optimum $\boldsymbol{\theta}$ with acceptable error in the learned mapping. Note that in figures to follow, the shaded ellipses represent the solution of individual tasks where as the overlapping region of multiple ellipses, marked by diagonal lines, represents the common solution space for all tasks.</p> <p>Let‚Äôs begin with a simple case of two tasks, task $\mathcal{A}$ and task $\mathcal{B}$. To have a configuration of parameters that performs well for both $\mathcal{A}$ and $\mathcal{B}$, the network should be able to pick $\boldsymbol{\theta}$ from the overlapping region of the individual solution spaces (see Figure 2). This is with the assumption that there is always an overlapping region for the solution spaces of all tasks for the network to learn them sequentially. A case of four tasks has been illustrated in Figure 2. In the first instance, the network can learn any</p> <p>\begin{equation} \boldsymbol{\theta} = \boldsymbol{\theta}_{\mathcal{A}} \end{equation}</p> <p>that performs well for task $\mathcal{A}$. But with the arrival of task $\mathcal{B}$, the network should pick up a</p> <p>\begin{equation} \boldsymbol{\theta} = \boldsymbol{\theta}_{\mathcal{A}, \mathcal{B}} \end{equation}</p> <p>The next question that arrives is how can the network learn the a set of parameters that lies in this overlapping region. To this end, EWC presents a method of selective regularization of parameters $\boldsymbol{\theta}$. After learning $\mathcal{A}$, this regularization method identifies which parameters are important for $\mathcal{A}$, and then penalizes any change made to the network parameters according to their importance while learning $\mathcal{B}$.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig2-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig2-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig2.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" title="Overlap of possible configurations of Œ∏*" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <strong>Overlap of possible configurations of</strong> $\boldsymbol{\theta}^\star$. The overlapping space represents an optimum parameter region where the network performs without any catastrophic degradation on previous tasks. </div> <h3 id="intractability-of-posterior-of-mathcala-and-its-approximation">Intractability of posterior of $\mathcal{A}$ and its approximation</h3> <p>To formulate the objective, we start by taking a Bayesian approach needed to estimate the network parameters $\boldsymbol{\theta}$. More specifically given the data $\boldsymbol{\Sigma}$, we want to learn the posterior probability distribution function $p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})$. Following <d-cite key="lherranz2018rotating"></d-cite> and using Bayes rule, we can write</p> <p>\begin{equation} \underbrace{p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})}_{\text{posterior}} = \dfrac{\overbrace{p(\boldsymbol{\Sigma}\mid\boldsymbol{\theta})}^{\text{likelihood}}\overbrace{p(\boldsymbol{\theta})}^{\text{prior}}}{p(\boldsymbol{\Sigma})} \end{equation}</p> <p>Since maximizing a function is same as maximizing its logarithm, we take $\log(\cdot)$ of the above equation as follows:</p> <p>\begin{equation} \log(p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})) = \log(p(\boldsymbol{\Sigma}\mid\boldsymbol{\theta})) +\log(p(\boldsymbol{\theta})) - \log(p(\boldsymbol{\Sigma})) \end{equation}</p> <p>To train the neural network on $\boldsymbol{\Sigma}$, the objective function to be optimized over the log-likelihood function:</p> <p>\begin{equation} \text{argmax}_{\boldsymbol{\theta}}{\ell(\boldsymbol{\theta}) = \log(p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma}))} \end{equation}</p> <p>For the case of given two independent tasks such that</p> <p>\begin{equation} \boldsymbol{\Sigma} = {\mathcal{A}, \mathcal{B}} \end{equation}</p> <p>(with $\mathcal{B}$ appearing in sequence after $\mathcal{A}$), the log-posterior can be written as:</p> <p>\begin{equation} \log(p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})) = \log(p(\mathcal{B}\mid\boldsymbol{\theta})) +\log(p(\boldsymbol{\theta}\mid\mathcal{A})) - \log(p(\mathcal{B})) \end{equation} where the independence of $\mathcal{A}$ and $\mathcal{B}$ is used. Following the Bayesian formulation, $p(\mathcal{B}\mid\boldsymbol{\theta})$ is the loss for current task $\mathcal{B}$, $p(\mathcal{B})$ is the likelihood for $\mathcal{B}$, and now posterior $p(\boldsymbol{\theta}\mid\mathcal{A})$ for $\mathcal{A}$ becomes prior for $\mathcal{B}$.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig3-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig3-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Laplace approximation of true posterior pdf" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <strong>Laplace approximation of true posterior pdf.</strong> $\mathbb{I}_\mathcal{A}$ represents the Fisher Information matrix. </div> <p>Referring to the log-posterior equation, it can be observed that we have to deal with the function $p(\boldsymbol{\theta}\mid\mathcal{A})$. This is the posterior function for $\mathcal{A}$ which contains the information about the parameters that explain $\mathcal{A}$ using the given network. As discussed in <d-cite key="kirkpatrick2017overcoming"></d-cite>, this posterior function is said to be intractable. Basically, the intractability of $p(\boldsymbol{\theta}\mid\mathcal{A})$ can be interpreted as the function not existing in some interpretable form. Hence, it is difficult to estimate its quantiles. See <d-cite key="tokdar2013lecture"></d-cite> for an example.</p> <p>Next as the posterior is difficult to analyze in its present form, we aim to approximate it using Laplace approximation. In simple terms, Laplace approximation methodology is employed to find a normal distribution approximation to a continuous probability density distribution (see Figure 3). Assuming $p(\boldsymbol{\theta}\mid\mathcal{A})$ is smooth and majorly peaked around its point of maxima (i.e. $\boldsymbol{\theta}^\star_{\mathcal{A}}$), we can approximate it with a normal distribution with mean $\boldsymbol{\theta}^\star_{\mathcal{A}}$ and variance $[\mathbb{I}_{\mathcal{A}}]^{-1}$. This brings us to the question on how did we come to the conclusion on these particular values of mean and variance for the normal distribution.</p> <p>To begin with, compute the second order Taylor expansion of $\ell(\boldsymbol{\theta})$ around $\boldsymbol{\theta}^\star_{\mathcal{A}}$ as follows:</p> <p>\begin{equation} \ell(\boldsymbol{\theta})\approx \ell(\boldsymbol{\theta}^\star_{\mathcal{A}}) +( \dfrac{\partial\ell(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}}) + \dfrac{1}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top(\dfrac{\partial^2\ell(\boldsymbol{\theta})}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}}) + \text{(higher order terms)} \end{equation}</p> <p>Neglecting higher order terms and noting that</p> <p>\begin{equation} \dfrac{\partial\ell(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}} = 0 \end{equation}</p> <p>(slope of tangent at peak), we have: \(\begin{equation} \ell(\boldsymbol{\theta})\approx \ell(\boldsymbol{\theta}^\star_{\mathcal{A}}) + \dfrac{1}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top\underbrace{(\dfrac{\partial^2\ell(\boldsymbol{\theta})}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})}_{\text{Hessian}}(\boldsymbol{\theta} -\boldsymbol{\theta}^\star_{\mathcal{A}}) \end{equation}\) Using the log-posterior equation, we can write the above for task $\mathcal{A}$ as following:</p> <p>\begin{equation} \log(p(\boldsymbol{\theta}\mid\mathcal{A})) = \log(p(\boldsymbol{\theta}^\star_{\mathcal{A}}\mid\mathcal{A})) + \dfrac{1}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top(\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}}) + \Delta \end{equation}</p> <p>where</p> <p>\begin{equation} \Delta = \log(p(\boldsymbol{\theta}^\star_{\mathcal{A}}\mid\mathcal{A})) \end{equation}</p> <p>Next, write</p> <p>\begin{equation} (\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}}) = -((-\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})^{-1})^{-1} \end{equation}</p> <p>and replace it back in the equation to express the same in the standard form of normal distribution function:</p> <p>\begin{equation} p(\boldsymbol{\theta}\mid\mathcal{A}) = \epsilon\exp(-\dfrac{1}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top((-\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})^{-1})^{-1}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})) \end{equation}</p> <p>where</p> <p>\begin{equation} \epsilon = \exp(\Delta) \end{equation}</p> <p>is a constant. From this equation, it can be concluded that we have obtained the Laplace approximation of posterior pdf as:</p> <p>\begin{equation} p(\boldsymbol{\theta}\mid\mathcal{A})\sim\mathcal{N}(\boldsymbol{ \theta}^\star_{\mathcal{A}}, (-\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})^{-1}) \end{equation}</p> <p>Notice the variance of the estimated normal distribution of $p(\boldsymbol{\theta}\mid\mathcal{A})$. Given $\boldsymbol{ \theta}^\star_{\mathcal{A}}$, the term $\log(p(\boldsymbol{\theta}\mid\mathcal{A}))$ represents the log-likelihood of posterior pdf $p(\boldsymbol{\theta}\mid\mathcal{A})$. Clearly, the term represents the inverse of <strong>Fisher information matrix</strong> (FIM), \(\begin{equation} \mathbb{I}_{\mathcal{A}} = \mathbb{E}[-\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}}] \end{equation}\)</p> <p>Note that we obtain $\mathbb{I}_{\mathcal{A}}$ by using the Bayesian equation and treating the prior $p(\boldsymbol{\theta})$ and $p(\mathcal{A})$ constant. This makes derivative of log of the Bayesian equation posterior and likelihood equal. More on this in <strong>Appendix A.2</strong> of <d-cite key="van2019three"></d-cite>. Finally, we get</p> <p>\begin{equation} p(\boldsymbol{\theta}\mid\mathcal{A})\sim\mathcal{N}(\boldsymbol{ \theta}^\star_{\mathcal{A}}, [\mathbb{I}_{\mathcal{A}}]^{-1}) \end{equation}</p> <p>Further, as FIM can also be computed from first order derivatives, we can avoid the Hessian computed in the Taylor expansion using the following property <d-cite key="kay1993fundamentals"></d-cite>:\(\begin{equation} \mathbb{I}_{\mathcal{A}} = \mathbb{E}[-\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}}] = \mathbb{E}[(\dfrac{\partial(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial\boldsymbol{\theta}})(\dfrac{\partial(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial\boldsymbol{\theta}})^\top\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}}] \end{equation}\) Now, we can write the log-posterior equation as: \(\begin{equation} \log(p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})) = \log(p(\mathcal{B}\mid\boldsymbol{\theta})) +\dfrac{\lambda}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top(\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}}) + \epsilon' \end{equation}\) where $\epsilon‚Äô$ accounts for all constants and $\lambda$ is a hyper-parameter introduced to have a trade off between learning $\mathcal{B}$ and not forgetting $\mathcal{A}$. Simplifying more, we have: \(\begin{equation} \log(p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})) = \log(p(\mathcal{B}\mid\boldsymbol{\theta})) - \dfrac{\lambda}{2}(\boldsymbol{\theta} - \boldsymbol{ \theta}^\star_{\mathcal{A}})^\top \mathbb{I}_{\mathcal{A}} (\boldsymbol{\theta} - \boldsymbol{\theta}^\star_{\mathcal{A}}) + \epsilon' \end{equation}\)</p> <p>This implies: \(\begin{equation} \underbrace{\ell(\boldsymbol{\theta})}_{\text{overall loss}} = \underbrace{\ell_\mathcal{B}(\boldsymbol{\theta})}_{\text{loss for \mathcal{B} }} - \underbrace{\dfrac{\lambda}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top\mathbb{I}_{\mathcal{A}}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})}_{\text{weight regularizer}} + \epsilon' \end{equation}\)</p> <p>Further simplification can be found in <d-cite key="van2019three"></d-cite>. Before we end this Section, let‚Äôs discuss how does the FIM indicates the <strong>importance</strong> of the parameters for the previous tasks.</p> <p>We say a network has learnt a task when its objective has reached a minimum in the loss surface. We know that the curvature of such surfaces represent the sensitivity of the network with respect to the optimum $\boldsymbol{\theta}^\star$. This sensitivity can be determined by looking at the direction along which $\boldsymbol{\theta}^\star$ changes. This implies the curvature is inversely proportional to change in $\boldsymbol{\theta}^\star$. Hence, if the more the curvature, a ‚Äò$\delta$‚Äô increment can result in large increase in the loss. Curvature of a curve is denoted by its Hessian and hence in our case, as the second derivative is of the log likelihood function of the posterior pdf, the FIM \(\mathbb{I}_{\mathcal{A}}\) comes into picture. Thus, \(\mathbb{I}_\mathcal{A}\) can tell us which parameter is important to the the previous task as its corresponding element in \(\mathbb{I}_\mathcal{A}\) will have a large value, indicating higher importance. See <d-cite key="maltoni2019continuous"></d-cite> for more.</p> <h2 id="conclusion">Conclusion</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig4-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig4-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sequential training on task B after task A" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <strong>Sequential training on task</strong> $\mathcal{B}$ <strong>after task</strong> $\mathcal{A}$. Left: Train the network as it is: results in 'Forgetting', Middle: Make no change in the parameters of previous tasks, Right: Make changes in the parameters of the previous tasks depending on their importance. </div> <p>In this blogpost, we have presented a theoretical support of the EWC method. We have shown how the intractable posterior function can be approximated using Laplace approximation, and how the Fisher Information Matrix can be used to identify the importance of parameters for previous tasks. The EWC method provides a principled approach to continual learning by selectively regularizing parameters based on their importance to previously learned tasks.</p>]]></content><author><name>Anonymous</name></author><category term="Continual Learning"/><category term="Regularization Methods"/><category term="Theoretical Analysis"/><category term="EWC"/><category term="Catastrophic Forgetting"/><category term="Fisher Information Matrix"/><category term="Bayesian Methods"/><category term="Laplace Approximation"/><summary type="html"><![CDATA[A theoretical deep-dive into the Elastic Weight Consolidation method for continual learning, explaining the mathematical foundations and intuitions behind this influential approach to preventing catastrophic forgetting.]]></summary></entry><entry><title type="html">EvalCards for Standardized Evaluation Reporting</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/evalcards/" rel="alternate" type="text/html" title="EvalCards for Standardized Evaluation Reporting"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/evalcards</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/evalcards/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Classic scientific scandals often turn on withholding of details around exactly how scientific hypotheses were evaluated. For example, the case of the Piltdown Man, where researchers selectively (mis-)reported crucial contextual details, misled evolutionary sciences for decades <d-cite key="vincent1999piltdown"></d-cite>. Lack of reporting standards can also lead to confusion, even when everyone acts in good faith. To illustrate, in 19th-century chemistry, the lack of agreed conventions on atomic weights left the field in chaos, with the same compounds appearing under conflicting formulas, until the Karlsruhe Congress established common standards <d-cite key="ihde1961karlsruhe"></d-cite>. These (and other similar) episodes <d-cite key="goldacre2009bad"></d-cite> instill the same lesson: without reliable reporting conventions, even important discoveries can distort rather than advance science.</p> <p>Evaluation - quantitative measurement of a model‚Äôs performance on a pre-defined task or benchmark - has long been one of the central means of assessing progress in NLP <d-cite key="jones1994towards, church2017emerging, church2019survey, bowman-dahl-2021-will, kiela-etal-2021-dynabench, sainz-etal-2023-nlp"></d-cite>. Despite this, our standards for reporting evaluations have not kept pace <d-cite key="bhatt2021case, belz-etal-2023-non, belz-etal-2025-standard, zhao-etal-2025-sphere"></d-cite>. Such a lack of standards becomes more concerning with the fast adoption of Large Language Models (LLMs) by a wide range of stakeholders, many of whom are not experts and yet heavily depend on such systems to make decisions that impact real-world outcomes <d-cite key="araujo2020ai, bommasani2023holistic"></d-cite>. As LLMs become embedded in critical domains, responsible deployment is a key consideration <d-cite key="10536000, radanliev2024ethics, orr2024building,tripathi2025ethical"></d-cite> and a major part of this includes transparency on what a model can and cannot do. Based on a survey of recent research in the field of evaluation studies, we identify three critical problems stemming from current reporting practices: the <em>Reproducibility Crisis</em>, the <em>Accessibility Crisis</em>, and the <em>Governance Crisis</em>. We discuss why current efforts at transparency <d-cite key="mitchell2019model, gebru2021datasheets"></d-cite> need reconsideration. In light of such issues, we propose EvalCards: concise evaluation summaries which are (i) <em>easy to write</em>, (ii) <em>easy to understand</em>, and (iii) <em>hard to miss</em>. We present case studies of three popular models, showing how difficult it was to gather consistent evaluation details when creating sample EvalCards, and also discuss directions of future work.</p> <p>Our main argument is one for a shift in norms: evaluation reporting is not a marketing exercise but a core component of what it means to release a model responsibly. While the broader challenge of how to evaluate models remains open and complex <d-cite key="laskar2024systematic, chang2024survey, gao2025llm"></d-cite>, our focus here is narrower but nonetheless critical: improving how evaluations are reported. We hope this work sparks conversation and helps move the field toward a culture of more honest and actionable evaluation disclosure practices.</p> <h2 id="problems-with-evaluation-reporting">Problems with Evaluation Reporting</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/eval_card_figure-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/eval_card_figure-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/eval_card_figure-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/eval_card_figure.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ground our analysis of problems in evaluation reporting in NLP and AI, we conducted a survey of recent work with systematic keyword searches related to evaluation and reporting (e.g., evaluation, reporting, disclosure, evaluation artifacts) in ACL Anthology, DBLP, and Google Scholar. We complemented this with a reverse snowball sampling from the most recent broad-scope seminal works in NLP evaluation as seeds <d-cite key="weidinger2025toward, gao2025llm,zhao-etal-2025-sphere,chang2024survey,laskar2024systematic,biderman2024lessons,burnell2023rethink, allen2021evaluation"></d-cite>. We then manually analyzed the works to extract recurring themes of discussion. From this, we identify three overarching crises of reproducibility, accessibility, and governance.</p> <h3 id="reproducibility-crisis">Reproducibility Crisis</h3> <p>In machine learning, the broader reproducibility crisis is well discussed <d-cite key="kapoor2023leakage"></d-cite> and also manifests in evaluation reporting <d-cite key="bouthillier2019unreproducible, dodge-etal-2019-show, belz-etal-2025-standard, zhao-etal-2025-sphere"></d-cite>. More specifically, in the context of model evaluations, prior work repeatedly highlights that published results often cannot be trusted without careful reconstruction of undocumented choices <d-cite key="belz-etal-2021-systematic,burnell2023rethink, belz-etal-2023-non, biderman2024lessons, belz-etal-2025-standard, zhao-etal-2025-sphere"></d-cite>. We conduct some case studies on some recent LLM model releases and discuss three such crucial details that are often inconsistent or missing from evaluation details:</p> <p><strong>Target Capability</strong> Model releases include benchmark names and scores, but can often fail to specify what each benchmark is intended to measure. This problem can be exacerbated for reported scores on large composite benchmarks such as SuperGLUE <d-cite key="wang2019superglue"></d-cite> or HELM <d-cite key="bommasani2023holistic"></d-cite> that aggregate tasks across domains and do not provide clarity, especially for non-experts, as to what capability is targeted.</p> <p><strong>Metric</strong> Reported scores frequently omit which evaluation metric was used, which makes it difficult to assess what a score reflects or to compare performance across models fairly <d-cite key="mizrahi2024state, hu2024unveiling"></d-cite>.</p> <p><strong>Prompting Strategy</strong> Prompting, i.e, the way a query is structured and phrased for LLMs, is one of the most significant variables in model performance <d-cite key="hu2024unveiling, sclar2024quantifying, zhuo-etal-2024-prosa, chatterjee-etal-2024-posix"></d-cite>, yet is often absent from reported results. Also, the absence of consistent reporting on a common baseline strategy, such as zero-shot prompting, further hinders meaningful comparison across models <d-cite key="10.1145/3582269.3615599, ousidhoum-etal-2021-probing"></d-cite>.</p> <h3 id="accessibility-crisis">Accessibility Crisis</h3> <p>A recurring theme in relevant work is the highly fragmented nature of available information on models. Documentation frameworks such as Model Cards, DataSheets, and FactSheets were introduced precisely to improve accessibility of information about models <d-cite key="mitchell2019model, gebru2021datasheets, arnold2019factsheets, bhardwaj2024machine, luo2025lack"></d-cite>. However, even today, evaluation details are dispersed across academic papers, technical appendices, GitHub READMEs, HuggingFace model cards, and blog posts, each using different terminology and presentation styles. This scattered documentation makes it difficult to locate and compare the evaluation results. This has consequences for both researchers and users.</p> <p><strong>For Researchers</strong> When evaluation details are scattered across sources, they are easily overlooked or lost altogether. Important context, such as benchmark versions, question framing, or metric details, may never reach the researchers who rely on these results <d-cite key="belz-etal-2023-non,belz-thomson-2024-2024,belz-etal-2025-standard, belz-etal-2025-2025"></d-cite>.</p> <p><strong>For Users</strong> For non-technical users and decision-makers, the problem is compounded by selective reporting, where only strong benchmark results are emphasized or marketed in some sources, further reducing trust in evaluation claims <d-cite key="arnold2019factsheets"></d-cite>. As a result, users may struggle to select appropriate models, a challenge that becomes particularly critical in high-stakes or sensitive deployment contexts <d-cite key="huijgens2024help, 10.1145/3706598.3713240"></d-cite>.</p> <p>For evaluation to be actionable, it must be consistently visible and easily accessible. Without a standardized way to report evaluations in one place, users are left to piece together incomplete information, undermining efforts to assess a model‚Äôs suitability for deployment.</p> <h3 id="governance-crisis">Governance Crisis</h3> <p>AI legislations across the world today, from US to EU <d-cite key="edwards2021eu, act2024eu, sloane2025systematic, carey2025regulating"></d-cite> and from Singapore to China <d-cite key="pande2023navigating, roberts2021chinese, dong2024meta"></d-cite>, are increasingly concerned with transparency <d-cite key="larsson2020transparency, agrawal2024accountability"></d-cite> and reporting mandates <d-cite key="nagendran2020artificial, laux2024three"></d-cite>. Without standardized evaluation reports, governance of models faces three key problems:</p> <p><strong>Risk Assessment</strong> It becomes challenging to determine model risks when the evaluation methods are not clearly reported <d-cite key="hogan2021ethics, novelli2024taking, reuel2024betterbench, reuel2024open"></d-cite>.</p> <p><strong>Algorithmic Accountability</strong> When developers can selectively report results or omit critical weaknesses, it makes it difficult for external reviewers or regulators to hold systems to consistent standards <d-cite key="shah2018algorithmic, wieringa2020account, horneber2023algorithmic"></d-cite>.</p> <p><strong>Compliance Washing</strong> Akin to ethics washing practices <d-cite key="bietti2020ethics"></d-cite>, AI developers can satisfy regulatory requirements by disclosing something‚Äîeven if that ``something‚Äù is incomplete, selectively positive, or methodologically weak <d-cite key="koshiyama2024towards"></d-cite>. Regulatory compliance becomes a box-ticking exercise, undermining the goals of safety, accountability, and public trust <d-cite key="veale2021demystifying"></d-cite>.</p> <p>These crises highlight that the problem is not only how models are evaluated, but how those evaluations are reported. In the next section, we examine limitations of current standardization efforts and introduce EvalCards as a practical solution for improving evaluation reporting.</p> <h2 id="problems-with-existing-standards">Problems with Existing Standards</h2> <p><strong>Lack of evaluation focus</strong> Existing documentation frameworks rarely place evaluation at the center. Model Cards, DataSheets, and FactSheets typically treat evaluation results as only one component among many. BenchmarkCards <d-cite key="sokol2024benchmarkcards"></d-cite> focuses on information specific to a single benchmark only, with no reference to models. For large language models, however, evaluation is the primary means by which users, researchers, and regulators can understand capabilities and limitations.</p> <p><strong>High effort for developers</strong> Most of the proposed documents, like Model Cards <d-cite key="mitchell2019model"></d-cite>, are lengthy and time-consuming to produce since they require additional analysis like listing out all possible use-cases and users, detailed demographic factor evaluation, intersectional quantitative analyses, etc. This can be especially problematic when many models are being released at a rapid pace. Dedicating extra time to such detailed analysis may not be possible.</p> <p><strong>Limited accessibility for non-experts</strong> For decision-makers, policymakers, and many end-users, these documents are often too technical or jargon-heavy to offer real clarity <d-cite key="mcgregor2025err, crisan2022interactive"></d-cite>. For example, OpenAI‚Äôs system cards, like the <a href="https://openai.com/index/gpt-4o-system-card/">GPT-4o card</a>, offer in-depth safety and governance information but are often very lengthy and complex. As a result, even when provided, they are underutilized by key stakeholders <d-cite key="blodgett-etal-2024-human"></d-cite>.</p> <p><strong>Lack of visibility</strong> While earlier works have talked about model documentation, not many have emphasized the need for visibility. Today, information about models is frequently buried in supplemental materials, obscure repositories, or separate websites‚Äîmaking it difficult to access and easy to overlook <d-cite key="mcgregor2025err"></d-cite>.</p> <h2 id="evalcards">EvalCards</h2> <p>To address the challenges outlined in previous sections, we propose Evaluation Disclosure Cards (EvalCards), a short-form standardized reporting format for model evaluations. In this section, we discuss the design principles of an EvalCard, what it should contain, when it should be created, and where it should be available.</p> <h3 id="design-principles-of-evalcards">Design Principles of EvalCards</h3> <p>Any reporting format must go beyond existing documentation efforts by tackling the practical barriers identified above: focusing on evaluation, reducing the burden on developers, making results clear to a wide range of users, and ensuring that evaluation information is consistently visible wherever models are accessed. We summarize the design principles here:</p> <p><strong>Evaluation Focus</strong> Unlike broader documentation frameworks such as Model Cards or DataSheets, which include information about training data, intended use cases, and ethical considerations, EvalCards place evaluation at the center. The goal is not to capture every possible aspect of model development, but to provide clear and standardized details about what was evaluated, how it was evaluated, and under what conditions.</p> <p><strong>Easy to Write</strong> For transparency to become standard practice, evaluation reporting must be easy to implement. EvalCards are designed to capture only the essential details of model evaluation, making them quick to produce and maintain. This is especially important for smaller organizations and open-source model developers that lack the resources to run large test suites.</p> <p><strong>Easy to Understand</strong> Transparency is meaningless if only a handful of experts can interpret it. Evaluation reports must be designed for broad accessibility, enabling not just researchers but also all those without domain expertise to grasp a model‚Äôs capabilities and risks quickly.</p> <p><strong>Hard to Miss</strong> As discussed, evaluation details are buried in academic papers, supplementary materials, or hidden deep within repositories. Standardized evaluation reports should be integrated directly into any landing pages where models can be accessed, whether that is on HuggingFace Hub, API dashboards, or third-party model provider repositories. By ensuring that evaluation disclosures are always visible and linked to the model itself, we would create a culture where understanding a model‚Äôs capabilities and risks becomes a default part of using models, not an optional deep dive.</p> <h3 id="what-should-an-evalcard-contain">What should an EvalCard contain?</h3> <p><strong>Modalities Evaluated</strong> EvalCards specify which input and output modalities‚Äîsuch as text, image, or audio‚Äîthe model has been evaluated on.</p> <p><strong>Languages Evaluated</strong> As with modalities, clearly stating which languages a model has evaluated on helps define the scope of its real-world applicability. Many models advertise multilingual <code class="language-plaintext highlighter-rouge">training'' or</code>support‚Äô‚Äô, but such claims do not indicate whether those languages have been explicitly tested in any systematic way. Without evaluation, such claims can be misleading <d-cite key="joshi-etal-2020-state, blasi-etal-2022-systematic,talat-etal-2022-reap"></d-cite></p> <p><strong>Capability Evaluation</strong> EvalCards do not prescribe specific benchmarks, but we require developers to explicitly state which core abilities (e.g., summarization, reasoning, factual recall, mathematical problem solving) were evaluated, and to indicate the benchmark chosen for each. Additionally, all reported results should be accompanied by the metric used (e.g., exact match, accuracy, precision@1), zero-shot prompting strategy (to enable better baseline comparison across models), and any alternative prompting strategies tested (e.g., few-shot, chain-of-thought).</p> <p><strong>Safety Evaluation</strong> AI models pose well-documented risks, such as bias <d-cite key="dai2024bias"></d-cite>, toxicity <d-cite key="luong-etal-2024-realistic"></d-cite>, and misinformation generation <d-cite key="zhang2024toward, chen2024combating"></d-cite>. These issues are often under-reported or selectively presented in current evaluation practices <d-cite key="burnell2023rethink, mcgregor2025err"></d-cite>. EvalCards should include a dedicated section for such safety risks. As with capability evaluations, developers should specify the safety feature evaluated, the benchmarks used, the metrics applied, and both the zero-shot and any alternative prompting scores.</p> <p><strong>Developer Footnotes</strong> In this free-text section of the EvalCard, model developers can choose to have relevant footnotes or include any information they think is relevant for users.</p> <h3 id="when-should-evalcards-be-created">When should EvalCards be created?</h3> <p>EvalCards should be generated as part of the initial model release workflow, whether for open-source models or commercial APIs. <em>First,</em> model developers are the ones who trained the model, chose the data, designed the architecture, and tuned the objectives. Standardized evaluation reporting works best when it is done by the people who know the model inside out and at the point of release. Also, most of these evaluations align with internal testing already conducted by developers during model validation phases, and direct reporting can prevent multiple runs of the model on the same tests, leading to reduced climate impact. <em>Second,</em> most models that require evaluation today‚Äîespecially large foundation models with hundreds of billions of parameters‚Äîare built by organizations with substantial compute resources. If a team can train a model with billions of parameters, it is likely to have enough compute to run a standard suite of evaluations.</p> <p>By embedding EvalCard creation into the release pipeline, developers ensure that transparency is delivered upfront, not left to third-party auditors. Furthermore, EvalCards should be updated with each major version change or significant fine-tuning event, reflecting how model behavior may evolve. This keeps users informed of both improvements and potential regressions across the capability and safety dimensions.</p> <h3 id="where-should-evalcards-be-displayed">Where should EvalCards be displayed?</h3> <p>Visibility is a core principle of EvalCards: Evaluation summaries should appear where the model appears. This includes:</p> <ul> <li><strong>Model Repositories</strong>: EvalCards should be a standard, prominently linked file in Huggingface Hub or Github‚Äîsimilar to a README or license‚Äîensuring that anyone downloading or browsing the model can immediately access it.</li> <li><strong>Commercial Platforms</strong>: For closed-source or hosted models accessed through APIs or user interfaces (e.g., ChatGPT, Google Gemini), EvalCards should be integrated into developer dashboards, product documentation, or user-facing pages. This allows users to review evaluation and safety information before deployment or interaction, supporting informed use and regulatory compliance.</li> <li><strong>Research Publications</strong>: For both open and closed-source models, EvalCards should be present as a section in the main text or appendix of academic papers and any technical blogs as a concise summary of evaluation results‚Äîproviding a clear alternative to selective performance highlights.</li> </ul> <h3 id="model-card-vs-evalcard">Model Card vs EvalCard</h3> <p>Model Cards <d-cite key="mitchell2019model"></d-cite> are an early effort at increasing transparency. However, EvalCards are not designed to serve the same purpose and are complementary to Model Cards. Below, we highlight how EvalCards differ from Model Cards and why they are essential in the current landscape:</p> <ul> <li><strong>Evaluation Focus:</strong> Model Cards were developed at a time when most models were custom-built, and detailed information about training, design motivations, and use cases was essential. EvalCards elevate evaluation to the primary focus and provides a decision-time snapshot to help users and regulators quickly evaluate if a model is appropriate for their needs.</li> <li><strong>Ease of Adoption:</strong> Model Cards focus on open-ended description of model information, including detailed analysis of ethnographic and ethical considerations. However, it can serve as a barrier to adoption by overwhelming end-users with less technical expertise. EvalCards are designed to be lightweight and easy to adopt, with structured fields that capture high-signal evaluation details with minimal overhead.</li> <li><strong>Visibility:</strong> Model Cards do not specify where or how they should be displayed, and the information is often buried across multiple sources. EvalCards are displayed where the model is accessed‚Äîon model hubs, APIs, or UIs.</li> </ul> <p>As the norm shifts towards the use of off-the-shelf generative AI models, evaluation becomes the key requirement for responsible model deployment.</p> <h2 id="evalcards-case-studies">EvalCards Case Studies</h2> <p>To implement our idea, we started with case studies of three popular LLMs. We document the issues we found for collecting details for each model‚Äôs evaluation followed by the creation of an EvalCard for each model. In Fig 1, we create an EvalCard for the model <em>OLMO-2-1124-7B-Instruct</em>, from the OLMo project on transparency and open research from the Allen Institute for AI <d-cite key="olmo20252olmo2furious"></d-cite>- which despite being one of the most open models. Metrics and prompting details were dispersed across sources and inconsistent across benchmarks. Accessibility was limited by scattered results throughout the paper, hindering quick assessment. Safety reporting was aggregated into a single score, offering no dataset-level transparency and preventing meaningful analysis of specific risks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_olmo-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_olmo-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_olmo-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_olmo.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In Fig 2, we have an EvalCard for Qwen3-4B-Base, released on the 29th of April 2025. The model‚Äôs release lacked timely and transparent evaluation details: key results were delayed, language coverage and capability claims were ambiguous, and many benchmarks lacked metrics or prompting information. Safety evaluations were entirely absent, making it difficult for users to reliably assess the model‚Äôs performance or risks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_qwen-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_qwen-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_qwen-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_qwen.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In Fig 3, we create an EvalCard for Gemini Flash 2.0 which lacks clear, comprehensive documentation: multilingual support is vaguely described without evidence of evaluated performance, benchmark scores omit metrics and prompting details, and no concrete safety evaluations or red-teaming results are disclosed. As a result, users must rely on incomplete information when assessing the model‚Äôs actual capabilities and risks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_gemini-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_gemini-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_gemini-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-evalcards/evalcard_gemini.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>These case studies reveal a common thread: while open-source models like OLMo 2 generally provide more information than their closed counterparts, the process of compiling that information remains time-consuming and often incomplete. EvalCards offer a structured way to consolidate key details in one place, reducing the need to spend hours navigating scattered documentation and supplementary sources. More importantly, EvalCards make it immediately visible what‚Äôs missing‚Äîturning the absence of evaluation details from something easy to hide into something impossible to ignore. For model providers, EvalCards serve as both a checklist and a commitment: a clear reminder of what transparency actually requires and a visible demonstration of their willingness to provide it.</p> <h2 id="alternative-views">Alternative Views</h2> <p>While we advocate for the adoption of EvalCards, it is important to acknowledge reasonable concerns from different stakeholders. Here, we outline two commonly raised perspectives and address them.</p> <p><strong>A Developer‚Äôs View</strong> Developers may argue that evaluation is already done internally, and publishing it publicly increases workload or reputational risk, especially when performance is uneven. However, as regulatory frameworks like the EU AI Act and US or UK guidelines begin to demand transparency in model capabilities and risks <d-cite key="act2024eu"></d-cite>, structured evaluation disclosures might become a requirement, not a preference. EvalCards offer a proactive way to meet these expectations. Even seeing what has not been evaluated is useful to avoid misinterpretations and build credibility ahead of external audits or compliance checks <d-cite key="raji2020closing, koshiyama2024towards"></d-cite>.</p> <p><strong>A Researcher‚Äôs View</strong> Researchers may note that evaluation is context-sensitive, where evaluations vary by tasks <d-cite key="chang2024survey"></d-cite> or user groups <d-cite key="hershcovich-etal-2022-challenges"></d-cite> and that standards for ``good‚Äô‚Äô benchmarks are still evolving <d-cite key="reuel2024betterbench,liu-etal-2024-ecbd, blodgett-etal-2024-human"></d-cite>. Introducing a fixed reporting format might seem premature. But EvalCards are not rigid templates. They do not enforce benchmark choices but merely require clarity about what was tested and how. This transparency helps everyone interpret results accurately, compare across models, and build on prior evaluations rather than duplicating or misapplying them.</p> <h2 id="conclusion">Conclusion</h2> <p>Evaluation reporting is a critical but under-prioritized part of responsible NLP and AI. Non-standardized practices create hurdles for researchers, users, and regulators, fueling reproducibility, accessibility, and governance crises. Existing documentation efforts like Model Cards <d-cite key="mitchell2019model"></d-cite> or BenchmarkCards <d-cite key="sokol2024benchmarkcards"></d-cite> have aimed to improved transparency, but they do not put evaluation at the center. EvalCards aim to close this gap with a format that is easy to write, easy to understand, and hard to miss. By making evaluation details visible and consistent, they turn scattered disclosures into a foundation for cumulative research, informed adoption, and accountability. Looking ahead, we highlight three directions to strengthen and extend the EvalCard framework.</p> <p><strong>Increased Transparency</strong> EvalCards can help establish a unified pipeline that links evaluation and reporting into a single transparent process by reducing ambiguity across sources <d-cite key="biderman2024lessons"></d-cite>. In the longer term, EvalCards could link to Benchmark Cards <d-cite key="sokol2024benchmarkcards"></d-cite> for each mentioned benchmark, creating a connected reporting ecosystem where model, benchmark, and evaluation details are transparently linked to ensure that both the tests and the results behind model claims are easy to trace and verify.</p> <p><strong>Increased Adoption</strong> Widespread adoption will require making EvalCards easy to produce and use. Methods like automated extraction from technical reports <d-cite key="liu-etal-2024-automatic"></d-cite> or community contribution to scores <d-cite key="10855627"></d-cite> can help ease the burden on model developers, while usability testing can help refine design and language for diverse stakeholders <d-cite key="crisan2022interactive"></d-cite>. This will help ensure that EvalCards are not only technically sound but also popular.</p> <p><strong>Regulatory Integration</strong> EvalCards can play a key role in bridging technical evaluation and regulatory transparency. They can serve as a standardized reporting format for models within compliance processes, such as regulatory sandboxes under the EU AI Act <d-cite key="lanamaki2025expect"></d-cite> or US AI Risk Management Framework <d-cite key="ai2023artificial"></d-cite>, by offering a consistent way to report model performance and limitations. Partnerships with platforms like HuggingFace or emerging standards bodies could help maintain vetted benchmark sets that align with evolving priorities. Future work in technical governance <d-cite key="reuel2024open, reuel2024position"></d-cite> can explore how EvalCards can be incorporated into regulatory compliance workflows, e.g., by establishing minimum mandatory fields tied to emerging AI regulations.</p> <p>As model development accelerates, reporting practices must evolve with equal urgency. Evaluation should drive progress, not confusion‚Äîbut that is only possible when what models can and cannot do is made clear. EvalCards take a small but concrete step toward that goal, embedding transparency into the model release process itself. We hope this work sparks deeper reflection and concrete action toward standardizing how we evaluate and report on models.</p> <hr/>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[In the age of rapidly released LLMs, evaluation reporting is fragmented, inconsistent, and often misleading. We surveyed the landscape and found three critical crises‚Äîreproducibility, accessibility, and governance‚Äîthat Model Cards alone can't solve. Our solution? EvalCards-- lightweight, standardized evaluation summaries that are easy to write, easy to understand, and impossible to miss. EvalCards are designed to enhance transparency for both researchers and practitioners while providing a practical foundation to meet emerging governance requirements.]]></summary></entry><entry><title type="html">Introduction to Stochastic Interpolants</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/introduction-to-stochastic-interpolants/" rel="alternate" type="text/html" title="Introduction to Stochastic Interpolants"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/introduction-to-stochastic-interpolants</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/introduction-to-stochastic-interpolants/"><![CDATA[<h2 id="background">Background</h2> <p>In generative modeling frameworks such as flow matching <d-cite key="lipman2022flow"></d-cite> and score-based diffusion models <d-cite key="song2020score"></d-cite>, one constructs a smooth transition from a Gaussian to a data distribution. This transition is governed by a velocity field that operates on the samples by pushing them from the Gaussian to the target. To generate new data samples, we begin by drawing from the latent noise distribution and then use a learned velocity field to iteratively move towards regions where the data distribution has high probability mass. The result is a synthetic sample from the approximate data distribution. In such generative models, the velocity field is a parameterized model learned from data.</p> <p>The goal of the stochastic interpolant framework <d-cite key="albergo2023stochastic, albergo2022building"></d-cite> is more general. Here we aim to interpolate between any two densities. The stochastic interpolant framework provides a flexible family of possible interpolations. After selecting the form of the interpolation, we can train a corresponding velocity field that gradually transforms samples from one distribution into the other.</p> <p>In the context of generative modeling, this corresponds to transforming noise into data: one distribution represents the latent noise, and the other corresponds to the data distribution. However, the framework goes beyond just flow matching and score-based diffusion, making it applicable to tasks other than generating data from noise.</p> <h3 id="notation">Notation</h3> <p>Throughout the blog post, we use the following conventions:</p> <ul> <li>Boldface letters (e.g., \(\mathbf{x}\)) denote column vectors in \(\mathbb{R}^d\), whereas non-bold letters (e.g., $t$) denote scalar quantities in \(\mathbb{R}\)</li> <li>We write \(\partial_t := \tfrac{\partial}{\partial t}\) for the partial derivative with respect to time.</li> <li>We denote by \(\nabla_{\mathbf{x}} := (\partial_{\mathbf{x}_1}, \ldots, \partial_{\mathbf{x}_d})\) the gradient operator with respect to the spatial variables.</li> <li>Throughout the blog post we abbreviate \(\mathcal{N}(\mathbf{x};\boldsymbol{\mu}, \sigma^2)\) for the density at point \(\mathbf{x}\) of the \(d\)-dimensional normal distribution \(\mathcal{N}(\boldsymbol{\mu}, \sigma^2)\) with mean $\boldsymbol{\mu}$ and a covariance matrix with main diagonal entries of \(\sigma^2\) and else \(0\).</li> <li>We write \(\delta(\mathbf{x} - \boldsymbol{\mu})\) for the delta distribution in \(\mathbb{R}^d\) where all the mass is concentrated at \(\boldsymbol{\mu}\). Intuitively, the delta distribution can be understood as the limit \(\mathrm{lim}_{\sigma \rightarrow 0} \mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \sigma^2)\).</li> </ul> <h2 id="methods">Methods</h2> <p>We begin with a short three-step overview of how to build a stochastic interpolant model. Afterwards, we will prove why this approach works.</p> <h3 id="building-a-stochastic-interpolant">Building a stochastic interpolant</h3> <p>To construct a stochastic interpolant, we need access to samples drawn proportional to some densities \(p_0\) and \(p_1\) that we want to connect. Throughout this blog, we restrict ourselves to densities defined on the \(d\)-dimensional real space \(\mathbb{R}^d\).</p> <h4 id="1-define-intermediate-densities">1. Define intermediate densities</h4> <p>To define the intermediate densities \(p_t\) (for all \(t \in [0, 1]\)) between \(p_0\) and \(p_1\), we introduce two functions: an <em>interpolant</em> \(\,\mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1)\) and a <em>noise amplitude function</em> \(\gamma(t)\). A natural choice for the <em>interpolant</em> is to use a simple convex combination \(\mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) = (1 - t)\mathbf{x}_0 + t\mathbf{x}_1,\) of the two samples that creates a line segment between them. A reasonable choice for the <em>noise amplitude function</em> is to have the noise contribution peak midway through the interpolation and vanish at the boundaries, e.g. \(\gamma(t) = t(1 - t).\)</p> <p>This means our intermediates can now be written algebraically as</p> \[p_t(\mathbf{x}) = \int p_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z}) p_0(\mathbf{x}_0) p_1(\mathbf{x}_1) p_\mathcal{N}(\mathbf{z}) \mathrm{d}\mathbf{x}_0 \mathrm{d}\mathbf{x}_1 \mathrm{d}\mathbf{z}\] <p>with \(p_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z}) = \delta(\mathbf{x} - \mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \gamma(t)\mathbf{z})\) and \(p_\mathcal{N}(\mathbf{z}) = \mathcal{N}(\mathbf{z}; \mathbf{0}, 1).\) Although this formulation of \(p_t\) might look intimidating at first, drawing samples from it is straightforward. We can sample \(\mathbf{x}_t \sim p_t\) by drawing \(\mathbf{x}_0 \sim p_0\), \(\mathbf{x}_1 \sim p_1\), and \(\mathbf{z} \sim \mathcal{N}(\mathbf{0}, 1)\), then setting \(\mathbf{x}_t = \mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \gamma(t)\mathbf{z}.\)</p> <p>When choosing the <em>interpolant</em> and the <em>noise amplitude function</em>, we must ensure that the boundary conditions hold:</p> \[p_0(\mathbf{x}) = \int p_0(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z}) p_0(\mathbf{x}_0) p_1(\mathbf{x}_1) p_\mathcal{N}(\mathbf{z}) \mathrm{d}\mathbf{x}_0 \mathrm{d}\mathbf{x}_1 \mathrm{d}\mathbf{z},\] \[p_1(\mathbf{x}) = \int p_1(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z}) p_0(\mathbf{x}_0) p_1(\mathbf{x}_1) p_\mathcal{N}(\mathbf{z}) \mathrm{d}\mathbf{x}_0 \mathrm{d}\mathbf{x}_1 \mathrm{d}\mathbf{z}.\] <p>A sufficient condition for this is that:</p> <ul> <li>the <em>interpolant</em> satisfies \(\mathbf{I}(0, \mathbf{x}_0, \mathbf{x}_1) = \mathbf{x}_0,\, \mathbf{I}(1, \mathbf{x}_0, \mathbf{x}_1) = \mathbf{x}_1\)</li> <li>and that the <em>noise amplitude function</em> fulfills \(\gamma(0) = 0 = \gamma(1)\), such that the noise vanishes at the start and end of the interpolantion.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/pt-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/pt-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/pt-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/pt.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Interpolation between two 1D Gaussian Mixture Models. Each $p_t$, defined by choosing an interpolant and a noise function, has an associated velocity field $\mathbf{b}_t$ that we aim to learn.</figcaption> </figure> <h4 id="2-training">2. Training</h4> <p>We learn a parametric model \(\mathbf{b}_{\boldsymbol{\theta}}\) of the velocity field for \(p_t\), which we defined in the first step, by solving the following optimization problem:</p> \[\min_{\boldsymbol{\theta}} \underset{\substack{t \sim \mathcal{U}[0, 1] \\ \mathbf{x}_0 \sim p_0 \\ \mathbf{x}_1 \sim p_1 \\ \mathbf{z} \sim \mathcal{N}(\mathbf{0}, 1) \\\mathbf{x}_t = \mathbf{I}(t,\mathbf{x}_0, \mathbf{x}_1) + \gamma(t)\mathbf{z}}}{\mathbb{E}} \big|\!\big| \underbrace{\partial_t \mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1)}_{\text{ e.g. } \mathbf{x}_1 -\mathbf{x}_0} + \underbrace{\partial_t\gamma(t)}_{\text{and } 1 - 2t \,}\mathbf{z} - \mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}_t, t)\big|\!\big|^2.\] <p>As in other areas of deep learning, this can be achieved using <strong>stochastic gradient descent (SGD)</strong>. We follow the gradient with respect to the parameters \(\boldsymbol{\theta}\) based on a Monte Carlo estimate of the objective function. This is done by selecting a mini-batch size and replacing the full expectation with an empirical average over the batch. In practice, the training loss can be implemented as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">b_model</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">
    x0, x1: of shape (batch_size, d)
    t:      of shape (batch_size, 1)
  </span><span class="sh">"""</span>
  <span class="n">z</span> <span class="o">=</span> <span class="nf">randn_like</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
  <span class="n">xt</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="nf">gamma</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">z</span>
  <span class="n">I_dt</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x0</span>
  <span class="n">gamma_dt</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">t</span>
  <span class="n">target</span> <span class="o">=</span> <span class="n">I_dt</span> <span class="o">+</span> <span class="n">gamma_dt</span> <span class="o">*</span> <span class="n">z</span>
  <span class="nf">return </span><span class="p">((</span><span class="n">target</span> <span class="o">-</span> <span class="nf">b_model</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div> <p>The loss is fully differentiable (as long as the parametric model itself is differentiable). Therefore, we can use any automatic differentiation library‚Äîsuch as PyTorch, JAX, or TensorFlow‚Äîto compute gradients with respect to the model parameters.</p> <h4 id="3-inference">3. Inference</h4> <p>To transform a sample \(\mathbf{x}_0\) from \(p_0\) into a sample from \(p_1\), we follow the estimated velocity field by simulating the <strong>ordinary differential equation (ODE</strong>):</p> \[\frac{\mathrm{d} \mathbf{x}}{\mathrm{d} t} = \mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}, t)\] <p>from \(t = 0\) to \(t = 1\).<br/> For the simulation, we can use <strong>Euler‚Äôs method</strong> and iterate</p> \[\mathbf{x}_{t + \Delta t} \leftarrow \mathbf{x}_{t} + \mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}_t, t)\, \Delta t, \qquad t \leftarrow t + \Delta t\] <p>for \(S &gt; 0\) steps with step size \(\Delta t = 1/S\).</p> <p>We can also simulate the ODE in reverse time. Moreover, we are not restricted to starting or ending at the boundaries: we can choose any two time points \(t_\text{start}\) and \(t_\text{end} \in [0, 1]\) and transform samples from \(p_{t_\text{start}}\) into samples from \(p_{t_\text{end}}\).</p> <p>In code we can implement this as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">euler</span><span class="p">(</span><span class="n">b_model</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">t_start</span><span class="p">,</span> <span class="n">t_end</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">x_start</span>
  <span class="n">timesteps</span> <span class="o">=</span> <span class="nf">linspace</span><span class="p">(</span><span class="n">t_start</span><span class="p">,</span> <span class="n">t_end</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>
  <span class="n">dt</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="n">S</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">timesteps</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="nf">b_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">dt</span>
  <span class="n">x_end</span> <span class="o">=</span> <span class="n">x</span>
  <span class="k">return</span> <span class="n">x_end</span>
</code></pre></div></div> <p>Score-based diffusion models also allow for continuously transforming samples using a <strong>stochastic differential equation (SDE)</strong>. Although the stochastic interpolant framework includes this option, in this blog post we focus on performing inference via the ODE formulation.</p> <h4 id="example">Example</h4> <p>In the following animation, we can see two stochastic interpolant options between two 1D bimodal Gaussian mixture models, where each Gaussian has a standard deviation of \(0.2\).</p> <div class="m-page"> <iframe src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-introduction-to-stochastic-interpolants/pt_2combo.html" style="min-width:650px; min-height:750px;" frameborder="0" scrolling="no"></iframe> </div> <p>Each of the two plots shows three key elements:</p> <ol> <li>The <strong>density values</strong> of \(p_t\) across space‚Äìtime, where yellow regions indicate areas of high density and black regions correspond to density values close to zero.</li> <li><strong>Cyan arrows</strong> representing the velocity field \((\Delta t, \Delta t \mathbf{b}_t(\mathbf{x}))\).</li> <li><strong>White lines</strong> showing trajectories \(\mathbf{x}(t)\) through space‚Äìtime generated by integrating along this velocity field.</li> </ol> <p>Across the animation frames, the strength of the <em>noise amplitude function</em> \(\gamma(t)\) changes, allowing us to see how this affects both the vector field and the resulting trajectories. In our 1D example we can see that a small noise amplitude can help to make the trajectories straighter. Trajectories with gentler curvature are easier to simulate during inference. At each step, Euler‚Äôs method assumes the path is locally straight, so when the trajectory bends sharply, this approximation breaks down. To capture those sharper turns accurately, we need to increase the number of inference steps \(S\). Therefore, we should design our interpolant to produce trajectories that are as straight as possible during inference.</p> <hr/> <p>But why is all of this possible? Why can we generate a sample from \(p_{t_\text{end}}\) at any \(t_\text{end} \in [0, 1]\) as long as we start with a sample \(\mathbf{x}_{t_\text{start}}\) from \(p_{t_\text{start}}\) and follow the velocity field \(\mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}, t)\) until \(t_\text{end}\)?</p> <p>To understand why this works, we first need an algebraic formulation of the <strong>true velocity field</strong> \(\mathbf{b}_t(\mathbf{x})\) that generates \(p_t\). We will see that this leads to an expression for \(\mathbf{b}_t(\mathbf{x})\) that is generally intractable in the interesting cases. To handle this intractability, we therefore train a <strong>surrogate model</strong> \(\mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}, t)\) that approximates \(\mathbf{b}_t\) in feasible time.</p> <h3 id="deriving-an-expression-for-the-velocity-field">Deriving an Expression for the Velocity Field</h3> <p>A velocity field \(\mathbf{b}_t(\mathbf{x})\) that generates \(p_t\) must obey the <strong>continuity equation</strong>:</p> \[\partial_t p_t(\mathbf{x}) = -\nabla_{\mathbf{x}} \cdot \big(p_t(\mathbf{x})\,\mathbf{b}_t(\mathbf{x})\big).\] <p>The continuity equation links a time-dependent density to its corresponding vector field.</p> <ul> <li>On the left-hand side, it expresses how the density \(p_t(\mathbf{x})\) changes over time ‚Äî the <em>rate of change</em> of probability mass at point \(\mathbf{x}\).</li> <li>On the right-hand side, it measures how much of the flux represented by the weighted velocity field \(p_t(\mathbf{x})\,\mathbf{b}_t(\mathbf{x})\) diverges/flows from that point.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/divergence-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/divergence-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/divergence-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/divergence.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">This visualization illustrates how the divergence operator $\nabla_{\mathbf{x}} \cdot $ quantifies local expansion or compression of a flux field. Each panel shows a different flux $p_t(\mathbf{x})\, \mathbf{b}_t(\mathbf{x})$ in two dimensions. <strong>Left:</strong> A positive divergence indicates a source region, where flux flows outward. For such a flux, the entropy of $p_t$ increases over time. <strong>Middle:</strong> Zero divergence represents a rotational or incompressible flux, where local inflow and outflow balance. Here, $p_t$ remains constant in time. <strong>Right:</strong> A negative divergence corresponds to a sink region, where flux converges inward. In this case, the entropy of $p_t$ decreases over time.</figcaption> </figure> <p>Intuitively, if a large amount of probability mass from neighboring regions flows into \(\mathbf{x}\) with high velocity, the local density increases. Conversely, if more mass flows out of \(\mathbf{x}\) than flows in, the density there decreases. In this way, the continuity equation ensures that total probability is conserved as the density moves in space and evolves over time.</p> <p>To find a formula for \(\mathbf{b}_t\), we start from the left side of the continuity equation and transform it into the right-hand side. Plugging in the definition of \(p_t\) and swapping \(\int\) with \(\partial_t\) gives:</p> \[\partial_t p_t(\mathbf{x}) = \int \partial_t p_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\, p_0(\mathbf{x}_0)\, p_1(\mathbf{x}_1)\, p_\mathcal{N}(\mathbf{z})\, \mathrm{d}\mathbf{x}_0\, \mathrm{d}\mathbf{x}_1\, \mathrm{d}\mathbf{z}.\] <p>Inside the integral, we find the partial rate of change \(\partial_t p_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\) given \(\mathbf{x}_0\), \(\mathbf{x}_1\), and \(\mathbf{z}\). This conditional distribution is a delta peak whose mass at time \(t\) is concentrated at \(\mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \gamma(t)\mathbf{z}\), defining a path through time.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/partial_just_one-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/partial_just_one-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/partial_just_one-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/partial_just_one.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The velocity along this path is simply \(\partial_t \mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \partial_t \gamma(t)\mathbf{z}\). By following this velocity over time, we move along the path. Thus, for the conditional distribution, the continuity equation holds:</p> \[\partial_t p_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z}) = \partial_t \delta(\mathbf{x} - (\mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \gamma(t)\mathbf{z})) \qquad \qquad \qquad \qquad \qquad \qquad\] \[\qquad \qquad \qquad \quad = -\partial_t (\mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \gamma(t)\mathbf{z}) \cdot\nabla_\mathbf{x} \delta(\mathbf{x} - (\mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \gamma(t)\mathbf{z}))\] \[\qquad \qquad \qquad \qquad = - \nabla_\mathbf{x} \cdot \big(\delta(\mathbf{x} - (\mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \gamma(t)\mathbf{z})) \partial_t (\mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \gamma(t)\mathbf{z}) \big)\] \[=- \nabla_\mathbf{x} \cdot \big(p_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\,\mathbf{b}_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\big),\] <p>with</p> \[\mathbf{b}_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z}) = \partial_t \mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \partial_t \gamma(t)\mathbf{z}.\] <p>We can now use this equality to obtain</p> <div class="l-body-outset"> <p>$$\partial_t p_t(\mathbf{x}) = \int \overbrace{- \nabla_\mathbf{x} \cdot \big(p_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\,\mathbf{b}_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\big)}^{\partial_t p_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z})} p_0(\mathbf{x}_0)\, p_1(\mathbf{x}_1)\, p_\mathcal{N}(\mathbf{z})\, \mathrm{d}\mathbf{x}_0\, \mathrm{d}\mathbf{x}_1\, \mathrm{d}\mathbf{z}.$$ </p> </div> <p>Pulling \(- \nabla_\mathbf{x} \cdot\) outside the integral yields:</p> <div class="l-body-outset"> <p>$$\partial_t p_t(\mathbf{x}) = - \nabla_\mathbf{x} \cdot \bigg(\int p_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\,\mathbf{b}_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\, p_0(\mathbf{x}_0)\, p_1(\mathbf{x}_1)\, p_\mathcal{N}(\mathbf{z})\, \mathrm{d}\mathbf{x}_0\, \mathrm{d}\mathbf{x}_1\, \mathrm{d}\mathbf{z}\bigg).$$ </p> </div> <p>Finally, we multiply inside the brackets by \(1 = \textcolor{blue}{p_t(\mathbf{x})} / \textcolor{blue}{p_t(\mathbf{x})}\):</p> <div class="l-body-outset"> <p>$$ \partial_t p_t(\mathbf{x}) = - \nabla_\mathbf{x} \cdot \bigg(\textcolor{blue}{p_t(\mathbf{x})} \underbrace{\int \mathbf{b}_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z}) \frac{p_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\, p_0(\mathbf{x}_0)\, p_1(\mathbf{x}_1)\, p_\mathcal{N}(\mathbf{z})}{\textcolor{blue}{p_t(\mathbf{x})}}\, \mathrm{d}\mathbf{x}_0\, \mathrm{d}\mathbf{x}_1\, \mathrm{d}\mathbf{z}}_{\mathbf{b}_t(\mathbf{x})}\bigg).$$</p> </div> <p>This gives us an algebraic formulation of the velocity field that generates \(p_t(\mathbf{x})\):</p> \[\mathbf{b}_t(\mathbf{x}) = \int \mathbf{b}_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z}) \overbrace{\frac{p_t(\mathbf{x}\,\vert\, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\, p_0(\mathbf{x}_0)\, p_1(\mathbf{x}_1)\, p_\mathcal{N}(\mathbf{z})}{p_t(\mathbf{x})}}^{p(\mathbf{x}_0, \mathbf{x}_1, \mathbf{z}\,|\, \mathbf{x})}\, \mathrm{d}\mathbf{x}_0\, \mathrm{d}\mathbf{x}_1\, \mathrm{d}\mathbf{z}.\] <p>Despite its intractable and complex form, this integral provides a way to derive a loss function for training the surrogate model \(\mathbf{b}_{\boldsymbol{\theta}}\). This is the focus of the next section.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/partial-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/partial-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/partial-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-introduction-to-stochastic-interpolants/partial.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">$p_t(\mathbf{x})$ is the mean of all partial densities $p_t(\mathbf{x}\,|\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})$ with respect to $p(\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})$, and the velocity field $\mathbf{b}_t(\mathbf{x})$ is the mean of all partial velocity fields $\mathbf{b}_t(\mathbf{x}\,|\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})$ over $p_t(\mathbf{x}_0, \mathbf{x}_1, \mathbf{z} \,|\, \mathbf{x})$.</figcaption> </figure> <h3 id="deriving-the-loss-function">Deriving the Loss Function</h3> <p>We now aim to design an objective function for which \(\mathbf{b}_t(\mathbf{x})\) is the optimal solution. Whether we can actually reach this optimum depends, of course, on the flexibility of the chosen parametric model.</p> <p>Let us start by writing a weighted least-squares regression loss that, by construction, achieves its minimum at \(\mathbf{b}_t(\mathbf{x})\):</p> \[\min_{\boldsymbol{\theta}} \int \|\mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}, t) - \mathbf{b}_t(\mathbf{x})\|^2\, p_t(\mathbf{x})\, \mathrm{d}\mathbf{x}\, \mathrm{d}t.\] <p>However, this formulation is not suitable for SGD. To perform SGD, we need all integrals, including the marginalization integrals in the definition of $\mathbf{b}_t$, to be outside the \(L^2\)-norm. We therefore begin by decomposing the squared norm:</p> <div class="l-body-outset"> <p>$$ \min_{\boldsymbol{\theta}} \int \|\mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}, t)\|^2\, p_t(\mathbf{x})\, \mathrm{d}\mathbf{x}\, \mathrm{d}t - 2 \int \mathbf{b}_t(\mathbf{x})^\top \mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}, t)\, p_t(\mathbf{x})\, \mathrm{d}\mathbf{x}\, \mathrm{d}t + \underbrace{\int \|\mathbf{b}_t(\mathbf{x})\|^2\, p_t(\mathbf{x})\, \mathrm{d}\mathbf{x}\, \mathrm{d}t}_{c_1}.$$ </p> </div> <p>The last term, \(c_1\), is independent of \(\boldsymbol{\theta}\) and therefore constant. Next, we plug in the definitions of \(p_t(\mathbf{x})\) and \(\mathbf{b}_t(\mathbf{x})\):</p> <div class="l-body-outset"> <p>$$ \min_{\boldsymbol{\theta}} \int \|\mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}, t)\|^2 \overbrace{\int p_t(\mathbf{x}\,|\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\, p_0(\mathbf{x}_0)\, p_1(\mathbf{x}_1)\, p_\mathcal{N}(\mathbf{z})\, \mathrm{d}\mathbf{x}_0\, \mathrm{d}\mathbf{x}_1\, \mathrm{d}\mathbf{z}}^{p_t(\mathbf{x})} \, \mathrm{d}\mathbf{x}\, \mathrm{d}t$$ $$ - 2 \int \mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}, t)^\top \underbrace{\int \partial_t \mathbf{b}_t(\mathbf{x}\,\vert\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z}) \frac{p_t(\mathbf{x}\,|\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\, p_0(\mathbf{x}_0)\, p_1(\mathbf{x}_1)\, p_\mathcal{N}(\mathbf{z})}{p_t(\mathbf{x})}\, \mathrm{d}\mathbf{x}_0\, \mathrm{d}\mathbf{x}_1\, \mathrm{d}\mathbf{z}}_{\mathbf{b}_t(\mathbf{x})} \, p_t(\mathbf{x})\, \mathrm{d}\mathbf{x}\, \mathrm{d}t + c_1. $$</p> </div> <p>We now move all integrals to the outside, cancel the factor \(p_t(\mathbf{x}) / p_t(\mathbf{x})\) in the second term, and add a constant for quadratic completion:</p> <div class="l-body-outset"> <p>$$ \min_{\boldsymbol{\theta}} \int \textcolor{blue}{\|\mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}, t)\|^2}\, p_t(\mathbf{x}\,|\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\, p_0(\mathbf{x}_0)\, p_1(\mathbf{x}_1)\, p_\mathcal{N}(\mathbf{z})\, \mathrm{d}\mathbf{x}_0\, \mathrm{d}\mathbf{x}_1\, \mathrm{d}\mathbf{z}\, \mathrm{d}\mathbf{x}\, \mathrm{d}t $$ $$ \textcolor{blue}{- 2} \int \textcolor{blue}{\mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}, t)^\top \partial_t \mathbf{b}_t(\mathbf{x}\,\vert\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})}\, p_t(\mathbf{x}\,|\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\, p_0(\mathbf{x}_0)\, p_1(\mathbf{x}_1)\, p_\mathcal{N}(\mathbf{z})\, \mathrm{d}\mathbf{x}_0\, \mathrm{d}\mathbf{x}_1\, \mathrm{d}\mathbf{z}\, \mathrm{d}\mathbf{x}\, \mathrm{d}t $$ $$ + \underbrace{\int \textcolor{blue}{\|\partial_t \mathbf{b}_t(\mathbf{x}\,\vert\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\|^2}\, p_t(\mathbf{x}\,|\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\, p_0(\mathbf{x}_0)\, p_1(\mathbf{x}_1)\, p_\mathcal{N}(\mathbf{z})\, \mathrm{d}\mathbf{x}_0\, \mathrm{d}\mathbf{x}_1\, \mathrm{d}\mathbf{z}\, \mathrm{d}\mathbf{x}\, \mathrm{d}t}_{c_2} + c_1 - c_2.$$ </p> </div> <p>Combining the terms highlighted in blue, we obtain:</p> <div class="l-body-outset"> <p>$$ \min_{\boldsymbol{\theta}} \int \textcolor{blue}{\|\partial_t \mathbf{b}_t(\mathbf{x}\,\vert\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z}) - \mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}, t)\|^2}\, p_t(\mathbf{x}\,|\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\, p_0(\mathbf{x}_0)\, p_1(\mathbf{x}_1)\, p_\mathcal{N}(\mathbf{z})\, \mathrm{d}\mathbf{x}_0\, \mathrm{d}\mathbf{x}_1\, \mathrm{d}\mathbf{z}\, \mathrm{d}\mathbf{x}\, \mathrm{d}t + c_1 - c_2.$$ </p> </div> <p>Finally, we can express this as an expectation and substitute the definition of the partial velocity field \(\partial_t \mathbf{b}_t(\mathbf{x}\,\vert\,\mathbf{x}_0, \mathbf{x}_1, \mathbf{z})\):</p> \[\min_{\boldsymbol{\theta}} \underset{\substack{ t \sim \mathcal{U}[0, 1] \\ \mathbf{x}_0 \sim p_0,\ \mathbf{x}_1 \sim p_1 \\ \mathbf{z} \sim \mathcal{N}(\mathbf{0}, 1) \\ \mathbf{x}_t = \mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \gamma(t)\mathbf{z} }}{\mathbb{E}} \big\|\partial_t \mathbf{I}(t, \mathbf{x}_0, \mathbf{x}_1) + \partial_t \gamma(t)\mathbf{z} - \mathbf{b}_{\boldsymbol{\theta}}(\mathbf{x}_t, t)\big\|^2 + c_1 - c_2.\] <p>Since we are only interested in the argument \(\boldsymbol{\theta}\) that minimizes the objective, rather than the actual loss value, we can safely ignore the constants \(+ c_1 - c_2\) during optimization.</p> <h2 id="connection-to-other-generative-models">Connection to Other Generative Models</h2> <p>Other generative modeling frameworks based on either flows or score-based diffusion can be described within the stochastic interpolant framework. Most generative models generate data samples‚Äîsuch as images‚Äîstarting from a Gaussian noise distribution. When either \(p_0\) or \(p_1\) is Gaussian, there are multiple ways to define the schedule using different choices of <em>interpolants</em> and <em>noise amplitude functions</em>.</p> <table> <thead> <tr> <th>Methods</th> <th style="text-align: center">\(\mathbf{I}(t, \mathbf{x}_0, \mathbf{x_1})\)</th> <th style="text-align: center">\(\gamma(t)\)</th> <th>boundary conditions</th> </tr> </thead> <tbody> <tr> <td>Karras EDM <d-cite key="karras2022elucidating"></d-cite></td> <td style="text-align: center">\(\mathbf{x}_0\)</td> <td style="text-align: center">\(t \cdot t_\text{max}\)</td> <td>\(p_0 = p_\text{data}, p_1 = \mathcal{N}(\mathbf{0}, t_\text{max}^2)\)</td> </tr> <tr> <td>¬†</td> <td style="text-align: center">\(\mathbf{x}_0 + t \mathbf{x}_1\)</td> <td style="text-align: center">\(0\)</td> <td>\(p_0 = p_\text{data}, p_1 = \mathcal{N}(\mathbf{0}, t_\text{max}^2)\)</td> </tr> <tr> <td>Song VE <d-cite key="song2020score"></d-cite></td> <td style="text-align: center">\(\mathbf{x}_0\)</td> <td style="text-align: center">\(\sqrt{t \cdot t_\text{max}}\)</td> <td>\(p_0 = p_\text{data}, p_1 = \mathcal{N}(\mathbf{0}, t_\text{max})\)</td> </tr> <tr> <td>¬†</td> <td style="text-align: center">\(\mathbf{x}_0 + \sqrt{t} \, \mathbf{x_1}\)</td> <td style="text-align: center">$0$</td> <td>\(p_0 = p_\text{data}, p_1 = \mathcal{N}(\mathbf{0}, t_\text{max})\)</td> </tr> <tr> <td>Flow Matching <d-cite key="lipman2022flow"></d-cite></td> <td style="text-align: center">\(t \mathbf{x}_1\)</td> <td style="text-align: center">\((1-t)\)</td> <td>\(p_0 = \mathcal{N}(\mathbf{0}, 1),p_1 = p_\text{data}\)</td> </tr> <tr> <td>¬†</td> <td style="text-align: center">\((1-t)\mathbf{x}_0 + t \mathbf{x}_1\)</td> <td style="text-align: center">$0$</td> <td>\(p_0 = \mathcal{N}(\mathbf{0}, 1),p_1 = p_\text{data}\)</td> </tr> <tr> <td>1-Rectified Flow <d-cite key="liu2022flow"></d-cite></td> <td style="text-align: center">\((1-t)\mathbf{x}_0 + t\mathbf{x}_1\)</td> <td style="text-align: center">\(0\)</td> <td>any \(p_0, p_1\)</td> </tr> </tbody> </table> <p>In the EDM schedule proposed by Karras et al., \(t_\text{max}\) is chosen such that</p> \[\mathbf{x}_1 = \mathbf{x}_0 + t_\text{max}\mathbf{z} \sim \mathcal{N}(\mathbf{0}, t_\text{max}^2), \qquad \mathbf{z} \sim \mathcal{N}(\mathbf{0}, 1).\] <p>In other words, \(t_\text{max}\) is set high enough that the signal from \(\mathbf{x}_0\) becomes negligible compared to the strong noise contribution. The same principle applies to the variance exploding (VE) schedule introduced by Song et al.</p> <p>The stochastic interpolant framework thus provides a unifying and highly flexible perspective for describing and designing flow-based and score-based diffusion models‚Äîand even for extending beyond them.</p> <h2 id="conclusion">Conclusion</h2> <p>In this post, we explored the stochastic interpolant framework. By viewing the transformation between two distributions as a stochastic interpolation, we can naturally recover well-known models like Flow Matching and score-based Diffusion Models as special cases. This perspective not only clarifies the shared principles behind these methods but also offers the possibility to new designs that interpolate between any pair of distributions. Future research may build on this framework to explore richer interpolation schemes, alternative noise functions, or applications beyond generative models‚Äîwhere learning smooth, probabilistic transformations between arbitrary densities are central.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Prominent generative modeling frameworks such as Flow Matching and score-based Diffusion Models establish a smooth transformation between a Gaussian distribution and a data distribution. In this blog post, we provide an introduction to the more general framework of Stochastic Interpolants, which allows one to flexibly interpolate between any two distributions and learn a velocity field to transform samples from one into samples of the other. No prior knowledge of generative models is required for this introduction.]]></summary></entry><entry><title type="html">JustRL: Scaling a 1.5B LLM with a Simple RL Recipe</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/justrl/" rel="alternate" type="text/html" title="JustRL: Scaling a 1.5B LLM with a Simple RL Recipe"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/justrl</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/justrl/"><![CDATA[<blockquote> <p>Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.</p> <p>‚Äî Antoine de Saint-Exup√©ry, Airman‚Äôs Odyssey</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig1_aime24_curves_added-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig1_aime24_curves_added-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig1_aime24_curves_added-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig1_aime24_curves_added.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: (a) The AIME24 (avg@32) performance curve for scaling from DeepSeek-R1-Distill-Qwen-1.5B into JustRL-DeepSeek-1.5B, from 28% to 58% over 4,000 steps; (b) from OpenMath-Nemotron-1.5B into our 1.5B reasoning SOTA model JustRL-Nemotron-1.5B, showing its training journey to the final 70+% score over 3,000 steps. </div> <h2 id="introduction">Introduction</h2> <p>Recent advances in Large Language Models (LLMs), such as OpenAI‚Äôs o1<d-cite key="jaech2024openai"></d-cite> and DeepSeek-R1<d-cite key="guo2025deepseek"></d-cite>, have demonstrated the remarkable effectiveness of large-scale Reinforcement Learning with Verifiable Rewards (RLVR) for challenging reasoning tasks in mathematics and coding. For smaller models in the 1-10B parameter range, researchers have increasingly turned to reinforcement learning to push performance boundaries beyond what distillation alone can achieve. Over the past year, we‚Äôve seen a proliferation of methods attempting to stabilize and improve RL training for small language models (SLMs): multi-stage training pipelines, dynamic hyperparameter schedules, adaptive temperature controls, response length penalties, and various forms of data curation and filtering<d-cite key="li2025questa,min2024imitate,deepscaler2025,liu2025prorl,hu2025prorlv2,hu2025brorl"></d-cite>.</p> <p>This proliferation of techniques raises an important question: <strong>Is this complexity necessary?</strong> The accumulated ‚Äúbest practices‚Äù may be fighting each other rather than the fundamental challenges of RL<d-cite key="liu2025part"></d-cite>. In this blog post, we explore <strong>whether stable, competitive training can be achieved with a simpler approach.</strong> We apply a minimal setup to two popular 1.5B reasoning models, using single-stage training with fixed hyperparameters derived from common practice. The results match or exceed more complex approaches while using 2√ó less compute. Importantly, we achieve this without the multi-stage pipelines or dynamic schedules, suggesting that simpler approaches may be sufficient when applied at adequate scale. Besides, the training process itself proves stable: smooth, monotonic improvement over 4,000+ steps without the collapses or oscillations often cited as motivation for complex interventions.</p> <p>Our goal is not to argue against all techniques or claim we‚Äôve found the optimal approach. Rather, we provide evidence that simpler baselines deserve more attention than they‚Äôve received. We offer a simple practice with a minimum set of tricks that can enhance the performance of models that are approaching their distillation limits. The field may benefit from establishing what‚Äôs fundamentally sufficient before layering on additional complexity. By open-sourcing our models and evaluation scripts, we hope to provide a reliable foundation that others can build upon, whether for practical deployment or as a baseline for developing and validating new techniques.</p> <h2 id="the-landscape-rl-for-small-reasoning-models">The Landscape: RL for Small Reasoning Models</h2> <p>Since DeepSeek-R1‚Äôs release in early 2025, the community has rapidly advanced RL for small language models in mathematical reasoning. The past year has seen a flourishing of approaches, each introducing techniques to stabilize training and push performance boundaries. These works fall into three main families based on their foundation models: DeepSeek-R1-Distill-Qwen-1.5B, OpenMath-Nemotron-1.5B, and Qwen3-1.7B, all starting from distilled bases.</p> <p>The evolution reveals a clear trend toward increasing sophistication. Early works like STILL<d-cite key="min2024imitate"></d-cite> explored hyperparameter tuning and reference model resets. Subsequent approaches introduced multi-stage training with progressive context lengthening<d-cite key="deepscaler2025,song2025fastcurl"></d-cite>, alternating between CoT compression and extension across five stages with varying data and batch sizes<d-cite key="song2025fastcurl"></d-cite>, or dividing training into eight stages with scheduled length penalties<d-cite key="liu2025prorl"></d-cite>. Later works incorporated hundreds of rollouts per example<d-cite key="hu2025brorl"></d-cite>, question augmentation with partial solutions<d-cite key="li2025questa"></d-cite>, dynamic dataset filtering<d-cite key="Polaris2025"></d-cite>, and test-time context extrapolation<d-cite key="Polaris2025,setlur2025e3"></d-cite>. A summary of RL techniques for various SLMs is shown in the following table.</p> <div class="l-page"> <table> <thead> <tr> <th><strong>Model</strong></th> <th><strong>Backbone</strong></th> <th><strong>Entropy Control</strong></th> <th><strong>Tune Hyperparameters</strong></th> <th><strong>Tune Training Prompt</strong></th> <th><strong>Reset KL Reference Model</strong></th> <th><strong>Length Control</strong></th> <th><strong>Adaptive Temperature</strong></th> <th><strong>Rollout Rescue</strong></th> <th><strong>Dynamic Sampling</strong></th> <th><strong>Split Training Stages</strong></th> <th><strong>Date</strong></th> </tr> </thead> <tbody> <tr> <td>STILL-3-1.5B-Preview<d-cite key="min2024imitate"></d-cite></td> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>Jan, 2025</td> </tr> <tr> <td>DeepScaleR-1.5B-Preview<d-cite key="deepscaler2025"></d-cite></td> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>Feb, 2025</td> </tr> <tr> <td>FastCuRL-1.5B-V3<d-cite key="song2025fastcurl"></d-cite></td> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>Mar, 2025</td> </tr> <tr> <td>ProRL-Nemotron-Qwen-1.5B-v1<d-cite key="liu2025prorl"></d-cite></td> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>May, 2025</td> </tr> <tr> <td>e3-1.7B<d-cite key="setlur2025e3"></d-cite></td> <td>Qwen3-1.7B</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>Jun, 2025</td> </tr> <tr> <td>Polaris-1.7B-Preview<d-cite key="Polaris2025"></d-cite></td> <td>Qwen3-1.7B</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>Jul, 2025</td> </tr> <tr> <td>Archer-Math-1.5B</td> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚ùå</td> <td>Jul, 2025</td> </tr> <tr> <td>ProRL-Nemotron-Qwen-1.5B-v2<d-cite key="hu2025prorlv2"></d-cite></td> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>Aug, 2025</td> </tr> <tr> <td>QuestA-Nemotron-1.5B<d-cite key="li2025questa"></d-cite></td> <td>OpenMath-Nemotron-1.5B</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>Sept, 2025</td> </tr> <tr> <td>BroRL<d-cite key="hu2025brorl"></d-cite></td> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚úÖ</td> <td>‚úÖ</td> <td>Oct, 2025</td> </tr> <tr> <td><strong>JustRL-DeepSeek-1.5B (Ours)</strong></td> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>Dec, 2025</td> </tr> <tr> <td><strong>JustRL-Nemotron-1.5B (Ours)</strong></td> <td>OpenMath-Nemotron-1.5B</td> <td>‚úÖ</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>‚ùå</td> <td>Dec, 2025</td> </tr> </tbody> </table> </div> <p>The pattern is striking: nearly every work employs multiple techniques from a growing toolkit‚Äîmulti-stage training, adaptive hyperparameters, length penalties, dynamic sampling, and various stabilization mechanisms. While these methods achieve strong results, each represents a different combination of design choices, making it difficult to isolate which elements truly matter. The engineering complexity also raises a practical question: Is there a simpler path that still achieves competitive performance?</p> <h2 id="justrl-simplicity-at-scale">JustRL: Simplicity at Scale</h2> <p>Our approach is deliberately simple. We constrain ourselves to the fundamentals of RL, avoiding the multi-stage pipelines, dynamic schedules, and specialized techniques that have become common in recent work. The goal is to establish what‚Äôs sufficient before adding complexity.</p> <h3 id="training-setup-what-we-use-and-dont-use">Training Setup: What We Use (and Don‚Äôt Use)</h3> <p><strong>Core algorithm:</strong> We use standard GRPO with binary outcome rewards‚Äînothing more. The reward signal comes from a lightweight rule-based verifier from DAPO<d-cite key="yu2025dapo"></d-cite>, without symbolic math libraries like SymPy that could add computational overhead.</p> <p><strong>What we keep simple:</strong></p> <ul> <li><strong>Single-stage training:</strong> No progressive context lengthening, no curriculum switching, no stage transitions. We train continuously from start to finish.</li> <li><strong>Fixed hyperparameters:</strong> No adaptive temperature scheduling, no dynamic batch size adjustments, no mid-training reference model resets.</li> <li><strong>Standard data:</strong> We train on DAPO-Math-17k<d-cite key="yu2025dapo"></d-cite> without offline difficulty filtering or online dynamic sampling strategies.</li> <li><strong>Basic prompting:</strong> A simple suffix prompt without tuning: ‚ÄúPlease reason step by step, and put your final answer within \boxed{}.‚Äù</li> <li><strong>Length control:</strong> We simply cap the maximum context length at 16K tokens, rather than using explicit length penalty terms.</li> </ul> <p><strong>The one technique we do use:</strong> We employ ‚Äúclip higher‚Äù, a well-established practice for stability in long-horizon RL training. This is our concession to practical stability, and we view it as part of the baseline rather than an added technique.</p> <p>We train this recipe on two 1.5B reasoning models using veRL: DeepSeek-R1-Distill-Qwen-1.5B and OpenMath-Nemotron-1.5B, each with 32 A800-80GB GPUs for ~15 days. The same hyperparameters work for both, without per-model tuning, and remain fixed throughout training‚Äîno schedules, no adaptation, no manual intervention, detailed as in the table below.</p> <div class="l-page"> <table> <thead> <tr> <th>Advantage Estimator</th> <th>Use KL Loss</th> <th>Use Entropy Regularization</th> <th>Train Batch Size</th> <th>Max Prompt Length</th> <th>Max Response Length</th> <th>PPO Mini Batch Size</th> <th>PPO Micro Batch Size Per GPU</th> <th>Clip Ratio Range</th> <th>Learning Rate</th> <th>Temperature</th> <th>Rollout N</th> <th>Reward Function</th> </tr> </thead> <tbody> <tr> <td>GRPO</td> <td>No</td> <td>No</td> <td>256</td> <td>1k</td> <td>15k</td> <td>64</td> <td>1</td> <td>[0.8, 1.28]</td> <td>Constant 1e-6</td> <td>1.0</td> <td>8</td> <td>DAPO<d-cite key="yu2025dapo"></d-cite></td> </tr> </tbody> </table> </div> <h3 id="evaluation-comprehensive-benchmarking">Evaluation: Comprehensive Benchmarking</h3> <p>We evaluate nine challenging mathematical reasoning tasks based on reproducible evaluation scripts from POLARIS<d-cite key="Polaris2025"></d-cite>:</p> <ul> <li><strong>Benchmarks:</strong> AIME 2024, AIME 2025, AMC 2023, MATH-500, Minerva Math, OlympiadBench, HMMT Feb 2025, CMIMC 2025, and BRUMO 2025.</li> <li><strong>Evaluation protocol:</strong> We report Pass@1 accuracy, averaging over N sampled responses per problem (N=4 for MATH-500, Minerva Math, and OlympiadBench; N=32 for others). We use temperature 0.7, top-p 0.9, and allow up to 32K tokens for generation.</li> </ul> <p>We observe that current rule-based verifiers may produce false negatives when a model generates a correct answer in an unexpected format. To address this, we augment existing systems with CompassVerifier-3B<d-cite key="CompassVerifier"></d-cite>, a lightweight model-based verifier. We compare our results with state-of-the-art models trained on the same foundation, providing direct apples-to-apples comparisons to isolate the impact of our approach.</p> <h2 id="experiment-results">Experiment Results</h2> <p>We apply JustRL on two popular 1.5B reasoning models to demonstrate that our minimal recipe achieves competitive performance with notably stable training dynamics. Both experiments use identical hyperparameters without per-model tuning.</p> <h3 id="scaling-a-weaker-base-justrl-deepseek-15b">Scaling a Weaker Base: JustRL-DeepSeek-1.5B</h3> <div class="highlight-card"> <div class="icon">üí•</div> <div class="content"> Starting from DeepSeek-R1-Distill-Qwen-1.5B, we achieve better results through single-stage training with fixed hyperparameters, outperforming more complex approaches while using 2√ó less compute. The training curve shows over 4,000 steps of stable improvement without intervention, suggesting that an adequate scale with simple methods can outperform sophisticated techniques. </div> </div> <p>We train DeepSeek-R1-Distill-Qwen-1.5B for 4,380 steps using our simple, single-stage recipe. We report the avg@32 results across nine mathematical benchmarks as follows:</p> <div class="l-page"> <table> <thead> <tr> <th>Model</th> <th>AIME24 (@32)</th> <th>AIME25 (@32)</th> <th>AMC23 (@32)</th> <th>MATH-500 (@4)</th> <th>Minerva (@4)</th> <th>OlympiadBench (@4)</th> <th>HMMT25 (@32)</th> <th>BRUMO25 (@32)</th> <th>CMIMC25 (@32)</th> <th>Avg</th> </tr> </thead> <tbody> <tr> <td>DeepSeek-R1-Distill-1.5B</td> <td>29.90</td> <td>22.40</td> <td>63.82</td> <td>84.90</td> <td>34.65</td> <td>45.95</td> <td>13.44</td> <td>30.94</td> <td>12.89</td> <td>37.65</td> </tr> <tr> <td>DeepScaleR-1.5B-Preview<d-cite key="deepscaler2025"></d-cite></td> <td>40.21</td> <td>28.65</td> <td>73.83</td> <td>89.30</td> <td>39.34</td> <td>52.79</td> <td>18.96</td> <td>40.00</td> <td>21.00</td> <td>44.88</td> </tr> <tr> <td>ProRL-V2<d-cite key="hu2025prorlv2"></d-cite></td> <td>51.87</td> <td>35.73</td> <td>88.75</td> <td>92.00</td> <td>49.03</td> <td>67.84</td> <td>19.38</td> <td>47.29</td> <td><strong>25.86</strong></td> <td>53.08</td> </tr> <tr> <td>BroRL$^‚Ä†$<d-cite key="hu2025brorl"></d-cite></td> <td><strong>57.50</strong></td> <td>36.88</td> <td>/</td> <td><strong>92.14</strong></td> <td>49.08</td> <td>61.54</td> <td>/</td> <td>/</td> <td>/</td> <td>/</td> </tr> <tr> <td><strong>JustRL-DeepSeek-1.5B</strong></td> <td>52.60</td> <td><strong>38.75</strong></td> <td><strong>91.02</strong></td> <td>91.65</td> <td><strong>51.47</strong></td> <td><strong>67.99</strong></td> <td><strong>21.98</strong></td> <td><strong>52.71</strong></td> <td>25.63</td> <td><strong>54.87</strong></td> </tr> </tbody> </table> <p>$^‚Ä†$ <em>BroRL results are officially reported but models not released; some benchmarks unavailable.</em></p> </div> <p>Our model (JustRL-DeepSeek-1.5B) achieves 54.87% average across benchmarks, outperforming ProRL-V2‚Äôs 53.08% despite ProRL-V2‚Äôs nine-stage training pipeline with dynamic hyperparameters and more sophisticated techniques. We also lead on six of nine benchmarks, demonstrating broad improvements rather than overfitting to a single task. However, the real question is whether our simplicity comes at a computational cost. It doesn‚Äôt:</p> <div class="l-page"> <table> <thead> <tr> <th>¬†</th> <th>w/ Dynamic Sampling?</th> <th>Training Steps</th> <th>Train Batch Size</th> <th>Rollout N</th> <th>Max Context Length</th> <th>Estimated Total Token Budget</th> </tr> </thead> <tbody> <tr> <td>DeepScaleR-1.5B-Preview<d-cite key="deepscaler2025"></d-cite></td> <td>‚ùå</td> <td>1,750</td> <td>128</td> <td>8</td> <td>8k ‚Üí 16k ‚Üí 24k</td> <td>$(1040√ó8k + 480√ó16k + 230√ó24k) √ó 128√ó8 ‚âà 2.2√ó10^6k$</td> </tr> <tr> <td>ProRL-V1<d-cite key="liu2025prorl"></d-cite></td> <td>‚úÖ Filter Ratio ‚âà50%</td> <td>2,450</td> <td>256</td> <td>16 ‚Üí 32 ‚Üí 16</td> <td>8k ‚Üí 16k</td> <td>$\frac{1}{50\%}(1700√ó16√ó8k + 550√ó32√ó8k + 200√ó16√ó16k) √ó 256 ‚âà 2.1√ó10^8k$</td> </tr> <tr> <td>ProRL-V2<d-cite key="hu2025prorlv2"></d-cite></td> <td>‚úÖ Filter Ratio ‚âà50%</td> <td>+1,000</td> <td>256</td> <td>16 ‚Üí 32 ‚Üí 16</td> <td>8k ‚Üí 16k ‚Üí 8k</td> <td>$2.1√ó10^8k + \frac{1}{50\%} √ó 1000√ó16√ó8k √ó 256 ‚âà 2.8√ó10^8k$</td> </tr> <tr> <td>BroRL<d-cite key="hu2025brorl"></d-cite></td> <td>‚úÖ Filter Ratio ‚âà50%</td> <td>+191</td> <td>128</td> <td>512</td> <td>16k</td> <td>$2.8√ó10^8k + \frac{1}{50\%}√ó191√ó512√ó16k√ó128 ‚âà 6.8√ó10^8k$</td> </tr> <tr> <td><strong>JustRL-DeepSeek-1.5B</strong></td> <td>‚ùå</td> <td>4,380</td> <td>256</td> <td>8</td> <td>16k</td> <td>$4380√ó256√ó8√ó16k ‚âà 1.4√ó10^8k$</td> </tr> </tbody> </table> </div> <p>We match half of ProRL-V2‚Äôs compute budget while using a single-stage recipe with fixed hyperparameters. BroRL requires 4.9√ó more compute by increasing rollouts to 512 per example, essentially exhaustively exploring the solution space. Our approach achieves competitive performance without this computational overhead.</p> <p><strong>Note on dynamic sampling:</strong> Models marked with ‚úÖ use dynamic sampling to filter examples. Following POLARIS<d-cite key="Polaris2025"></d-cite>, we estimate a 50% filter ratio for DeepSeek-R1-Distill-Qwen-1.5B using dynamic sampling, as rollouts often contain many trivial/hard cases (e.g., 8/8 or 0/8 correct rollouts). Even assuming no filtering (i.e., 0% ratio), our compute use remains comparable or even lower, making our estimates conservative.</p> <p><strong>Training stability:</strong> Figure 1(a) shows our training curve for JustRL-DeepSeek-1.5B, showing smooth and monotonic improvement without the oscillations or plateaus that typically require intervention. The stability itself suggests we‚Äôre not fighting against our training setup.</p> <p>As of this writing, we‚Äôve continued training beyond 4,380 steps:</p> <div class="l-page"> <table> <thead> <tr> <th>Training Steps</th> <th>AIME24 (@32)</th> <th>AIME25 (@32)</th> <th>AMC23 (@32)</th> <th>MATH-500 (@4)</th> <th>Minerva (@4)</th> <th>OlympiadBench (@4)</th> <th>HMMT25 (@32)</th> <th>BRUMO25 (@32)</th> <th>CMIMC25 (@32)</th> <th>Avg</th> </tr> </thead> <tbody> <tr> <td>4,380</td> <td>52.60</td> <td><strong>38.75</strong></td> <td>91.02</td> <td>91.65</td> <td><strong>51.47</strong></td> <td>67.99</td> <td><strong>21.98</strong></td> <td>52.71</td> <td>25.63</td> <td><strong>54.87</strong></td> </tr> <tr> <td>4,520</td> <td>51.15</td> <td>37.71</td> <td>90.78</td> <td>91.20</td> <td>50.55</td> <td><strong>68.40</strong></td> <td>21.77</td> <td>53.54</td> <td>24.77</td> <td>54.43</td> </tr> <tr> <td>4,720</td> <td>52.45</td> <td>38.02</td> <td><strong>91.09</strong></td> <td><strong>91.80</strong></td> <td>48.62</td> <td>66.95</td> <td>21.04</td> <td>53.33</td> <td>25.16</td> <td>54.27</td> </tr> <tr> <td>4,860</td> <td><strong>54.06</strong></td> <td>38.44</td> <td>90.16</td> <td>91.40</td> <td>49.63</td> <td>66.62</td> <td>21.88</td> <td><strong>53.54</strong></td> <td><strong>25.86</strong></td> <td>54.62</td> </tr> </tbody> </table> </div> <p>Performance appears to plateau around 54-55% average, with AIME 2024 continuing to improve (54.06% at step 4,860). This plateau might represent the ceiling for this foundation model without additional techniques, or it might simply need more training.</p> <h3 id="scaling-a-stronger-base-justrl-nemotron-15b">Scaling a Stronger Base: JustRL-Nemotron-1.5B</h3> <div class="highlight-card"> <div class="icon">üí•</div> <div class="content"> The same recipe scales OpenMath-Nemotron-1.5B to the current best math reasoning performance without any hyperparameter adjustment, matching state-of-the-art results that use curriculum learning and question augmentation. Competitive performance across two different starting points suggests the approach is robust rather than carefully tuned to specific conditions. </div> </div> <p>We train OpenMath-Nemotron-1.5B for 3,440 steps using the identical recipe, without hyperparameter changes. We report the evaluation results across nine challenging mathematical benchmarks as follows:</p> <div class="l-page"> <table> <thead> <tr> <th>Model</th> <th>AIME24 (@32)</th> <th>AIME25 (@32)</th> <th>AMC23 (@32)</th> <th>MATH-500 (@4)</th> <th>Minerva (@4)</th> <th>OlympiadBench (@4)</th> <th>HMMT25 (@32)</th> <th>BRUMO25 (@32)</th> <th>CMIMC25 (@32)</th> <th>Avg</th> </tr> </thead> <tbody> <tr> <td>OpenMath-Nemotron-1.5B</td> <td>58.75</td> <td>48.44</td> <td>90.55</td> <td>92.40</td> <td>26.93</td> <td>71.70</td> <td>30.10</td> <td>61.67</td> <td>30.08</td> <td>56.74</td> </tr> <tr> <td>QUESTA-Nemotron-1.5B<d-cite key="li2025questa"></d-cite></td> <td><strong>71.56</strong></td> <td>62.08</td> <td>93.44</td> <td>92.95</td> <td><strong>32.08</strong></td> <td>72.28</td> <td><strong>40.94</strong></td> <td><strong>67.50</strong></td> <td>41.48</td> <td>63.81</td> </tr> <tr> <td><strong>JustRL-Nemotron-1.5B</strong></td> <td>69.69</td> <td><strong>62.92</strong></td> <td><strong>96.02</strong></td> <td><strong>94.15</strong></td> <td>30.24</td> <td><strong>76.59</strong></td> <td>40.63</td> <td>66.88</td> <td><strong>41.72</strong></td> <td><strong>64.32</strong></td> </tr> </tbody> </table> </div> <p>We achieve 64.32% average, slightly outperforming QuestA‚Äôs 63.81% and leading on five of nine benchmarks. The gap is narrow, which makes sense‚Äîboth approaches are pushing the boundaries of what‚Äôs achievable at 1.5B scale. The key difference is in how we get there.</p> <p>QuestA<d-cite key="li2025questa"></d-cite> introduces an innovative curriculum learning approach that augments questions with partial CoT solutions as hints, splitting training stages with different difficulty. This requires not just ground-truth answers but full reasoning trajectories for curriculum construction with additional data requirements and engineering complexity. Our approach uses only the standard question-answer pairs without augmentation or curriculum design.</p> <div class="l-page"> <table> <thead> <tr> <th>¬†</th> <th>w/ Dynamic Sampling?</th> <th>Training Steps</th> <th>Train Batch Size</th> <th>Rollout N</th> <th>Max Context Length</th> <th>Estimated Total Token Budget</th> </tr> </thead> <tbody> <tr> <td>QUESTA-Nemotron-1.5B<d-cite key="li2025questa"></d-cite></td> <td>‚úÖ Filter Ratio ‚âà50%</td> <td>2,000</td> <td>128</td> <td>16</td> <td>32k</td> <td>$\frac{1}{50\%}√ó2000√ó128√ó16√ó32k ‚âà 2.6√ó10^8k$</td> </tr> <tr> <td><strong>JustRL-Nemotron-1.5B</strong></td> <td>‚ùå</td> <td>3,440</td> <td>256</td> <td>8</td> <td>16k</td> <td>$3440√ó256√ó8√ó16k ‚âà 1.1√ó10^8k$</td> </tr> </tbody> </table> </div> <p>We use 2√ó less compute while achieving slightly better average performance without designing a complex curriculum as used in QuestA.</p> <p><strong>Training stability:</strong> Figure 1(b) shows another smooth training curve. The fact that the same recipe works for both models without hyperparameter tuning suggests genuine robustness rather than lucky optimization for a single model.</p> <p>These results don‚Äôt diminish QuestA‚Äôs contribution‚Äîquestion augmentation is a clever technique that clearly helps. Rather, they demonstrate that competitive performance is achievable through simpler means. If you‚Äôre building on these foundations, you can start with our baseline and add techniques like question augmentation if needed, rather than assuming complexity is required from the start.</p> <h2 id="training-dynamics-analysis">Training Dynamics Analysis</h2> <p>The ultimate test of a training recipe isn‚Äôt just the final numbers; it‚Äôs whether you can get there reliably. Complex techniques often emerge as responses to training instability: oscillating rewards, collapsing policies, or runaway response lengths. If a simpler approach can avoid these failure modes entirely, it suggests we may have been treating symptoms rather than causes. We examine the training dynamics of JustRL-DeepSeek-1.5B in detail, tracking three key dynamics over 4,000 training steps: mean training reward, policy entropy, and mean response length. These dynamics reveal whether the model is learning stably or requires constant intervention.</p> <div class="l-page"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig2_training_dynamics-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig2_training_dynamics-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig2_training_dynamics-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig2_training_dynamics.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Training Dynamics of JustRL-DeepSeek-1.5B. (a) Policy entropy remains stable throughout training, oscillating naturally around 1.2-1.4 without drift or collapse. (b) Mean reward shows smooth, monotonic improvement from negative to ~0.4, indicating consistent learning without plateau-breaking interventions. (c) Response length naturally converges from initial verbosity (~7,000 tokens) to a stable range (4,000-5,000 tokens) with 16k max context length, without explicit length penalties. </div> </div> <ul> <li><strong>Entropy:</strong> Figure 2(a) shows policy entropy oscillating naturally between 1.0 and 1.6 at later training steps, with no systematic drift upward (exploration collapse) or downward (premature convergence), indicating that the simple ‚Äúclip higher‚Äù technique is well-performed for large-scale RL.</li> <li><strong>Mean Reward:</strong> Figure 2(b) shows the mean reward climbing from around -0.6 to +0.4 over training. The curve is noisy but the trend is unmistakably upward. More importantly, there are no extended plateaus or sudden drops that would typically trigger intervention in multi-stage approaches. The signal is consistent enough that the model can learn continuously.</li> <li><strong>Mean Response Length:</strong> The model starts verbose, generating responses averaging ~8,000 tokens. Without any explicit length penalty in our objective, it naturally compresses to 4,000-5,000 tokens by step 1,000 and maintains this range thereafter. This organic compression may be more robust than explicit penalties, which can create adversarial pressure that models learn to game, aligned with DLER<d-cite key="liu2025dler"></d-cite>.</li> </ul> <p><strong>The contrast with typical RL:</strong> While we don‚Äôt have the computational resources to run extensive controlled comparisons, the literature provides context. Many recent works explicitly cite training instabilities as motivation for their techniques: ProRL-v2<d-cite key="hu2025prorlv2"></d-cite> introduces scheduled length penalties after observing length drift; BroRL<d-cite key="hu2025brorl"></d-cite> increases rollouts to hundreds after hitting plateaus; multiple works reset reference models when KL divergence grows too large. Our training exhibits none of these pathologies that motivate intervention.</p> <p><strong>What we can‚Äôt claim:</strong> These smooth curves don‚Äôt prove that simpler approaches are always more stable, or that techniques never help. We can‚Äôt isolate which specific complex techniques cause instability versus which ones solve it. But the contrast is striking: a minimal recipe produces training dynamics that simply don‚Äôt require the interventions that have become standard practice.</p> <h2 id="ablation-studies-when-adding-techniques-doesnt-help">Ablation Studies: When Adding Techniques Doesn‚Äôt Help</h2> <p>We conduct two ablation studies starting from our base recipe on JustRL-DeepSeek-1.5B, both trained for 3,000+ steps on the same data:</p> <ol> <li><strong>w/ Overlong Penalty:</strong> Add an explicit length penalty term for the last 4k tokens (as used in DAPO<d-cite key="yu2025dapo"></d-cite>) to actively discourage verbose responses</li> <li><strong>w/ Overlong Penalty and Robust Verifier:</strong> Further add a more sophisticated verifier from DeepScaleR<d-cite key="deepscaler2025"></d-cite> to reduce false negatives (correct solutions misclassified as incorrect)</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig3_training_dynamics-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig3_training_dynamics-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig3_training_dynamics-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-justrl/fig3_training_dynamics.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Ablation Study Results. (a) AIME 2024 performance diverges after ~2,000 steps. Our base recipe (blue) reaches 55%, while adding overlong penalty (orange) plateaus at 50%, and adding both overlong penalty and robust verifier (red) plateaus at 45%. (b) Entropy: Both modifications show collapsed exploration (entropy ~0.5-0.6) compared to healthy oscillation in the base recipe (~1.2-1.4). (c) Mean reward: The similar trend, despite the base verifier's stricter scoring, indicates the model learns to produce higher-quality solutions. (d) Response length: All approaches converge to similar lengths (3,500-4,500 tokens), but the explicit penalty forces faster convergence at the cost of exploration diversity. </div> <ul> <li> <p><strong>On the overlong penalty:</strong> We hypothesized that explicitly penalizing verbose responses might improve training efficiency by pushing the model toward conciseness faster. Instead, performance degraded significantly as a trade-off. The entropy plot in Figure 3(b) reveals why: the explicit penalty collapses exploration, driving entropy down to 0.5-0.6 compared to the 1.2-1.4 range in our base approach. The explicit penalty appears to create pressure that conflicts with the learning objective, forcing premature convergence to shorter responses before the model has explored what actually works.</p> </li> <li> <p><strong>On the robust verifier:</strong> We further hypothesized that reducing false negatives (correct solutions marked wrong) would provide a cleaner learning signal. However, even after normalizing reward scales, its use leads to worse final performance, plateauing at 45% AIME 2024. Why? We offer two possible explanations: first, the stricter base verifier creates a richer spectrum of learning signals by reducing ‚Äúperfect‚Äù scores, whereas the robust verifier‚Äôs permissiveness offers less nuanced guidance. Second, the stricter verifier‚Äôs reliance on precise formatting may pressure the model to develop more robust internal computations, an incentive lost when the verifier corrects errors externally. Thus, a forgiving verifier might fail to encourage the precision required for optimal generalization.</p> </li> </ul> <div class="highlight-card"> <div class="icon">üí°</div> <div class="content"> <div class="title">What This Tells Us</div> <ul> <li><strong>Not all "standard tricks" transfer:</strong> The overlong penalty works in DAPO's context<d-cite key="yu2025dapo"></d-cite> but hurts in ours. Techniques aren't universally beneficial; they interact with other design choices in complex ways.</li> <li><strong>Simpler isn't always easier to improve:</strong> We tried two seemingly reasonable modifications and both made things worse. This suggests our base recipe is achieving some balance that's easy to disrupt.</li> </ul> </div> </div> <div class="highlight-card"> <div class="icon">üíî</div> <div class="content"> <div class="title">What We Still Don't Know</div> <p>We want to be clear about the limits of these ablations. We tested two specific modifications, but there are many other techniques we haven't explored: curriculum learning, adaptive temperature, reference model resets, different verifier designs, etc. Some of these might help. Our point isn't that techniques <em>never</em> work‚Äîit's that they need to be validated empirically rather than assumed to be beneficial.</p> </div> </div> <h2 id="discussion">Discussion</h2> <p>Our experiments provide clear evidence: competitive RL performance for small language models doesn‚Äôt require complex multi-stage pipelines or sophisticated techniques. A minimal recipe with fixed hyperparameters achieves strong results across two foundation models while maintaining stable training dynamics.</p> <ul> <li> <p><strong>What this suggests:</strong> The smooth training curves with healthy entropy, monotonic rewards and natural length convergence stand in contrast to instabilities often cited as motivation for complex techniques. Our negative ablations show that adding ‚Äúimprovements‚Äù (explicit length penalties, more permissive verifiers) actively degrades performance. This suggests complexity may sometimes address symptoms created by other design choices rather than fundamental RL challenges.</p> </li> <li> <p><strong>What we don‚Äôt know:</strong> We demonstrate that simple RL works well, but can‚Äôt isolate why. Is it the hyperparameters? The training dataset? The verifier design? Our results are also limited to two backbones in mathematical reasoning at 1.5B scale. Generalization to other domains, model sizes, and tasks remains an open question.</p> </li> <li> <p><strong>When might complexity help?</strong> We don‚Äôt advocate simplicity as dogma. Additional techniques may be valuable under extreme compute constraints, when encountering specific failure modes we didn‚Äôt face, when pushing beyond current performance ceilings, or in domains with noisier reward signals. Our argument is methodological: <strong>establish simple baselines first, then add complexity only when you identify specific problems it solves.</strong></p> </li> </ul> <h2 id="conclusion-simplicity-as-a-starting-point">Conclusion: Simplicity as a Starting Point</h2> <p>The debate over RL for small models has been clouded by assumptions that complexity is necessary for stability and performance. We set out to answer a straightforward question: What happens if we apply RL to small language models without the multi-stage pipelines, dynamic hyperparameters, and specialized techniques that have become standard practice? By stepping back to a cleaner, simpler approach, our findings provide a clear answer: adequate scale with stable fundamentals can match sophisticated techniques.</p> <p>Starting from two foundation models, we achieved comparable or even better performance, respectively, using single-stage training with fixed hyperparameters. These results match or exceed approaches that employ eight-stage training, adaptive penalties or curriculum learning. More striking than the final numbers is the path: smooth, stable improvement over thousands of steps without the interventions typically required to prevent training collapse.</p> <p>We‚Äôre not claiming simpler is always better or that techniques never help. We advocate a methodological shift: <strong>start simple, scale up, and only add complexity when a simple, robust baseline demonstrably fails.</strong> If the field can establish clearer baselines, we‚Äôll better understand when techniques actually matter versus when they compensate for other issues.</p> <p>We release our models and evaluation scripts as a baseline for the community. Use it, build upon it, critique it. If simple is enough more often than current practice assumes, that seems worth paying attention to.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Training small reasoning models with RL has become a race toward complexity, using multi-stage pipelines, dynamic schedules, and curriculum learning. We ask whether this complexity necessary? We show that JustRL, a simple recipe with fixed hyperparameters, achieves state-of-the-art performance on two different 1.5B base models (54.5% and 64.3% across 9 math benchmarks) while using 2√ó less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training remains stable over thousands of steps without intervention. This suggests the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline.]]></summary></entry></feed>