<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/feed.xml" rel="self" type="application/atom+xml"/><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-11T20:46:59+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/feed.xml</id><title type="html">ICLR Blogposts 2026</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Trade-off Between Parallel Environments and Steps in PPO</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/ppo-batch-size/" rel="alternate" type="text/html" title="The Trade-off Between Parallel Environments and Steps in PPO"/><published>2026-11-13T00:00:00+00:00</published><updated>2026-11-13T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/ppo-batch-size</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/ppo-batch-size/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In this post, we are going to explore a common dilemma we face when tuning PPO hyperparameters: constructing the batch size.</p> <p>It’s easy to think of batch size as just a single number, but in PPO, it is actually the product of two distinct levers we can pull:</p> <ul> <li>The number of parallel environments ($N$).</li> <li>The number of steps collected per environment ($T$).</li> </ul> <p>Does it matter if we reach a batch size of 2,048 by running 2 environments for 1,024 steps, or by running 1,024 environments for 2 steps?</p> <p>We will look at how this choice affects the bias and variance of our gradient estimation.</p> <p>We will keep the heavy equations to a minimum and rely on illustrations to build an intuition for what is happening under the hood.</p> <h2 id="clearing-up-the-terminology-defining-batch-vs-mini-batch">Clearing up the Terminology: Defining Batch vs. Mini-Batch</h2> <p>If you have ever looked at PPO implementations across different libraries, you might have noticed that “batch size” doesn’t always mean the same thing.</p> <p>To keep things clear in this post, here is the hierarchy we will use:</p> <ul> <li>Rollout Buffer (Total Batch Size): This is the full dataset collected before a policy update. It is the product of the number of parallel environments ($N$) and the number of steps collected per environment ($T$).</li> </ul> \[\text{Total Batch Size} = N \times T\] <ul> <li>Mini-Batch: This is the subset of the Rollout Buffer used for a single Gradient Descent step. We shuffle the Rollout Buffer and chop it into these smaller pieces to perform the updates.</li> </ul> <p><strong>Why is this confusing?</strong></p> <p>The confusion stems from the fact that popular libraries label these variables differently. For instance, Stable Baselines3 <d-cite key="stable-baselines3"></d-cite>, Dopamine <d-cite key="castro18dopamine"></d-cite>, OpenAI Baselines (PPO1) <d-cite key="baselines"></d-cite>, and Ray RLlib <d-cite key="liang2018rllib,liang2021rllib"></d-cite> often refer to the mini-batch variable simply as batch.</p> <p>This naming convention can lead to subtle but critical implementation errors. As pointed out by <d-cite key="shengyi2022the37implementation"></d-cite>:</p> <blockquote style="font-size:0.9em; margin:6px 0; padding-left:12px; border-left:3px solid #ccc;">Some common mis-implementations include (1) always using the whole batch for the update, and (2) implementing mini-batches by randomly fetching from the training data, which does not guarantee all training data points are fetched.</blockquote> <p>If we aren’t careful, we might think we are tuning the total experience collected ($N \times T$), when we are actually just changing how much data fits into a single GPU update step.</p> <h3 id="data-distribution">Data Distribution</h3> <p>In reinforcement learning, a trajectory $\tau = (s_0,a_0,s_1,a_1,\dots,s_T)$ is generated jointly by the policy and environment dynamics. Its probability under policy $\pi_\theta$ and transition matrix $P$ is:</p> \[P(\tau\mid \pi_\theta) = p(s_0)\, \prod_{t=0}^{T-1} \pi_\theta(a_t\mid s_t)\, P(s_{t+1}\mid s_t,a_t).\] <p>As the policy updates online, this trajectory distribution shifts between updates. PPO optimizes expected return using samples from the current distribution; near convergence, updates become small and the distribution stabilizes.</p> <p>Our focus here is not convergence, but how structuring a fixed total batch $NT$ (many short vs. few long rollouts) influences the variance of gradient estimates.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/data_distribution-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/data_distribution-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/data_distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/data_distribution.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="bias-and-variance-in-policy-gradients">Bias and Variance in Policy Gradients</h3> <p>Assumption for this section: gradients refer to those computed from the full collected batch per update ($N$ environments $\times$ $T$ steps), not mini-batches.</p> <p>We don’t see all possible trajectories—only a sample from the current distribution—so our policy gradient estimate is noisy, similar to stochastic gradients in supervised learning. Variance describes how much the gradient would change if we re-sampled another batch from the same distribution; larger effective sample sizes lower this variance.</p> <p>When variance is high, successive updates can point in very different directions, which makes learning unstable. Our goal here is to reason about one important source of variance: how rollout length $T$ (and thus temporal correlation) interacts with the number of environments $N$ when $NT$ is fixed.</p> <p>Bias can arise through how we estimate advantages and values from limited or skewed data (e.g., sparse rewards or highly stochastic transitions), nudging updates off the true gradient direction. Bias is intuitively a systematic drift of the mean gradients from the true gradients arising due to insufficient knowledge of the environment, introduced via incorrect estimates of the value function.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/batch_true_gradient-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/batch_true_gradient-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/batch_true_gradient-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/batch_true_gradient.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We established that noise in gradient updates makes training unstable. To control it, we must understand its sources.</p> <p>Unlike supervised learning, where noise usually just comes from drawing random samples from a fixed dataset, Reinforcement Learning adds other sources of inherent variability: environment dynamics and the tricky problem of credit assignment.</p> <h3 id="sources-of-variance-in-rl">Sources of variance in RL</h3> <p>We can think of our sampled gradient estimate ($G_B$) as the ideal gradient ($G_*$) plus these various sources of error:</p> \[G_B \approx G_* + \text{sampling noise} + \color{blue}{\text{trajectory variability}} + \color{brown}{\text{credit assignment noise}},\] <p>Sampling Noise: This is the familiar variance from using a finite batch size, common in all stochastic gradient methods.</p> <p>$\color{blue}{\text{Trajectory Variability}}$: This is noise inherent to the RL loop. It accounts for how much trajectories can differ due to environment stochasticity, sensitive dynamics (a small change in $s_0$ leads to a vastly different $s_T$), or policy stochasticity.</p> <ul> <li>Mitigation: Increasing the effective sample size—meaning increasing the number of independent trajectories ($N$).</li> </ul> <p>$\color{brown}{\text{Credit Assignment Noise}}$: This reflects the difficulty of figuring out which early actions caused a distant reward, especially with sparse or delayed feedback.</p> <ul> <li>Mitigation: Improving the advantage estimation, primarily by adjusting the Generalized Advantage Estimation (GAE) parameters or using longer rollouts ($T$).</li> </ul> <h3 id="practical-consideration-using-mini-batches">Practical Consideration: using Mini-batches</h3> <p>The full batch ($N\times T$) is often too large for a single gradient update due to memory limits or compute cost. To address this, we split the batch into mini-batches. Intuitively, let $G_B$ denote the gradient computed from the full batch. Let $G_{MB}$ denote the gradient from a mini-batch. In expectation, $G_{MB}$ updates parameters in approximately the same direction as $G_B$, as illustrated below.</p> <div style="width:55%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/mini_batch-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/mini_batch-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/mini_batch-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-ppo-batch-size/mini_batch.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Mini-batches introduce additional variance because each mini-batch is a smaller sample of the full batch, yielding noisier gradient estimates. To isolate the effects we care about, in our analysis we compute gradients from the entire collected batch per update and avoid policy changes during collection.</p> <p>Under this setup, the probability ratio $r$ (defined below) is approximately 1 across the batch, since $\theta$ is not updated between samples. In practice, PPO uses clipping to enable multiple mini-batch updates while preventing the policy from changing too much between mini-batch steps.</p> <h3 id="deconstructing-the-gradient-and-its-variance">Deconstructing the Gradient and Its Variance</h3> <p>We’ve set the stage: our goal is to manage the bias (from short rollouts $T$) and the variance (from correlation, or high $T$). Now, let’s look at the PPO math to see exactly where the variance comes from.</p> <p><strong>PPO Gradient Equation</strong>: The core of PPO’s objective is to maximize the expected advantage, weighted by the probability ratio. The PPO objective (without the clipping mechanism initially) is:</p> \[L = \frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} \underbrace{\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}}_{r} \, A^{\theta_{\text{old}}}(s_t, a_t)\] <p>(To keep things simple, we assume using entire collected batch $\mathcal{D}$ for a single policy update, ignoring mini-batches and hence clipping).</p> <p>Where:</p> <ul> <li>$N \times T$ is our Total Batch Size.</li> <li>$r$ is the Probability Ratio, comparing the new policy $\pi_\theta$ to the old $\pi_{\theta_{\text{old}}}$.</li> <li>$A^{\theta_{\text{old}}}(s_t, a_t)$ is the Advantage Estimate calculated on the collected data.</li> </ul> <p>Taking the gradient of this loss function gives us our gradient estimate, $G_B$:</p> \[G_B = \nabla_\theta L = \frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} \nabla_\theta \left[ r \cdot A^{\theta_{\text{old}}}(s_t, a_t) \right]\] <p>Since the advantage $A$ is constant with respect to the new policy $\theta$, and knowing that $\nabla_\theta r = r \cdot \nabla_\theta \log \pi_\theta(a_t \mid s_t)$, the gradient simplifies to:</p> \[\nabla_\theta L = \frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} r \, A^{\theta_{\text{old}}}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(a_t \mid s_t)\] <p>For the initial full-batch gradient calculation, the old and new policies are very close, so $r \approx 1$. This gives us the simplest form of the gradient estimate:</p> \[G_B = \frac{1}{N T} \sum_{(s_t, a_t) \in \mathcal{D}} \underbrace{ A^{\theta_{\text{old}}}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(a_t \mid s_t) }_{g_i}\] <p>This tells us something fundamental: Our policy gradient estimate $G_B$ is simply the mean of the individual policy gradients ($g_i$) calculated for every single step in our batch.</p> <h3 id="unpacking-the-variance">Unpacking the Variance</h3> <p>The variance of a mean is critical because it tells us how noisy $G_B$ is. High variance means we can’t trust the update.</p> <p>The total variance of our gradient estimate $G_B$ can be mathematically decomposed into two parts:</p> \[\text{Var}(G_B) = \text{Var}\Bigg(\frac{1}{N T} \sum_i g_i \Bigg) = \underbrace{\frac{1}{(N T)^2} \sum_i \text{Var}(g_i)}_{\text{Term 1: Individual Variance}} + \underbrace{\frac{1}{(N T)^2} \sum_{i \neq j} \text{Cov}(g_i, g_j)}_{\text{Term 2: Covariance (Correlation)}}\] <p>This decomposition holds the key to the $N$ vs. $T$ trade-off:</p> <ul> <li>Term 1: Individual Variance</li> </ul> <p>This is the noise coming from each step $g_i$. Since the sum is divided by $(NT)^2$, this term shrinks rapidly as the Total Batch Size ($NT$) increases. If every sample were independent, this is all we would have to worry about.</p> <ul> <li>Term 2: Covariance (The Correlation Problem)</li> </ul> <p>This is the term that makes RL different from Supervised Learning. $\text{Cov}(g_i, g_j)$ is not zero because $g_i$ and $g_{i+1}$ come from consecutive steps in the same environment. Consecutive states are highly related, so the policy gradients derived from them are also highly correlated.</p> <p>The Impact: When $T$ is large, we have many highly correlated steps in our batch, leading to a large positive covariance term. This effectively lowers the “effective sample size” of our batch. Even if $NT$ is large, if $T$ is too long, the covariance term can keep the overall $\text{Var}(G_B)$ high.</p> <p>The take-away is clear: To aggressively reduce variance, we must minimize the covariance term. This requires breaking the temporal correlation, which means we need more independent starting points—that is, increasing the number of parallel environments, $\mathbf{N}$.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[This blog post explores batch size in PPO-what happens when we increase the number of parallel environments versus the number of rollout steps, while keeping the total samples per update fixed. We discuss how this affects bias and variance in gradient estimation.]]></summary></entry><entry><title type="html">Beyond Attention as a Graph</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/beyond-attention-as-graph/" rel="alternate" type="text/html" title="Beyond Attention as a Graph"/><published>2026-04-29T00:00:00+00:00</published><updated>2026-04-29T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/beyond-attention-as-graph</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/beyond-attention-as-graph/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/hero-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/hero-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/hero-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/hero.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="introduction">Introduction</h2> <p>Most attention variants have been designed to retain as much sample efficiency as possible, under the constraint of achieving subquadratic scaling with respect to sequence length.</p> <p>While this has clearly been a powerful research direction, recent changes in the pretraining paradigm have directed <em>attention</em> to architectures capable of increasing sample-efficiency.</p> <p>In my previous blogpost I had briefly introduced, as a tool to explain attention sinks, a simple way of viewing attention as a graph operation.</p> <p>We will use this same viewpoint to argue that regular transformers may be <strong>fundamentally limited</strong> in their message-passing capabilities, arguing in favor of higher-order attention methods, such as 2-simplicial Attention <d-cite key="roy2025fastsimplex"></d-cite> and provide a natural way of generalizing it to $n$-simplices, while explaining them from a <strong>topological perspective</strong>.</p> <p>Finally, we will also poke at the very mechanism that makes Machine Learning “deep”: <strong>layer composition</strong>.</p> <h2 id="motivation">Motivation</h2> <p>“Deep Learning” is named after the typical definition of Neural Network models as a set of subsequent, composed <em>Layers</em>.</p> <p>Layers represent atomic, parametric transformations between vector spaces, rendered non-linear by a selection of activation functions.</p> <p>In transformers, layers are organized in transformer blocks, and the two are often used interchangeably. Transformer blocks are nothing more than subsequent attention and MLP transformations operating on the residual stream.</p> <p>Intuitively, depth is easy to justify: while the Universal Approximation Theorem guarantees that a single, infinitely wide non-linear layer can approximate any continuous function arbitrarily well, it doesn’t mean that width scaling is practical.</p> <p>As it turns out, properly approximating functions becomes exponentially hard with respect to the dimension of the spaces the functions map between, which can be seen as another angle of the curse of dimensionality <d-cite key="poggio2017deepnotshallow_journal"></d-cite>.</p> <p>For this reason, it becomes convenient to instead “break down” the approximation problem by composing several parametric layers, one after the other.</p> <p>This allows the model to increase in expressivity without exploding in (latent) dimensionality.</p> <p>As for all worthwhile architectural choices in deep learning, this exposes us to a tradeoff: composing operations sequentially is <em>by definition</em> the least <strong>parallel (and hence fast) architectural choice we can make</strong>.</p> <h3 id="depth-for-transformers">Depth for Transformers</h3> <p>While the previous considerations apply in general for all Neural Network architectures, transformers in particular have their specific drawbacks when scaling depth: Transformers’ success has been greatly propelled by their natural parallelism during Next Token Prediction tasks, and, apart from inevitably increasing latency in both inference and training, depth exposes the network to further instability in gradients, as, depending on normalization, the model risks vanishing or exploding gradients.</p> <p>In sequence modelling, though, one key element justifies depth: attention is an operation that message-passes between pairs of tokens in a graph. This means that individual transformer blocks can only possibly encode interactions between pairs of tokens. <strong>Depth allows information to be passed beyond a single-hop</strong>: if we reframe the $AV$ multiplication as in the attention sinks blogpost (seeing as “diffusion” of $V$ on the graph), we can reconnect this intuition to regular graph theory by noticing how powers of the adjacency matrix of a graph, $A^k$, represent $k$-hop walks from each node, and therefore depth approximates this due to attention’s fully connected, yet sparse, input-dependent adjacency matrix.</p> <p>As a result, depth is a fundamental ingredient in transformers that allows them to effectively message-pass between <em>tuples</em> of tokens, and hence build complex and useful representations of tokens in sequences.</p> <p>But what if there existed a way to message-pass between tuples of tokens without resorting to depth?</p> <h2 id="what-lies-beyond-graphs">What Lies Beyond Graphs</h2> <p>As we know, the message-passing operation happening during attention can be conceptualized as a graph operation. This simple observation, while trivial, has a relevant practical consequence: an entire field of science has, since roughly 2017, been extensively studying Neural Networks as message-passing on graphs, and has developed a variety of theories and techniques to best represent information on topological objects. Of course, the field in question is Geometric Deep Learning, and its central contributions, Graph Neural Networks and Topological Deep Learning.</p> <p>Notably, one key element of that vast literature has been an expressivity bound on GNN architectures: if we define “expressivity” as the capability of distinguishing graphs that are different, then a GNN is only as expressive as the Weisfeiler-Lehman test <d-cite key="huang2022wl"></d-cite> (also referred to as the WL-test). I won’t go in the details of what the test is, and will gladly refer the interested reader to Federico Barbero’s excellent video explaining it.</p> <p>If you don’t have the time, here’s the gist of it: the WL-test is designed to understand when two graphs are isomorphic (the same graph), but it doesn’t always work. It can be shown that a GNN is at most as expressive at graph isomorphism as the WL-test itself <d-cite key="xu2018powerful"></d-cite>.</p> <p>If you’re anything like me, this sounds like bad news: what do you mean we have a theoretically bounded expressivity? Isn’t universal approximation the reason we like Neural Networks so much?</p> <p>Fortunately, not everything is lost. As it turns out, it’s possible to “break” the WL-test bound by inserting higher-order topological information.</p> <p>But what does it mean?</p> <p>As you know, a graph is a pair $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V} = {1, 2, \cdots, n}$ is a set of <em>nodes</em>, and $\mathcal{E}: \mathcal{V} \times \mathcal{V} \rightarrow {0,1}$ is a set of <em>links</em>, also called <em>edges</em> if $(i,j) \in \mathcal{E}$ also implies $(j,i) \in \mathcal{E}$.</p> <p>In other words, elements in $\mathcal{E}$ represent directed, pairwise relations between nodes in the graph.</p> <p>This can be naturally extended by considering a generalization of $\mathcal{E}$, say $\mathcal{E}^{(k)}$, with $k \in \mathbb{N}$, where</p> \[\mathcal{E}^{(k)} : \mathcal{V}^{k+1} \rightarrow \{0,1\}.\] <p>Intuitively, this represents <em>$k$-sized tuples</em> of nodes. For example, for $k=2$, this is equivalent to all <strong>directed triangles</strong> between nodes, while the case $k=1$ recovers the original graph with pairwise links. Note, how, intuitively, for $\mathcal{E}^{(k)}$, we would be effectively considering $k$-dimensional geometric objects: nodes would be 0-dimensional points, edges 1-dimensional lines, triangles 2-dimensional surfaces, and so on (of course this is just an intuition, for this to be true we would need to embed our nodes in a space and require relations to be undirected).</p> <p>Inserting higher-order information in message passing in GNNs can be shown to increase expressivity beyond the regular WL-test. More generally, it can be shown <d-cite key="bodnar2021weisfeiler"></d-cite> that the networks with <strong>order $k$ topological information are bounded by the $k$-WL test</strong>.</p> <p>While this is by no means a formal introduction to higher-order topological objects like Simplicial Complexes, it should be sufficient to paint an intuition about where we’re going: if we manage to message-pass also considering higher order topological objects, instead of just pairs of tokens, we may be able to capture more complex patterns in parallel, instead of having to rely on depth.</p> <h2 id="2-simplicial-attention">2-Simplicial Attention</h2> <p>The Higher-order Attention idea has been floating around for a while: its first implementation in a transformer architecture is dated to the 2019 work by Clift et al. <d-cite key="clift2020simplicialtransformer"></d-cite>, and further along has been reinvented/reinterpreted/tangentially rediscovered in a series of works, such as Representational Strengths and Limitations of Transformers <d-cite key="sanford2023representational"></d-cite>, Tensor attention <d-cite key="liang2024tensorattentiontraining"></d-cite>, The Cellular Transformer <d-cite key="ballester2024cellulartransformer"></d-cite>, AlphaFold 2 <d-cite key="jumper2021alphafold"></d-cite>, and TransNAR <d-cite key="bounsi2024transformersmeetnar"></d-cite>. Even I, since last year, have been obsessed with the idea, proposing it in public a couple of times.</p> <p>Apart from theoretical work, what this idea really needed was a step towards experimental validation under a modern paradigm. Fortunately, Aurko, Rohan and their colleagues delivered well beyond that: a novel implementation <d-cite key="roy2025fastsimplex"></d-cite> of a Higher-order Attention method was the first architectural change to seem to induce a change in the exponent in the scaling law of Large Language Models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 1: Scaling law results from the Fast and Simplex: 2-Simplicial Attention in Triton paper. </div> <h3 id="rediscovering-simplicial-attention-from-the-topological-perspective">Rediscovering Simplicial Attention from the Topological Perspective</h3> <p>So, how do we extend our graph-based perspective on attention, so that it naturally becomes a (potentially higher-order) topological perspective?</p> <p>Refreshing the graph case, let’s take, for example</p> \[X \in \mathbb{R}^{n \times d}\] <p>And let’s treat the rows of $X$ as a <strong>point-cloud</strong>:</p> \[X = \begin{bmatrix} x_1^{\top}\\ x_2^{\top}\\ \vdots\\ x_n^{\top} \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\] <p>Constructing the $Q$, $K$, $V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{n \times d_q}\] \[K = X W_k \in \mathbb{R}^{n \times d_q}\] \[V = X W_v \in \mathbb{R}^{n \times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong></p> \[\alpha_{ij} = \langle q_i, k_j \rangle = q_i k_j^{\top}, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\] <p>Stacking all scores:</p> \[\alpha = Q K^{\top} \in \mathbb{R}^{n \times n}.\] <p>The intuition is: the more points align in Query-Key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 2: An attention matrix encodes a graph. </div> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp(\alpha_{ij}/\sqrt{d_k})}{\sum_{j'=1}^n \exp(\alpha_{ij'}/\sqrt{d_k})}, \qquad A = \text{softmax}\left(\frac{\alpha}{\sqrt{d_k}}\right)\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\text{attention}(x) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graph</strong> which is diffused from its neighbors to each node.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/message_passing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 3: Left: central node (i) weighs via attention neighboring nodes; Right: central node aggregates via attention weights the value function defined on neighboring nodes. </div> <p>But we already knew all of this from the previous blogpost. The key point to notice, here, is the operation we perform to extract a graph: we project $X$ into two distinct spaces via $W_Q$ and $W_K$, precisely because we need to perform a <em>bi</em>-linear form (the dot product) to extract a two-way relationship.</p> <p>What if we wanted to capture three-way relationships? Naturally, one could think of adding a second $K’$ matrix, resulting from a $W_{K’}$ projection, such that we would have a 3D tensor</p> \[T_{ijk} = \sum_{l} Q_{il} K_{jl} K'_{kl}\] <p>Which can also be seen as taking a multilinear product, if viewed per query:</p> \[T_{ijk} = \langle q_i, k_j, k'_s \rangle\] <p>Notice how, before, each attention score $A_{ij}$ represented the link weight going from node $i$ to node $j$. Now, each entry $T_{ijk}$ can instead be seen as the collective weight assigned to the triangle determined by the (directed) walk from node $i$, passing through node $j$, and ending up in node $k$.</p> <p>Such a triangle, in algebraic topology, may also be called a <em>2-simplex</em> (a node is a 0-simplex, an edge is a 1-simplex), explaining the naming of the attention mechanism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 4: 2-simplicial attention's tensor T, in each of its entries, represents a (directed) 2-simplex (triangle). </div> <p>Now that we’ve found a formulation to represent 2-simplices (or simplexes, one day I’ll have to decide which version of the plural I prefer), how do we transfer our regular sparsification mechanism (softmax) to it? And, moreover, what even is a neighborhood in this case?</p> <p>The intuitive extension of attention (also used in 2-simplicial attention) treats this by keeping the query token as central: instead of being a matrix, our attention score is now a 3D tensor. This simply means that, instead of rows, we now normalize over entire slices associated with query $i$.</p> <p>Meaning, our softmax operation becomes:</p> \[\alpha^{(2)}_{ijk} = \text{softmax}(T)^{(2)}_{ijk} = \frac{e^{T_{ijk}}}{\sum_{jk} e^{T_{ijk}}}\] <p>Intuitively, this is defining the node’s neighborhood as the <strong>triangles it’s included in</strong>. Hence, here, we’re squashing to zero triangles with low three-way similarity, and amplifying the signal from the more similar ones.</p> <p>This makes sense because our final goal will be to use this information to update the nodes’ embeddings. With that said, there exist more ways to define adjacency for higher order structures: an interesting idea could be to normalize over triangles sharing faces, instead.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 5: Left: message passing now happens between 2-simplices (oriented triangles). Each 2-simplex is weighed by an entry in tensor T. Right: each 2-simplex has an aggregated value vector that is used to update the node's representation. </div> <p>The last piece of the puzzle is the $V$ matrix of regular attention. As we discussed previously, it can be thought of as a vector-valued function defined on nodes, where individual vectors are rows $V_i$.</p> <p>So what about 2-simplicial attention? Naturally, $V$ would still have to be defined token-wise, but now we have to engineer it so that it can represent, for node $i$, the value associated with the neighbors in a triangle, just like in regular attention $V$ was being aggregated from neighbors in the graph. Furthermore, in order to express value of tokens with full degrees of freedom, we introduce a second value projection, $V’$, that we use analogously to $K’$.</p> <p>What we need is for all triangles $(i,j,k)$ to aggregate $V_j$ and $V_{k}^{\prime}$ with some function $f:\mathbb{R}^{h}\times \mathbb{R}^{h} \rightarrow \mathbb{R}^{h}$. such that we have, for each triangle, a resulting vector \(V^{(2)}_{ijk} = f(V_{k},V_{k}^{\prime})\). In the paper, f is just the product of the entries of $V$, which can be conveniently written as an element-wise product between $V$ and $V^{\prime}$: \(V^{(2)}_{ijk} = V_{ik}V_{jk}^{\prime}\) Apart from convenience, this choice can also be seen as combining value vectors using an “AND” operation, in the sense that large values will compound, and a single small value is sufficient to drop the magnitude of the vector. This is opposed, for example, to having the function be \(V^{(2)}_{ijk} = V_{ik}+ V_{jk}^{\prime}\) which would, instead, be analogous to an “OR” operation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 6: v and v' from each triangle are aggregated and used to update the central node's embedding. </div> <p>At last, we end up with $V^{(2)}$ being another 3D tensor. This allows us to perform the final operation of attention as a tensor contraction taking us back to our regular $\mathbb{R}^{n \times d}$ shape:</p> \[\text{attention}(x)_{il} = \sum_{jk} \frac{\alpha^{(2)}_{ijk} V^{(2)}_{jkl}}{\sqrt{d}}\] <p>Note how this operation can still be thought of as some kind of “diffusion”: we are aggregating value vectors from each triangle including node $i$, scaling them and summing them to update the vector in node $i$.</p> <p>Now, the extension to the n-simplicial case is trivial:</p> <p>For n-simplices, we just repeat the 2-simplicial recipe with $n$ Key projections. For an $(n+1)$-tuple $(i, j_1, \ldots, j_n)$ define the score tensor by a multilinear form</p> \[T_{i\,j_1 \cdots j_n} = \sum_{\ell} Q_{i\ell} \prod_{m=1}^n K^{(m)}_{j_m \ell} = \langle q_i, k^{(1)}_{j_1}, \ldots, k^{(n)}_{j_n} \rangle,\] <p>and normalize per-query over all $n$-tuples to get</p> \[\alpha^{(n)}_{i\,j_1 \cdots j_n} = \frac{\exp T_{i\,j_1 \cdots j_n}}{\sum_{(j_1, \ldots, j_n)} \exp T_{i\,j_1 \cdots j_n}}.\] <p>Values remain token-wise but are combined along each $n$-simplex via a symmetric $n$-ary reducer $f$; the simplest is the element-wise product “AND”</p> \[V^{(n)}_{i\,j_1 \cdots j_n} = \prod_{m=1}^{n} V^{[m]}_{j_m i},\] <p>though sum/mean (an “OR”) or MLP reducers are possible. The update is then a contraction over all $n$-tuples incident to $i$:</p> \[\text{attn}(X)_{i\ell} = \frac{1}{\sqrt{d}} \sum_{j_1, \ldots, j_n} \alpha^{(n)}_{i\,j_1 \cdots j_n} \left[V^{(n)}_{j_1 \cdots j_n}\right]_\ell\] <p>Topologically, we’re diffusing over the star of $i$ in the $n$-skeleton (cofaces incident to $i$), so higher-order interactions are captured in one hop.</p> <p>Naturally, an $n$-simplicial attention mechanism’s memory scales catastrophically quickly with sequence length, precisely with $O(L^{n+1})$. This means that we have to come up with ways of sparsifying this mechanism in order to make it practical.</p> <p>In the 2-simplicial attention paper, this is solved by performing Sliding Window Attention (SWA) with potentially different windows per dimension in the attention tensor.</p> <p>But is this the only way to tackle this? When I first started pondering these ideas, my first thought was instead to route tokens dynamically to a fixed size window. A very similar idea came recently with DeepSeek 3.2 <d-cite key="deepseek2025v32exp"></d-cite>, in the shape of DeepSeek Sparse Attention (DSA). The intuition is simple: why have a sliding window when you can hand-pick the tokens you want to use, with your preferred sparsity?</p> <p>DSA (DeepSeek Sparse Attention) replaces dense attention with a two-stage sparse mechanism: a <strong>lightweight indexer</strong> followed by <strong>top-k token selection</strong>.</p> <p>The indexer computes cheap similarity scores between each query and all past tokens. For each query token $i$ and indexer head $h$, it first computes</p> \[s_{ijh} = \text{ReLU}(\langle q^I_{ih}, k^I_{j} \rangle) \cdot w^I_{ih},\] <p>where $q^I_{ih}$ is the indexer’s query vector for token $i$ and head $h$, $k^I_j$ is the (shared) indexer key for token $j$, and $w^I_{ih}$ is a learned per-head weight. Summing over heads gives the final score</p> \[S_{ij} = \sum_{h=1}^{H_I} s_{ijh}.\] <p>For each query $i$, the top-k keys according to $S_{ij}$ are selected:</p> \[\mathcal{K}_i = \text{TopK}_j(S_{ij}, k),\] <p>and full attention is then computed <strong>only</strong> on this restricted set.</p> <p>This reduces the core attention complexity from $O(L^2)$ to $O(Lk)$, while preserving the most relevant interactions, making it particularly effective for long contexts.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 7: Intuitively representing full attention, SWA and DSA in the regular case. </div> <p>In our case, we use a modified version of DSA to substitute SWA: first, we notice that substituting ReLU with softmax performs better on our small experiments on a token-wise level. Furthermore, to avoid individual computation of $qk_1^T$ and $qk_2^T$ distinct pairs, we instead leverage existing $QK^T$ from the previous regular attention layers, and directly index based on those scores, obtaining the same exact top-k scorers for both $k_1$ and $k_2$.</p> <p>This yields a tiny speedup to our very small model / small token-horizon run, while keeping the same scaling as SWA, where we have $O(Lk^2)$ (with $k$ chosen to be equivalent to the window size of SWA) instead of the full sequence $O(L^3)$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 8: Losses for a 127M variant of nanogpt using a 3:1 regular to 2-simplicial attention ratio, with a block size of 512 and a top-k/SWA window of 128 tokens. In gray, is the SWA-sparsified version, in green the DSA-inspired technique we introduced. In orange, regular self-attention. Total token horizon is of around 60M tokens. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/speedup-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/speedup-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/speedup-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/speedup.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 9: speedup across steps of DSA and SWA vs baseline </div> <p>While in Fig. 8 we can see that baseline appears to have roughly the same acceleration as windowed simplicial attention, we notice how the 2-simplicial attention paper itself only notices gains against the transformer at a much larger parameter size, as seen in Fig. 10.</p> <p>Overall, though, our acceleration is of an average of <strong>0.76%</strong> (so barely noticeable) with respect to the Sliding Window Attention version.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 9: Reported performance comparison between transformers and 2-simplicial attention in the original paper. </div> <h3 id="what-does-n-simplicial-attention-mean-for-depth">What Does n-Simplicial Attention Mean for Depth</h3> <p>As we’ve discussed, one of the key elements of depth is <strong>multi-token representation-learning</strong>. Another way to view it, is that individual tokens are in a <strong>constant relay race</strong>: each token wants to get to a target representation, but needs crucial information from other tokens’ representations to do so. If the proper representation is very hard to find, the model eventually runs out of depth to message-pass. 2-simplicial attention goes in the direction of fixing this, because it <strong>combinatorially opens up surface area</strong> for the model to do message-passing, for each block. Of course, the present one is just its first, prototypal iteration, which will inevitably change in the future.</p> <h2 id="wrapping-up">Wrapping Up</h2> <p>We’ve explored a recent advance in attention architecture, and explained it using our previously established topologically-oriented angle. We’ve also outlined a trivial extension to n-simplices of the mechanism, as well as demonstrated tiny gains in expressivity by utilizing a DSA-like sparsification of 2-simplicial attention keys, substituting SWA. Given my obsession with the topic, you’re very likely to read something from me on the topic soon. In the meantime, let me know what you think!</p>]]></content><author><name>anonymous</name></author><summary type="html"><![CDATA[We extend a graph-based perspective on attention to higher-order topological structures, exploring 2-simplicial attention and its implications for transformer depth and expressivity.]]></summary></entry><entry><title type="html">A Hitchhiker’s Guide to Agent Evaluation</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/agent-evaluation/" rel="alternate" type="text/html" title="A Hitchhiker’s Guide to Agent Evaluation"/><published>2026-04-28T00:00:00+00:00</published><updated>2026-04-28T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/agent-evaluation</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/agent-evaluation/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>As Large Language Models (LLMs) evolve from standalone text generators into <strong>autonomous agents</strong> capable of taking actions in the real world, the way we evaluate them must fundamentally change. Traditional benchmarks that measure text quality or accuracy are no longer sufficient. We need evaluation frameworks that assess whether agents can perform multi-step tasks in dynamic environments to achieve goals in a reliable and safe way.</p> <p>This blog provides a hitchhiker’s guide to the emerging field of agent evaluation. It begins by detailing the key distinctions from traditional LLM evaluation, and then describes how these differences affect evaluation solutions. We organize the paper around the main questions that can allow easy entrance to newcomers to the field.</p> <hr/> <h2 id="how-do-llm-and-agent-evaluation-differ">How Do LLM and Agent Evaluation Differ?</h2> <p>The shift from LLMs to agents introduces three fundamental changes in evaluation philosophy:</p> <h3 id="single-step-vs-multi-step">Single-step vs. Multi-step</h3> <p>LLM benchmarks mostly assess one-step tasks. Agents handle <strong>long-horizon tasks</strong> requiring planning and multiple steps. For example, the SWE-bench coding tasks require editing multiple functions and files to fix a bug, going far beyond single-line code generation <d-cite key="jimenez2023swebench"></d-cite>. Similarly, τ-bench tasks involve multi-turn dialogues with tools and database queries, which standard LLM evaluation would not capture <d-cite key="yao2024tbench"></d-cite>.</p> <blockquote> To make an analogy, LLM evaluation is like examining the performance of an engine. In contrast, agent evaluation assesses a car’s performance comprehensively, as well as under various driving conditions. </blockquote> <h3 id="output-vs-outcome">Output vs. Outcome</h3> <p>Traditional LLM evaluation focuses on <strong>text-generation quality</strong>, accuracy on a benchmark, likelihood scores, or fluency metrics. Agent evaluation, by contrast, focuses on <strong>task completion</strong>. We care about whether a goal is achieved (e.g., a flight booked, GitHub issue solved), not just the plausibility of generated text <d-cite key="mohammadi2025survey"></d-cite>.</p> <h3 id="passive-vs-interactive">Passive vs. Interactive</h3> <p>Agents operate in <strong>dynamic environments</strong>, interacting with users or APIs. This means evaluation must account for interactivity and adherence to external rules. For instance, τ-bench highlights that an agent must gather user information, call backend APIs, and follow domain-specific policy rules during a conversation. Safety also becomes critical: unlike pure LLM tasks, an agent evaluation must check for policy compliance and avoidance of unsafe actions (e.g. deleting the code base) <d-cite key="levy2024stwebagentbench"></d-cite>.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph LLM["Traditional LLM Evaluation"]
        A[Input Prompt] --&gt; B[Text Output]
        B --&gt; C[Quality Metrics]
    end
    
    subgraph Agent["Agent Evaluation"]
        D[Goal/Task] --&gt; E[Multi-step Actions]
        E --&gt; F[Environment Interaction]
        F --&gt; G[Outcome Assessment]
        G --&gt; H[Safety &amp; Policy Check]
    end
</code></pre> <hr/> <h2 id="what-does-agent-evaluation-actually-measure">What Does Agent Evaluation Actually Measure?</h2> <h3 id="model-vs-system-performance">Model vs. System Performance</h3> <p>When evaluating a fixed agent framework, performance reflects the underlying LLM’s capability (e.g., tool-calling accuracy, reasoning). In this case, we are effectively measuring the model’s problem-solving ability on multi-step agentic tasks.</p> <p>By contrast, when comparing different agent architectures or “scaffolds,” the evaluation measures the full agent architecture. Leaderboards like the ones for SWE-bench and AppWorld have more focus on evaluating the different scaffolds and not only the model itself. More broadly, the <strong>Holistic Agent Leaderboard (HAL)</strong> conducts standardized trials across many tasks and frameworks to isolate architectural effects <d-cite key="kapoor2025hal"></d-cite>. For example, HAL ran 21,730 rollouts over 9 LLMs, 9 different agent “scaffolds,” and multiple benchmarks (coding, web navigation, etc.), revealing how agent design (e.g., planning algorithm, memory use) affects success.</p> <h3 id="primary-metrics">Primary Metrics</h3> <p>Most agent benchmarks report <strong>success rates</strong> or <strong>task completion percentages</strong> as the main metric (analogous to accuracy). These metrics test whether the agent was able to achieve the task, but other auxiliary measures are needed for evaluating the multi-step agent actions (trajectory) quality and efficiency of the agent, as pointed out by the AI-Agent that matters paper. Auxiliary measures include:</p> <table> <thead> <tr> <th>Metric Type</th> <th>Examples</th> </tr> </thead> <tbody> <tr> <td>Primary</td> <td>Success rate, task completion %</td> </tr> <tr> <td>Efficiency</td> <td>Latency, token cost, number of steps</td> </tr> <tr> <td>Partial Credit</td> <td>Subtask completion, milestone-based accuracy</td> </tr> <tr> <td>Trajectory Quality</td> <td>Action sequence correctness, tool usage accuracy</td> </tr> </tbody> </table> <p>Other metrics from traditional NLP (perplexity, F1) are rarely appropriate for agents because the “text output” is just one small part of the process. Instead, agent evaluation often includes metrics for action sequences, tool usage, and end-state correctness. This change reflects a higher focus on semantic evaluation than on syntactic one.</p> <hr/> <h2 id="how-to-evaluate-agent-reliability-and-safety">How to Evaluate Agent Reliability and Safety?</h2> <p>Beyond raw performance, agents must demonstrate <strong>reliability</strong> and <strong>safety</strong>. This section covers three critical dimensions.</p> <h3 id="consistency-metrics">Consistency Metrics</h3> <p>Because LLM agents are nondeterministic, it is not sufficient to only measure the agent success rate; we also need to measure how reliably an agent performs a task over multiple runs. Common metrics are <strong>pass@k</strong> and <strong>pass^k</strong> rates <d-cite key="yao2024tbench"></d-cite>:</p> \[\text{pass@}k = \text{Success in one of } k \text{ attempts}\] \[\text{pass}^k = \text{Success in all } k \text{ attempts}\] <p>Where:</p> <ul> <li>Success is defined by the task completion metric</li> <li>$k$ is the number of attempts</li> <li><strong>pass@k</strong> represents the agent’s ability to succeed <em>at least once</em> in $k$ attempts</li> <li><strong>pass^k</strong> represents the agent’s ability to succeed on <em>every one</em> of $k$ trials</li> </ul> <d-footnote>The pass@k metric is useful for scenarios where you can retry, while pass^k is crucial for production systems where consistent performance is required.</d-footnote> <p>For example, τ-bench explicitly introduced pass^k to quantify agent consistency. In practice, modern agents often have high pass@1 but rapidly falling pass^k. Yao et al. report that <strong>GPT-4’s success on τ-bench drops from ~61% (pass@1) to only ~25% for pass^8</strong>, underscoring that a good agent must not only succeed sometimes, but succeed consistently.</p> <h3 id="policy-adherence">Policy Adherence</h3> <p>In interactive or enterprise settings, agents must obey rules or policies. Benchmarks now include safety constraints as part of the task. For instance, <strong>ST-WebAgentBench</strong> explicitly provides a hierarchy of organizational policies and measures whether the agent completes the task under those policies <d-cite key="levy2024stwebagentbench"></d-cite>.</p> <p>A proposed metric is <strong>Completion under Policy (CuP)</strong>, which gives credit only if no policy is violated. Studies find that state-of-the-art agents often fail on these criteria—for example, many succeed at completing a web task but ignore critical safety rules. These metrics are especially important for high-risk organizational agents.</p> <h3 id="adversarial-safety-tests">Adversarial Safety Tests</h3> <p>Additional evaluations probe harmful or unsafe behaviors by design. For example, the <strong>CoSafe benchmark</strong> feeds agents adversarial prompts (e.g., requests for illicit instructions) and measures the rate of unsafe completions <d-cite key="pan2024cosafe"></d-cite>. Other tests like <strong>AgentHarm</strong> quantify the agent’s tendency to produce disallowed content.</p> <p>In practice, one might report the percentage of trials in which the agent violates a safety rule (akin to a “failure rate” under adversarial stress). These measures complement success metrics, ensuring an agent is not only effective but also aligned with ethical and safety standards.</p> <hr/> <h2 id="how-to-evaluate-agent-trajectories">How to Evaluate Agent Trajectories?</h2> <p>Beyond final outcomes, understanding <em>how</em> an agent arrives at its solution is crucial. This section covers trajectory-level evaluation.</p> <h3 id="milestones-and-subgoals">Milestones and Subgoals</h3> <p>Many agent tasks are naturally hierarchical. Benchmarks often define intermediate checkpoints or key subgoals along the trajectory. For instance, <strong>TheAgentCompany</strong> benchmark explicitly designs tasks that require many consecutive steps and provides partial credit for completing subtasks <d-cite key="xu2024theagentcompany"></d-cite>. Likewise, <strong>WebCanvas</strong> measures success rates at “key nodes” in the workflow <d-cite key="zhou2024webcanvas"></d-cite>.</p> <p>By breaking a task into milestones, evaluators can compute metrics like:</p> <ul> <li>Fraction of subtasks achieved</li> <li>Milestone-based accuracy</li> <li>Progress score (even for failed tasks)</li> </ul> <p>This provides a finer-grained view of progress than a single binary outcome.</p> <h3 id="agent-as-a-judge">Agent-as-a-Judge</h3> <p>A recent trend is to use LLMs (or agents) themselves to evaluate trajectories. The <strong>LLM-as-a-Judge</strong> paradigm employs a large model to score or critique an agent’s multi-step output <d-cite key="zhuge2024agentjudge"></d-cite>.</p> <p>For example, Zhuge et al. (2024) propose an <strong>Agent-as-a-Judge</strong> framework: multiple AI agents read an execution trace and vote on success. Such approaches can automatically assess factors like:</p> <ul> <li>Logical consistency</li> <li>Goal alignment</li> <li>Efficiency of the solution path</li> </ul> <p>They remain experimental, but they show promise for scalable, subjective evaluations.</p> <h3 id="tool-call-analysis">Tool-Call Analysis</h3> <p>Since agent interaction with its environment is based on tool calling, evaluating the sequence of tool invocations is critical. Key questions are:</p> <ul> <li>Did the agent call the <strong>right tools</strong>?</li> <li>Were they called in the <strong>right order</strong>?</li> </ul> <pre><code class="language-mermaid">flowchart TD
    A[Agent Action] --&gt; B{Tool Call?}
    B --&gt;|Yes| C[Invocation Accuracy]
    B --&gt;|No| D[Text Response]
    C --&gt; E[Tool Selection Accuracy]
    E --&gt; F[Sequence Analysis]
    F --&gt; G[Graph-based Metrics]
    G --&gt; H[Node F1: Correct tools]
    G --&gt; I[Edge F1: Correct ordering]
    G --&gt; J[Edit Distance: Path similarity]
</code></pre> <p>Metrics include <d-cite key="mohammadi2025survey"></d-cite>:</p> <table> <thead> <tr> <th>Metric</th> <th>What it Measures</th> </tr> </thead> <tbody> <tr> <td>Invocation Accuracy</td> <td>Was a tool call needed at each step?</td> </tr> <tr> <td>Tool Selection Accuracy</td> <td>Was the correct tool chosen?</td> </tr> <tr> <td>MRR/NDCG</td> <td>Ranking quality of tool selection</td> </tr> <tr> <td>Node F1</td> <td>Correct tools chosen (graph-based)</td> </tr> <tr> <td>Edge F1</td> <td>Correct ordering of tools</td> </tr> <tr> <td>Normalized Edit Distance</td> <td>Similarity to reference trajectory</td> </tr> </tbody> </table> <p>In addition, <strong>execution-based evaluation</strong> runs the tool calls to verify they produce the right output. For instance, <strong>GorillaBench</strong> executes each proposed function call to verify it produces the right output <d-cite key="patil2023gorilla"></d-cite>. Similarly, τ-bench applies the tools to measure if the agent achieves the desired database state.</p> <hr/> <h2 id="what-are-the-big-open-problems--research-questions-in-agent-evaluation">What Are the Big Open Problems &amp; Research Questions in Agent Evaluation?</h2> <p>Despite rapid progress, several fundamental challenges remain in agent evaluatio<d-cite key="yehudai2025survey"></d-cite>.</p> <h3 id="scalability">Scalability</h3> <p>Current agent evaluations are <strong>resource-intensive</strong>. Running complex tasks with many trials (especially using large models) can cost thousands of dollars. HAL’s evaluation harness reduced wall-clock time, but still required 21,730 agent rollouts across 9 benchmarks at a cost of about <strong>$40,000</strong> <d-cite key="kapoor2025hal"></d-cite>. Moreover, creating evaluation data is time and cost intensive, making it hard to evaluate agents on new domains.</p> <p>Future work must improve:</p> <ul> <li>Efficient evaluation: achieving comparable evaluation signal with fewer resources</li> <li>Automation: automate synthetic benchmark creation with LLM-based pipelines</li> </ul> <h3 id="cost-efficiency">Cost-Efficiency</h3> <p>As model inference is expensive, a key question is how to balance performance against computational cost. HAL, for example, emphasizes <strong>Pareto frontiers of accuracy vs. inference cost</strong> <d-cite key="kapoor2025hal"></d-cite>.</p> <p>Such multi-objective evaluation is still nascent: we need standardized ways to report cost (token usage, latency, cloud bills) alongside success rates. Without this, improvements may hide astronomical costs.</p> <h3 id="long-term-autonomy">Long-term Autonomy</h3> <p>Evaluating agents over extended interactions remains challenging. Most benchmarks cover <strong>minutes-long tasks</strong>; long-term autonomy would involve days or continuous deployment.</p> <p>Some recent efforts study simulated multi-day environments, or tasks with long-horizon goals. However, metrics for tracking sustained goal achievement or adaptation over time are still underdeveloped. How do we measure an agent’s ability to pursue a goal if the task evolves over hours or days? This “life-long” evaluation is an open frontier.</p> <h3 id="generalist-agents">Generalist Agents</h3> <p>Many benchmarks focus on narrow domains, but we aspire to agents that generalize across tasks and environments. Evaluating such <strong>generalist agents</strong> requires broad, heterogeneous test suites.</p> <p>TheAgentCompany attempted this by mixing coding, management, and finance tasks; even so, the best agent solved only <strong>~24% of tasks</strong> <d-cite key="xu2024theagentcompany"></d-cite>, highlighting the difficulty.</p> <p>Open questions include:</p> <ul> <li>How to aggregate performance across diverse tasks (weighted averages, worst-case)?</li> <li>How to design benchmarks that fairly test versatility?</li> <li>How to maintain agent and environment generality when producing a general agent evaluation framework?</li> </ul> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Agent evaluation is at an exciting inflection point. As LLMs become autonomous actors in the world, we need evaluation paradigms that go beyond text quality to assess:</p> <ol> <li><strong>Task completion</strong> over long horizons</li> <li><strong>Safety and policy compliance</strong> in interactive settings</li> <li><strong>Consistency and reliability</strong> across multiple runs</li> <li><strong>Trajectory quality</strong> including tool usage and intermediate steps</li> <li><strong>Cost-efficiency</strong> and scalability of the evaluation itself</li> </ol> <p>The benchmarks and metrics described here—from SWE-bench and τ-bench to HAL and ST-WebAgentBench—represent important first steps. But as agents become more capable and are deployed in higher-stakes domains, the evaluation frameworks must continue to evolve.</p> <p>For practitioners, the key takeaway is: <strong>don’t just measure if your agent works, measure if it works safely, consistently, and efficiently</strong>. For researchers, the open problems in scalability, long-term autonomy, and generalist evaluation offer rich opportunities for contribution.</p> <p>The hitchhiker’s guide to agent evaluation is still being written—and there’s plenty of galaxy left to explore.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[An introductory guide to LLM-based agents' evaluation. We explore what makes agent evaluation different from traditional LLM benchmarks, how to measure success, safety, and trajectory quality, and highlight open challenges in the field.]]></summary></entry><entry><title type="html">Attention Sinks from the Graph Perspective</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/attention-sinks-graph-perspective/" rel="alternate" type="text/html" title="Attention Sinks from the Graph Perspective"/><published>2026-04-28T00:00:00+00:00</published><updated>2026-04-28T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/attention-sinks-graph-perspective</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/attention-sinks-graph-perspective/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/hero.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="introduction">Introduction</h2> <p>Attention sinks have recently come back to the forefront of architecture discussion, especially due to their appearance in <a href="https://github.com/openai/gpt-oss">gpt-oss</a> (although in a different form than the effect we’re discussing today).</p> <p>As a mechanism, attention sinks are easy to describe: when trained, decoder-only transformer models tend to allocate a disproportionate amount of attention to the first few tokens, and especially to the first.</p> <p>This effect is well studied in its practical terms, and is often attributed to the model “offloading” probability mass to the early tokens to avoid their spurious allocation elsewhere. Recent works, like Softpick <d-cite key="softpick2025"></d-cite>, provide architectural choices that prevent sinks from forming. While this explanation may sound convincing at a first glance, my intuition is still bothered by it: what do you mean the model “offloads”? Of course it doesn’t explore that possibility intentionally, there must be some mechanism by which the attention sinks are either advantageous or a result of an intrinsic bias to the model. In this blogpost, we will argue that there is a significant bias in decoder-only transformers that may be to blame, at least partially, for this phenomenon. Moreover, this will also allow us to introduce a series of blogposts focused on analyzing transformers from the lens of message passing on graphs.</p> <h2 id="attention-as-message-passing">Attention as Message-Passing</h2> <p>Recent work by Chaitanya K. Joshi <d-cite key="joshi2025"></d-cite> has finally freed us from having to formalize independently a well known property of Transformers (and especially of attention layers): them being a special case of Graph Neural Networks (just like pretty much anything else, to be fair).</p> <p>As a setting to our discussion, though, we will go over another angle with which attention can be seen as message-passing on a graph.</p> <p>Most people are usually introduced to (multi-headed) self-attention directly via the original Transformer paper <d-cite key="vaswani2017attention"></d-cite>. Despite this being generally a good practice in my opinion, it generally directs attention to being interpreted as the simplest way of making tokens interact in a transformer, or as just a soft version of a dictionary lookup. While neither being wrong, such interpretations often drown out some interesting geometric details that lie in attention itself.</p> <p>Let’s start with regular, multiheaded attention.</p> <p>Say you have $n$ tokens, with an embedding dimension $d$.</p> <p>Let our input tokens be shaped as a matrix $X \in \mathbb{R}^{n \times d}$, we first process $X$ with three different linear projections, namely $W_q$, $W_k$ and $W_v$, and end up with the respective $Q \in \mathbb{R}^{n \times d_q}$, $K \in \mathbb{R}^{n \times d_k}$ and $V \in \mathbb{R}^{n \times d_v}$ matrices.</p> <p>We then perform the well-known attention operation</p> \[\text{attention}(X) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Let’s take a look at $\alpha = QK^T$.</p> <p>If we rewrite it component-wise we get</p> \[\alpha_{ij} = \sum_{l=1}^{d_k} Q_{il}(K^T)_{lj} = \sum_{l=1}^{d_k} Q_{il}K_{jl}\] <p>and if we note that $Q$ and $K$’s rows, respectively $q_i$ and $k_i$, we see that</p> \[\alpha_{ij} = q_i k_j^T = \langle q_i, k_j \rangle\] <p>The attention matrix $\alpha$’s entries are thus simply speaking the euclidean dot product between token embeddings, projected via the query and key matrices.</p> <p>This still falls within the classical presentation of attention, so nothing to see here as of yet.</p> <p>What if we could reinterpret these operations from a more geometric/topological perspective?</p> <p>Let’s take, for example</p> \[X \in \mathbb{R}^{n \times d}\] <p>And let’s treat the rows of $X$ as a <strong>point-cloud</strong>:</p> \[X = \begin{bmatrix} x_1^{\top}\\ x_2^{\top}\\ \vdots\\ x_n^{\top} \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\] <p>Constructing the $Q$, $K$, $V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{n \times d_q}\] \[K = X W_k \in \mathbb{R}^{n \times d_q}\] \[V = X W_v \in \mathbb{R}^{n \times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong></p> \[\alpha_{ij} = \langle q_i, k_j \rangle = q_i k_j^{\top}, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\] <p>Stacking all scores:</p> \[\alpha = Q K^{\top} \in \mathbb{R}^{n \times n}.\] <p>The intuition is: the more points align in Query-Key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp(\alpha_{ij}/\sqrt{d_k})}{\sum_{j'=1}^n \exp(\alpha_{ij'}/\sqrt{d_k})}, \qquad A = \text{softmax}\left(\frac{\alpha}{\sqrt{d_k}}\right)\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\text{attention}(x) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graph</strong>.</p> <p>If we write it row-wise (hence focusing on each token, or node, at a time), we see that the updated function’s value associated with the node becomes</p> \[\text{attention}(x)_i = \sum_l A_{il} V_l\] <p>But what does multiplying a function defined on a graph by the adjacency mean? Let’s say we have a directed graph $\mathcal{G} = (V, E)$ with adjacency $A$, with a function $f: v \rightarrow \mathbb{R}$ and $v \in V$.</p> <p>Then, the multiplication $y = Af$ can be written, component-wise, as</p> \[y_i = \sum_{j} A_{ij} f_{j}\] <p>Remember that, for an adjacency matrix, elements of column $i$ represent incoming links from other nodes in the graph. This means that $y_i$, or the result of the adjacency-multiplied function $f$, is the weighted average of $f$ over incoming nodes to node $i$, where the weights are decided by the adjacency matrix’ entries. Intuitively, you can think of this process as a sort of <em>diffusion</em>: features are aggregates of their neighbours. This means that, if we start with a rather unequally spatially distributed function (say a very localized highly positive region, and the rest being zero), then nodes on the boundary of the highly positive region would “diffuse” the highly positive values towards neighbouring nodes. Of course the topology of the graph heavily influences the speed of this diffusion. Unsurprisingly, this ties back very well with the actual physical phenomenon of heat diffusion, as we will see in a future blogpost.</p> <h2 id="causal-transformers-and-attention-sinks">Causal Transformers and Attention Sinks</h2> <p>Note that the discussion so far has been agnostic of masking strategies applied to the attention score. While several uses of Transformer models employ attention bidirectionally, LLMs, our Large Model protagonists, are usually causally masking attention to leverage parallelism for their Next Token Prediction task.</p> <p>In our attention mechanism, this is done by substituting our $\alpha$ adjacency matrix with a masked, causal one, in the shape of $\alpha_m = \alpha \odot M$, with $M_{ij} = 1$ if $j \leq i$ and zero otherwise. Note that this gives our attention graph an even more interesting structure: our graph is now, by design, a <strong>Directed Acyclic Graph</strong> (<em>DAG</em>), meaning the graph contains no loops, and its adjacency matrix is nilpotent (meaning there exists $k$ such that $(A^k)_{ij} = 0$, $\forall i,j$).</p> <p>One interesting corollary of this observation is that adjacency-based diffusion over DAGs is bound to accumulate information in sinks, specifically, in the first tokens of a causal model. This can be made explicit by looking at the shape of powers of $A$:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig.1-4: We simulate an attention matrix being composed with itself several times, across three different configurations: bidirectional, masked pre-softmax, softmax. As we can see, the combination of masking (making the matrix nilpotent) and softmax (forcing row-wise mass to sum to one) rapidly recovers the familiar attention pattern we see for attention sinks. </div> <p>These plots (Fig.1-4) show exactly what we expect on a DAG: as we take powers of the (masked) attention matrix $A$ the mass moves “leftward” toward early tokens. In the strictly lower-triangular case (no self-loops) this is a nilpotent operator, so sufficiently high powers collapse entirely into the earliest positions.</p> <p>To connect this with learning dynamics, linearize one residual attention block (one head, for intuition; treat the MLP as a node-wise map) as</p> \[X^{\ell+1} \approx X^{\ell} + A^{\ell} X^{\ell} B^{\ell}, \qquad B^{\ell} = W_v^{\ell} W_o^{\ell}.\] <p>Stacking $L$ such blocks yields an end-to-end map that is a polynomial in the $A^{\ell}$’s:</p> \[X^{L} \approx \left(\prod_{\ell=1}^{L}(I + A^{\ell} B^{\ell})\right) X^{0} = X^{0} + \sum_{\ell} A^{\ell} B^{\ell} X^{0} + \sum_{\ell_2 &gt; \ell_1} A^{\ell_2} B^{\ell_2} A^{\ell_1} B^{\ell_1} X^{0} + \cdots\] <p>When the $A^{\ell}$ are geometrically similar across depth, dominant terms behave like <strong>powers of a causal $A$</strong>. That is the same “multi-hop diffusion” we saw in the previous figures, progressively concentrating influence onto the first columns (early tokens).</p> <p>But if that’s the case during a forward pass, what makes a model exhibit this bias across training, as it’s been noticed in the literature?</p> <p>As it turns out, backprop itself mirrors this geometry. Gradients w.r.t. hidden states propagate with Jacobian transposes along the value path:</p> \[g^{\ell} \approx (I + {B^{\ell+1}}^{\top} {A^{\ell+1}}^{\top}) \cdots (I + {B^{L}}^{\top} {A^{L}}^{\top}) g^{L}.\] <p>Hence token-wise gradients accumulate along <strong>column sums of products of $A$</strong> (or, equivalently, row sums of products of $A^{\top}$). In a causal DAG those column sums are largest for earlier positions, so both activations <strong>and</strong> gradients preferentially route through (and update) paths that point to early tokens.</p> <p>Practically, residual connections make the map a <strong>polynomial</strong> (not a single $A^k$), multi-head mixing and $B^{\ell}$ projections reshape directions, and layer-norm rescales signals. But the structural bias remains: deeper layers inherit updates that look like compositions of attention-diffusion steps, which, under causal masking, tend to be more and more “first-column concentrated”.</p> <p>Another corollary of our observation is that it would suggest that later layers are more subject to the attention sink phenomenon, while the very first layer should be much less impacted. This turns out to be true and well known when studying attention sinks, as is the case, for example, for LLaMA-2 <d-cite key="xiao2023streamingllm"></d-cite>, or in Sun et al. <d-cite key="sun2024massive"></d-cite> and Cancedda et al. <d-cite key="cancedda-2024-spectral"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Attention patterns across layers showing the accumulation effect in later layers. </div> <p>Note that, while this <strong>may not be the single effect responsible for attention sinks</strong>, this means we should expect any causal decoder-only transformer to exhibit a bias towards allocating attention to its first few tokens (and increasingly so to the first).</p> <p>This fundamentally clashes with many interpretations of sinks: several works characterize them as a useful feature that is learned by the model. If what we propose is true, it’s exactly the opposite: when sinks <strong>don’t</strong> show up, it means <strong>the message-passing mechanism of your transformer is fundamentally flawed</strong>, and hence it performs worse.</p> <p>The attention sinks become a signal of <strong>healthy communication</strong> of tokens in attention, being a bias that is <strong>intrinsic to the causal, decoder-only transformer</strong>.</p> <h2 id="wrapping-up">Wrapping Up</h2> <p>So, to recap, what does this mean? We individuated a possible mechanism that may bias Causal Transformers to accumulate attention on its first few tokens. Note that we showed the mechanism in a highly simplified setting, and are proposing the idea that, despite those simplifications, the underlying effect is still strong enough to accumulate across training steps of a large transformer, and eventually explain the existence of attention sinks as we know them. In the next blogposts, we will use the same graph-centric framing of attention to analyze the problem of long context in transformer models, connecting it to heat diffusion and the oversmoothing and oversquashing phenomena known in the GNN literature. Stay tuned!</p>]]></content><author><name>anonymous</name></author><summary type="html"><![CDATA[We explore attention sinks in decoder-only transformers through the lens of message passing on graphs, revealing an intrinsic structural bias toward early tokens that may explain this phenomenon.]]></summary></entry><entry><title type="html">Square Peg, Round Hole: Plugging Non-Sequential Data into Sequential Language Models</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/autoregressive-tokenization/" rel="alternate" type="text/html" title="Square Peg, Round Hole: Plugging Non-Sequential Data into Sequential Language Models"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/autoregressive-tokenization</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/autoregressive-tokenization/"><![CDATA[<div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; max-width: 300px; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/square_peg-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/square_peg-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/square_peg-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/square_peg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="introduction">Introduction</h1> <p>Autoregressive sequence models sit at the center of modern generative AI, excelling in settings like natural language where data arrive in a well-defined sequence. However, many important modalities do not immediately offer such a linear structure. Images, graphs, point clouds, and sets lack an intrinsic notion of “the next token.”</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/intro-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/intro-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/intro-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/intro.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: center;"> How can one apply sequential models to non-sequential (e.g. non-language) data? (Note that we will use “autoregressive model” and “sequential model” interchangeably.) </div> <p>Despite this apparent mismatch between modeling assumption and data structure, autoregressive (AR) models have been repeatedly applied in such non-lingual settings <d-cite key="Antunes2024,10.1609/aaai.v39i24.34804,sun2024autoregressivemodelbeatsdiffusion"></d-cite>. There are good reasons: AR models offer variable-length generation, precise likelihoods, flexible conditioning, and step-by-step controllability <d-cite key="wang2024diverse,chen2024diffusion"></d-cite>. Moreover, from a practical perspective, autoregressive models have been engineered and scaled to perfection, with well-established scaling laws, training recipes, and ready-to-use open source libraries.</p> <p>This blog post explores the emerging landscape of techniques for turning non-sequential data into discrete 1D sequences, which autoregressive models can effectively process. It is intended for a diverse audience, including anyone who wishes to design machine learning systems for non-sequential data (images, molecules, point clouds, etc).</p> <p>We start with a primer on autoregressive modeling, including tokenization and positional encodings. Readers familiar with these concepts already should skip ahead to the following <a href="#what-exactly-are-tokens">section</a>, which defines “non-sequential data”. We then categorize recent research into two distinct kinds of approaches: <strong>model-level methods</strong>, which optimize the generation order for a fixed set of tokens, and <strong>tokenization-level methods</strong>, which redesign the discrete input representation itself to align with a sequential prior. In the case of tokenization-level methods, we highlight the inherent tradeoff between compressibility and modelability. Although these methods often originate in different communities and target different modalities, they are instances of the same underlying challenge. Our aim is to draw out these connections, map the shared structure across approaches, and sketch a broader landscape of possibilities for modeling non-sequential data with sequential architectures.</p> <h1 id="primer-autoregressive-modeling">Primer: Autoregressive Modeling</h1> <p>From the early days of recursive neural networks to the current transformer revolution, <strong>Autoregressive Models (ARMs)</strong> have emerged as a central paradigm for sequence-based generative modeling. By treating data as a series of discrete tokens (drawn from some finite vocabulary) and modeling their joint distribution through next-token prediction, machine learning systems achieve strong performance on tasks ranging from fluent text generation to complex program synthesis <d-cite key="chen2021evaluatinglargelanguagemodels,openai2024gpt4technicalreport"></d-cite>.</p> <p>A central assumption behind ARMs is that data can be meaningfully factorized into a sequence of tokens, \(x = (x_1,\dots,x_n)\) where each token depends on those who came before it. Although <strong>any</strong> sequence of data can be factorized as</p> \[p(x_1,\dots,x_n)=p(x_1)p(x_2\mid x_1)p(x_3 \mid x_1,x_2)\dots\] <table> <tbody> <tr> <td>via the chain rule, by “meaningfully” we refer to how easy it is to model each of the individual factors $p(x_i</td> <td>x_1,\dots,x_{i-1})$ — more on this in the next section.</td> </tr> </tbody> </table> <p>Under this factorization, the model is trained to predict the next token $x_i$ given the context of preceding tokens $x_1,\dots,x_{i-1}$. In transformers, this is implemented via causal masking, where the self-attention mechanism prevents any position from attending to “future” tokens.</p> <div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; max-width: 450px; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/cropped_CLM-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/cropped_CLM-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/cropped_CLM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/cropped_CLM.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div style="margin-top: 0.5rem; font-size: 0.9rem; color: #555;"> Autoregressive models predict next-token distributions over tokens based only on the preceding tokens. </div> </div> </div> <h2 id="what-exactly-are-tokens">What exactly are tokens?</h2> <p>So, an autoregressive model operates on a sequence of discrete tokens representing a piece of data. But what exactly are tokens, and how are they computed? At first, tokens might seem unnecessary. For example, one could simply input the raw byte sequence (e.g. UTF-8 values for text) into an autoregressive model. However, byte sequences can become very long, making long-range dependencies harder to learn and obscuring meaningful linguistic structure that the model could otherwise exploit. As a result, byte-level models often require more computation and struggle to match the efficiency and performance of systems that use more semantically informed units <d-cite key="10.1609/aaai.v33i01.33013159"></d-cite>.</p> <p>This motivated the use of <strong>tokenization</strong>: the process of mapping raw data into a sequence of discrete symbols drawn from a finite vocabulary<sup id="fnref:soft"><a href="#fn:soft" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. For text data, the most commonly used tokenization schemes such as Byte-Pair Encoding (BPE) <d-cite key="sennrich-etal-2016-neural"></d-cite>, WordPiece <d-cite key="devlin-etal-2019-bert"></d-cite>, or unigram tokenization segment strings into subword units:</p> <ul> <li> <p>“tokenization” → [‘token’, ‘ization’]</p> </li> <li> <p>“codebook” → [‘code’, ‘book’]</p> </li> </ul> <p>Subword methods strike a balance between vocabulary size and sequence length, whereas byte-level tokenization uses a near-minimal vocabulary that results in no information loss but produces considerably longer sequences. These choices reflect a fundamental tension in tokenizer design: <strong>a tokenizer optimized purely for compression (i.e. the best reconstruction per bit budget) is not necessarily the one that is easiest for a generative model to predict</strong> (as evidenced by e.g. CITE, who demonstrate that a naive compression-based tokenizer works poorly for language modeling). Later sections will revisit this reconstruction-generation tradeoff from multiple angles.</p> <p>Each token is moreover associated with a positional encoding, which encodes that token’s position in the sequence and breaks the native permutational invariance of the attention mechanism. However, even without positional encodings, the causal attention mask of autoregressive models still forces token generation to occur in a specific order. Thus, simply removing positional encodings does not fundamentally change the sequential nature of causal attention, as works like NoPos <d-cite key="haviv-etal-2022-transformer"></d-cite> have demonstrated.</p> <h2 id="tokenization-free-methods">Tokenization-free methods</h2> <p>A few recent “tokenization-free” approaches have moved away from the tokenization paradigm, which suffers from various idiosyncrasies and challenges for multilingual data <d-cite key="neitemeier2025hierarchical"></d-cite>. Instead of committing to a predefined vocabulary or a fixed sequence structure, approaches such as the Byte Latent Transformer <d-cite key="npagnoni-etal-2025-byte"></d-cite> and H-Net <d-cite key="hwang2025dynamicchunkingendtoendhierarchical"></d-cite> let the representational units evolve during generation. If the model can decide how to construct these building blocks as it trains, then the tokenization becomes an emergent property of the model’s inference dynamics, rather than something constructed ahead of time. While promising as methods for transcending hand-designed, modality-specific tokenizations, both of these models <strong>remain autoregressive</strong>. In other words, they both work with fixed sequences of bytes. Thus, although we will focus on the more widespread tokenization paradigm in the rest of this blog post, the mismatch between sequential models and non-sequential data prevails for tokenization-free methods, too.</p> <h2 id="advantages">Advantages</h2> <p>Autoregressive models have several useful properties that make them appealing across a wide range of generative settings <d-cite key="chen2024diffusion"></d-cite>:</p> <ul> <li><strong>Variable-length generation</strong>: The model can decide dynamically when to stop generating, for example by emitting an end-of-sequence symbol. This is especially important in settings where the desired output length is unknown or input-dependent.</li> <li><strong>Flexible conditioning</strong>: ARMs allow conditioning on a prefix of any length, which enables a broad class of conditional generation and editing tasks.</li> <li><strong>Efficient sampling</strong>: In large transformers, once a token has been generated, its key-value representations can be cached and reused for a dramatic speed-up at inference time.</li> <li><strong>Compatibility with search and planning algorithms</strong>: Because AR models assign a decomposable likelihood to each partial sequence, they can be combined with search procedures that explore multiple continuations in parallel (e.g., beam search).</li> <li><strong>Online feedback control</strong>: Since generation proceeds step-by-step, external signals can intervene during generation. Intermediate states can be adjusted to guide the next token, enabling closed-loop interaction and real-time control <d-cite key="Hafner2020Dream"></d-cite>.</li> </ul> <p>However, these advantages come at the cost of a rigid dependency structure: the model must commit to a specific, one-token-at-a-time generation order.</p> <h1 id="non-sequential-data">Non-sequential data</h1> <p>We’ve alluded to the idea that autoregressive models rely on a <em>meaningful</em> factorization of the data into a sequence, but that certain data is “non-sequential”. What does this mean exactly? Let’s start with some examples. Spoken and written language are clearly “sequential” in a meaningful way: they are both generated in, and meant to be consumed in, a certain temporal sequence. But if someone asked you to order the pixels from an image into a sequence, what would you choose? Raster order? Top-to-bottom, or bottom-to-top? How about the atoms in a molecule?</p> <p>Perhaps you would answer that it depends on what you want to <em>use</em> the sequence for. Otherwise, how can you choose between many seemingly equivalent orderings? Images, molecular graphs, and 3D point clouds are defined by spatial relationships and symmetries, but there is no single, canonical ordering of elements on which all readers of this post would agree. To distinguish between orderings, we require the notion of <strong>modelability for autoregressive models</strong>.</p> <p>At a high-level, modelability is a general and ubiquitous idea in representation learning: simply put, some representations are easier for models to learn from than others. This perspective echoes e.g. Xu et al.’s <d-cite key="Xu2020A"></d-cite> notion of usable information, which highlights that two representations can encode exactly the same information yet differ dramatically in how easy they are for a model to approximate <d-cite key="dieleman2025latents"></d-cite>. A similar view appears in the rate-distortion-usefulness tradeoff<sup id="fnref:tradeoff"><a href="#fn:tradeoff" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> of Tschannen et al. <d-cite key="tschannen2018recentadvancesautoencoderbasedrepresentation"></d-cite>, where the “usefulness” of a representation depends not only on information content, but also on how that information is organized.</p> <p>We focus here on the specific notion of modelability for autoregressive models. The data representation is now not a single vector per datapoint, but a sequence of discrete tokens $x_1,\dots,x_n$ per datapoint. Since autoregressive models factor the data distribution as $\prod_{i=1}^n p(x_i \mid x_{&lt;i})$, modelability asks: are the induced conditional distributions $p(x_i \mid x_{&lt;i})$ <strong>learnable by your model class</strong>? Formally, we can write this as the expected binary cross-entropy (BCE) loss (denoted by $\ell(\text{distribution}, \text{true label})$) of the best next-token prediction model $p_{\theta^*}$ from the model class:</p> \[\mathbb{E}_{x_1,\dots,x_n} \ell \left(p_\theta(\cdot \mid x_{&lt; i}), x_i \right)\] <p>Here, by the “best” model we mean the model produced by a training procedure (usually, optimizing for the same BCE loss) over a finite training set.</p> <p>In words, the autoregressive modelability of a tokenization is simply the test perplexity of the best next-token prediction model. Language (under any standard tokenizer) is highly modelable because next-token models do a good job at, well, predicting the next tokens. Note that modelability is a property of a specific tokenization of a data distribution, not the modality itself. For any data distribution and model class, we can ask what tokenization yields the optimal modelability score (the equation above). Thus, this notion of modelability implicitly depends on the inductive biases and computational limitations (e.g., finite context length and recency bias) of the model class.<sup id="fnref:tokenization"><a href="#fn:tokenization" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <p>In this sense, the dichotomy of “sequential” and “non-sequential” is overly simplistic, since one can arbitrarily pick a sequence for any input data. What we really mean is, based on domain knowledge or just common sense, is there an <strong>obviously modelable</strong> sequence? If not, we call the modality “non-sequential”, and assert that more complex methods are needed to identify a modelable tokenization (more on this later).</p> <h2 id="examples">Examples</h2> <p>The difference in modelability between different tokens orders is especially clear in domains where different prediction orders induce subproblems of highly varying difficulty. For example, consider training a model to solve Sudoku puzzles. At a given current state, some cells might be nearly forced, while others are highly ambiguous – so, the difficulty of the prediction subproblem depends strongly on which cell is predicted first. As explored by Kim and Shah et al. <d-cite key="kim2025train"></d-cite>, changing the prediction order of the unfilled Sudoku tiles can shift the model from easy, highly-constrained cases to much harder, underdetermined ones.</p> <div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; max-width: 450px; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/sudoku-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/sudoku-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/sudoku-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/sudoku.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>A similar example appears in arithmetic tasks, where models have been observed to perform better when generating blocks of digits right-to-left than left-to-right, perhaps reflecting how carries propagate in the computation <d-cite key="singh2024tokenizationcountsimpacttokenization,lee2024digitstodecisions"></d-cite>. Across these settings, a consistent pattern emerges: prediction orders that better align with a task’s underlying structure tend to be more effective.</p> <h2 id="caution-semantic-meaning-of-data-ordering">Caution: semantic meaning of data ordering</h2> <p>In this blogpost, we talk a lot about permuting data tokens. However, naively permuting data tokens clearly doesn’t make sense for domains like language or vision: the sentence “Work is more important than family” has the opposite meaning from its permutation, “Family is more important than work”<sup id="fnref:poem"><a href="#fn:poem" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>. Similarly, permuting the pixels of an image can create an entirely new image. In contrast, permuting the points in a point cloud or nodes in a graph preserves the underlying object, and therefore doesn’t lose any information. (It’s possible to formalize this using the concept of equivalence classes, but we focus on modelability, since it is the most relevant concept for autoregressive modeling.) Thus, when we talk about permuting data tokens, what we really mean is changing the generation order – while retaining the information describing the input object itself.</p> <p>Applying standard language modeling objectives to non-sequential data requires us to force a square peg into a round hole: we must linearize the intrinsic geometry of the data into a flat sequence. This raises a challenge: <strong>how should sequential models operate when there is no intrinsic order to exploit?</strong></p> <h1 id="can-we-just-use-non-sequential-models">Can we just use non-sequential models?</h1> <p>Given that many modalities lack a natural left-to-right structure, one might reasonably ask: why use autoregressive models at all? Indeed, some of the most popular alternative frameworks – like generative masked language models and diffusion models – avoid one-token-at-a-time prediction, and operate on entire sequences at once. A brief overview of these approaches is provided in the <a href="#appendix">Appendix</a>.</p> <p>However, compared to autoregressive models, these non-sequential architectures sacrifice several desirable properties discussed <a href="#primer-autoregressive-modeling">earlier</a>, including variable-length outputs, efficient sampling with the KV cache, and step-wise guidance. Thus, there remain many settings in which applying autoregressive methods is still desirable. We should note that there is a growing line of work aiming to combine the strengths of autoregressive and diffusion models <d-cite key="hoogeboom2022autoregressive,chen2024diffusion,arriola2025block"></d-cite>. We set these aside in this post, as our focus is specifically on autoregressive models.</p> <p>A well-studied domain where the lack of an inherent ordering becomes especially salient is vision. Images do not come with a built-in sequence structure, yet significant effort has been devoted to making them autoregressively modelable. We start with this domain as a case study, before diving into more recent trends of tokenization-model alignment.</p> <h1 id="example-images">Example: Images</h1> <p>Images have no inherent traversal order, yet to apply autoregressive models, we must linearize them into a 1D sequence. The earliest attempts, such as PixelRNN <d-cite key="10.5555/3045390.3045575"></d-cite>, flattened the image into a sequence of raw pixels and predicted them one by one in raster scan order (top-to-bottom, left-to-right). While these models achieved strong likelihood scores, they struggled to generate high-quality samples compared to diffusion models <d-cite key="theis2016noteevaluationgenerativemodels"></d-cite>. One limiting factor of pixel-level flattening was that it would result in very long sequences that are difficult to model (e.g., a 256 x 256 image results in a sequence of length 65,536). In fact, this is the same reason that tokenization arose for language <d-cite key="sennrich-etal-2016-neural"></d-cite>!</p> <p>To solve the sequence length problem, the fundamental unit of computation shifted from pixels to patches. This strategy was standardized by Vision Transformers (ViTs) <d-cite key="dosovitskiy2021an"></d-cite>: divide the image into fixed-size squares (e.g., 16 x 16), embed each patch as a token with positional encodings, and arrange them in a sequence. Crucially, ViTs retained the raster scan order, as shown at the bottom of the following Figure from Dosovitsky et al. <d-cite key="dosovitskiy2021an"></d-cite>.</p> <div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/vit-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/vit-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/vit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/vit.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div style="margin-top: 0.5rem; font-size: 0.9rem; color: #555;"> The Vision Transformer architecture, where the input image is converted into a sequence by flattering the patch grid according to a raster scan order. </div> </div> </div> <p>The modern paradigm of autoregressive models for images adopts this patch-based approach via the following two-stage architecture:</p> <ol> <li> <p><strong>Stage 1 - Tokenization</strong>: Models first compress the image into a codebook drawn from a learned visual dictionary, e.g. using a VQGAN <d-cite key="esser2020taming"></d-cite> or VQ-VAE <d-cite key="vandenOord2017"></d-cite>. The tokenizer is trained once, typically with reconstruction or perceptual losses, and then frozen.</p> </li> <li> <p><strong>Stage 2 - Modeling</strong>: A separate autoregressive (AR) Transformer is trained on the learned tokens in raster order. By offloading low-level reconstruction to the tokenizer, the AR model can devote its computational capacity to modeling global structure and long-range interactions.</p> </li> </ol> <div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/two_stage-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/two_stage-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/two_stage-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/two_stage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div style="margin-top: 0.5rem; font-size: 0.9rem; color: #555;"> Autoregressive image modeling tends to follow a two-step approach. In the first stage, discrete image tokens are trained using a reconstruction loss. In the second stage, the now-fixed tokens are fed into a transformer for generation. </div> </div> </div> <p>Although VQ-VAE and VQGAN tokenizers learn a visual “vocabulary”, they do not remove the need for a fixed ordering, as the tokens are still ordered for input to the transformer. Thus, the <strong>inherent mismatch between the prediction order and the causal structure of natural images remains</strong>: predicting a patch from only previously raster-ordered predecessors might force the model to commit to global structural decisions (e.g., “this is a dog”) based on ambiguous local evidence (e.g., a patch of fur in the top-left corner). This is just one example of how a representation can induce conditional prediction tasks as subproblems that are poorly aligned with the modality’s underlying structure. With this as motivation, we now turn to the general problem of aligning sequential models with non-sequential data.</p> <h1 id="aligning-sequential-models-and-non-sequential-data">Aligning sequential models and non-sequential data</h1> <p>Earlier, we noted a basic tension: representations that preserve all the information in the data tend to make generation more difficult, while representations that simplify prediction inevitably compress or bias the underlying signal, to the detriment of generation <d-cite key="lester2024training"></d-cite>. The goal, then, is to make sequential modeling easier without giving up too much reconstruction quality. We organize the emerging methods for navigating this tradeoff into two distinct categories. One approach keeps the tokenization, and therefore the reconstruction quality, fixed, but modifies the model so that the existing sequence becomes easier to generate (still autoregressively). The other approach keeps the model class fixed but changes the tokenization itself, aiming to find representations that are jointly good for reconstruction and sequential prediction. In other words, the two overarching categories of approaches we identify are:</p> <ul> <li><strong>Model-level</strong>: Given a fixed tokenization, adjust or learn an optimal ordering.</li> <li><strong>Tokenization-level</strong>: Given a desired ordering, adjust or learn the tokenization itself so that the resulting tokens are better aligned with it.</li> </ul> <p>We further subdivide these categories according to the following flowchart, which serves as a roadmap for the remainder of the post.</p> <h2 id="model-level-alignment">Model-level alignment</h2> <p>When a modality lacks an intrinsic traversal order, the challenge is to decide (or discover) the prediction order that leads to the easiest, most structurally coherent subproblems for a model to learn. Several lines of work explore increasingly sophisticated ways of doing this.</p> <h3 id="marginalizing-over-orderings">Marginalizing over orderings</h3> <p>What if we simply train the model to be robust to any sequence using data augmentation? This is precisely the method behind <strong>Any-Order Autoregressive Models (AO-ARMs)</strong>, where the model is trained under random orderings drawn uniformly from all permutations of the input sequence <d-cite key="wang2025learningorder"></d-cite>. Given a permutation $\sigma$ of indices ${1,\dots,n}$, the learned distribution factorizes as \(p(\mathbf{x} \mid \sigma) = \Pi_{i=1}^n p(x_{\sigma_i} \mid \mathbf{x}_{\sigma_{&lt;i}})\) where $\sigma_{&lt;i}$ corresponds to indices ${1, \ldots, i-1}$ under the permutation σ. In this formulation, the permutation can be interpreted as a latent variable that specifies which conditional subproblem the model solves at each step.</p> <div style="text-align: center; margin: 2rem 0;"> <div style="display: inline-block; max-width: 450px; width: 100%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/aoarm4_cropped-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/aoarm4_cropped-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/aoarm4_cropped-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/aoarm4_cropped.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div style="margin-top: 0.5rem; font-size: 0.9rem; color: #555;"> Any-order autoregressive models predict next-token distributions over randomly selected token orderings. </div> </div> </div> <p>Moreover, each permutation effectively defines a masking pattern: at step \(i\), the model observes the variables in $\sigma_{&lt;i}$ and treats all variables in $\sigma_{&gt;i}$ as unobserved. Training across many permutations therefore exposes the model to a wide variety of partially observed inputs and forces it to learn conditional distributions of the form \(p(x_i \mid \mathbf{x}_{-i}),\) which is the same family of conditionals targeted by masked language models. As noted by <d-cite key="kim2025train"></d-cite>, <strong>AO-ARMs can be viewed as an autoregressive reformulation of the masked language modeling objective</strong>, differing only in whether the observed subset is chosen by a permutation or by an explicit mask.</p> <p>A related approach is <strong>σ-GPT</strong> <d-cite key="10.1007/978-3-031-70368-3_9"></d-cite>, which also trains on random permutations but does so with explicit positional encodings for both the true index and the permutation order. This enables generation and conditioning in any order.</p> <h3 id="heuristically-choosing-the-order">Heuristically choosing the order</h3> <p>AO-ARMs already introduce the view that, once tokenization is fixed, the remaining structural choice is the permutation that determines the order of prediction. The default any-order training treats this latent variable as uniformly distributed over all permutations, which forces the model to learn a wide family of conditionals. The methods4 in this section build directly on this perspective, exploring how the choice of latent permutation influences the difficulty of the induced prediction tasks and downstream performance.</p> <p>Kim, Shah et al. <d-cite key="kim2025train"></d-cite> examine this sensitivity in Masked Diffusion Models (MDMs) trained under the same uniform distribution over permutations used in AO-ARMs. Instead of sampling tokens in a random or left-to-right order, they propose <strong>adaptive MDM inference</strong>, greedily selecting the position with the lowest predictive entropy. This “most confident first” rule is shown to dramatically improve performance on tasks such as Sudoku by allowing the model to avoid the hard, ambiguous subproblems. Notably, when this learned ordering from the MDM model is included in the training of an ARM, it also improves the performance compared to an ARM trained on the default (left-to-right) ordering.</p> <p>Pramanik et al. <d-cite key="pramanik2025distillingsemanticallyawareorders"></d-cite> adopt a related idea for images in Ordered AR. After training on random patch permutations, their model evaluates all unfilled positions in parallel and selects the next patch using a top-k scoring heuristic. Fine-tuning on these adaptively chosen paths yields a canonical semantic order that leads to improved Frèchet Inception Distance (FID) metrics compared to the raster order. Taken together, both Ordered AR and adaptive MDM inference treat order selection as a heuristic search problem, showing that even simple rules for choosing the next token can substantially improve autoregressive generation.</p> <h3 id="learning-the-order">Learning the order</h3> <p>Rather than using a heuristic, Wang et al. <d-cite key="wang2025learningorder"></d-cite> directly parametrize the ordering via a neural network. In <strong>Learning Order Autoregressive Models (LO-ARMs)</strong>, the ordering is treated explicitly as a latent variable: the model learns an order-policy \(p_x(\sigma) = \Pi_i p(\sigma_i \mid \sigma_{&lt;i}, x_{\sigma_{&lt;i}})\) that, given a partially masked input, chooses a position to unmask next. At the same time, a shared model (UNet for images, Graph Transformer for molecules) produces value logits for every position. Once the order-policy selects a position \(\sigma_i\), the model applies a softmax to choose <strong>what value to place there</strong>. The resulting orderings were found to reflect coherent structural patterns, such as placing border pixels in images or bond structures in molecular graphs first in the ordering!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/lo-arm-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/lo-arm-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/lo-arm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/lo-arm.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: center;"> The LO-ARM sampling process, in which a shared model jointly predicts which position to unmask next and what value to assign. </div> <p>A closely related learned-ordering approach is <strong>REOrder</strong> <d-cite key="kutscher2025reorderingpatchesimprovesvision"></d-cite>, which also trains a policy network to select the next position, and then trains an autoregressive model to follow the discovered task-optimal ordering.</p> <p>The model-level methods show that reordering can meaningfully reduce the difficulty of the generative task, but only within the limits imposed by the underlying tokenization. Since the representation itself is fixed, these approaches cannot introduce coarse-to-fine structure or reshape the information content of the tokens; they can only choose a more favorable sequence in which to model them. As a result, <strong>model-level alignment can improve modelability, but it cannot fully explore the reconstruction-generation tradeoff</strong>.</p> <h2 id="tokenization-level-alignment">Tokenization-level alignment</h2> <p>Tokenization-level methods are motivated by the following question: how can we provide sequential models with the “most sequential,” i.e. most modelable, representation of the input data? If the model generates autoregressively, then the tokenizer can be designed with autoregressive generation in mind from the start!. Thus, they address the reconstruction-generation tradeoff directly at the representation level, rather than merely reordering the prediction path.</p> <h3 id="heuristically-encouraging-ar-modelability">Heuristically encouraging AR modelability</h3> <p>A natural starting point is to impose an ordering that we have good reason to believe (e.g. based on domain intuition) will be easier for an autoregressive model to learn. Instead of relying on the model to discover a useful sequence structure on its own, we can choose an ordering that reflects how information in the modality is organized. A prominent example is <strong>Visual Autoregressive Modeling</strong> <d-cite key="tian2024visual"></d-cite>, which aims to align the prediction order with the hierarchical structure of images. VAR follows a <strong>next-scale</strong> prediction strategy, predicting coarse global structure first and then refining with higher-resolution tokens<sup id="fnref:diffusion"><a href="#fn:diffusion" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>. By replacing a rigid raster order with a more semantically coherent prediction schedule, VAR closed much of the historical gap between AR and diffusion models in metrics such as image quality, inference speed, data efficiency, and scalability.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/var-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/var-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/var-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/var.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: center;"> Figure from <d-cite key="tian2024visual"></d-cite>. AR and VAR both perform sequential generation, but AR generates one patch at a time, whereas VAR generates a (globally) better-resolved image with each timestep. </div> <p>Note that if prefixes of the token sequence already encode global structure, then shorter sequences essentially give a coarse depiction of the image, while longer sequences progressively increase fidelity. Several works have leveraged this property of coarse-to-fine tokenization to allow for a <strong>variable number of tokens per image</strong>. In doing so, they also tackle a core limitation of the fixed-size patch grid in standard tokenizers, where every image is forced into the same number of tokens regardless of its complexity. A plain sky and a dense texture both consume the same token budget, creating inefficiency for simple images and information loss for complex ones. Instead, sequence length can track the information density of the image. Some examples of variable-length tokenization methods include:</p> <ul> <li> <p><strong>FlexTok</strong> <d-cite key="bachmann2025flextok"></d-cite> uses nested dropout, repeatedly chopping off the tail during training so that high-level content is forced into the early positions.</p> </li> <li> <p><strong>Matryoshka Multimodal Models</strong> <d-cite key="cai2025matryoshka"></d-cite> learn nested token subsets where each prefix is already a valid representation and additional tokens just add finer details.</p> </li> <li> <p><strong>One-D-Piece</strong> <d-cite key="miwa2025onedpieceimagetokenizermeets"></d-cite>introduces a “Tail Token Drop” regularization that removes later tokens on the fly and pushes essential global semantics into the head of the sequence.</p> </li> </ul> <p>This shift from “next-patch’’ to “next-scale’’ prediction reframes what it means for images to be “non-sequential”, and illustrates the flexibility that comes with modifying the data representation directly (rather than just the ordering of predefined tokens). While there is no obvious choice of order in the spatial dimension, VAR suggests that the ordering along the resolution dimension is highly modelable in an autoregressive manner.</p> <p>This principle likely extends far beyond computer vision. Whether generating molecular graphs (e.g., defining the scaffold before functional groups) or 3D geometry (e.g., blocking out shapes before refining surface details), the more modelable sequence for complex data might be a trajectory from low to high complexity or resolution, rather than a linear path through spatial components. Moreover, the optimal generation path need not be semantically interpretable. Beyond explicit hierarchies like resolution, we expect that many high-dimensional datasets possess hidden latent structures that define a natural generation order, even if it is abstract and invisible to human observers<sup id="fnref:language"><a href="#fn:language" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. Identifying the most modelable order may require deep domain knowledge in most settings. This, in turn, motivates methods that dispense with predefined heuristics and instead build autoregressive-friendly structure directly into the learned tokenization.</p> <h3 id="autoregressive-priors">Autoregressive priors</h3> <p>In the standard two-stage tokenization-generation paradigm, Stage 1 and Stage 2 are often treated as independent problems. The tokenizer minimizes reconstruction loss, while the generator minimizes prediction loss. However, a tokenizer that is optimal for reconstruction with a global, bidirectional decoder is often suboptimal for autoregressive generation. To bridge this gap, recent works have introduced an autoregressive prior directly into the Stage 1 training process. By enforcing causal constraints during tokenization, these methods ensure the resulting codebook aligns with the sequential nature of the downstream generator. We visualize three representative approaches to this alignment in the figure below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/token_level-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/token_level-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/token_level-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-autoregressive-tokenization/token_level.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: center;"> Three approaches to incorporating autoregressive priors during tokenization. CRT <d-cite key="ramanujan2025when"></d-cite> adds next-token prediction on continuous latents, AliTok <d-cite key="wu2025sequencemodelingalignmenttokenizer"></d-cite> imposes causal decoding during Stage 1 but relaxes it during Stage 2, and LARP <d-cite key="wang2025larp"></d-cite> applies an AR prior only to global query tokens produced by a stochastic quantizer. </div> <p><strong>Causally Regularized Tokenization</strong> (CRT). Ramanujan et al. <d-cite key="ramanujan2025when"></d-cite> keep the standard encoder-quantizer-decoder architecture, but modify the objective function. CRT adds a next-token prediction loss on the pre-quantized continuous latents, encouraging tokens to be predictable from their predecessors. This explicitly trades off reconstruction quality for AR predictability and yields better downstream generative performance as well as computational efficiency.</p> <p><strong>Aligned Tokenizer</strong> (AliTok). Wu et al. <d-cite key="wu2025sequencemodelingalignmenttokenizer"></d-cite> propose AliTok, which directly constrains the decoder to be autoregressive. While the causal decoder provides a mechanism for enforcing sequential structure in the tokens, it also limits reconstruction quality. During the second stage, this limitation is mitigated by jointly training a high-fidelity bidirectional decoder while retaining the causal structure from the first stage. This approach offers a practical means of reconciling the two objectives, albeit through a multi-step process.</p> <p><strong>Learned AutoRegressive generative Prior</strong> (LARP): Instead of forcing autoregressive constraints onto all patch tokens, LARP <d-cite key="wang2025larp"></d-cite> adds a set of learned “holistic” query tokens that summarize high-level video semantics. An AR prior is trained only on these (de-quantized) query vectors, giving them a coherent causal structure without imposing constraints on the low-level patch tokens. Since they opt to use a stochastic vector quantization scheme (sampling from the codebook similarity distribution), the AR prior is trained to predict the next-token distribution. Despite the surface-level methodological differences between these methods, they all build sequentiality directly into the learned tokenizer, using general learning methods rather than heuristics.</p> <h1 id="conclusion">Conclusion</h1> <p>In sum, autoregressive models offer a flexible, efficient method for generative modeling tasks, but they require tokens to be input in some ordering. However, many modalities of interest outside language lack a clear, natural ordering for tokenization. Although one can simply choose an arbitrary ordering convention, it may not optimize the resultant model’s generation quality. This is the notion of “modelability,” which we apply specifically to autoregressive models. To improve the autoregressive modelability of arbitrary modalities, model-based approaches broadly attempt to find the most modelable ordering of some fixed tokenization. In contrast, tokenization-based approaches take sequential generation into account when constructing the tokenization itself. This encourages ordered tokenizations that are easy to incrementally predict. In a scattered landscape of diverse tokenization and ordering strategies, we hope to have provided not just a methodological survey, but a unifying perspective.</p> <p>For researchers who work with boutique architectures or non-language data modalities, we want to highlight tokenization and “sequential-ization” as promising directions for future research – in particular, modality-specific tokenization methods that anticipate the sequential nature of their downstream model and align with it. Notably, most existing alignment strategies have been explored primarily in the image domain, leaving substantial room for discovering analogous structures in other forms of data.</p> <p>There are many possible routes for the future of non-sequential data. Perhaps specialized architectures for each modality will win out in the end, and the mismatch we expound upon in this blogpost won’t be relevant! But at this point, it seems highly unlikely that the application of large, generalist sequential models for non-sequential data will disappear entirely. After all, even agents calling specialized models as tools must be able to describe the objects of interest with a sequence of tokens. Thus, the square peg for the round hole remains.</p> <h1 id="appendix">Appendix</h1> <h2 id="examples-of-non-sequential-generative-models">Examples of non-sequential generative models</h2> <p>Masked Language Models (MLMs) depart from the autoregressive likelihood factorization by adopting a denoising objective that permits bidirectional context. The model learns to reconstruct missing tokens from a global view of the input, rather than relying only on past context. This approach was popularized at scale by BERT, which showed that masked language modeling can produce highly transferable representations for a wide range of downstream tasks <d-cite key="devlin-etal-2019-bert"></d-cite>.</p> <p>Given an input sequence $\mathbf{x}$, a random subset of tokens at positions $M \subset {1,\dots,n}$ are replaced with a special [MASK] token. The model receives the masked sequence $\tilde{\mathbf{x}}$, where $\tilde{\mathbf{x}}_i = \text[MASK]$ for $i \in M$, and is optimized to estimate the conditional distributions, \(p(x_i \mid \tilde{\mathbf{x}}), \quad \text{for all } i \in M.\)</p> <p>In expectation, this objective is repeated over many different random masks \(M\), exposing the model to a rich family of partially observed subproblems. In this sense, <strong>autoregressive models can be seen as a specific subproblem of MLMs</strong>, where the masking is always applied to the final position in the input sequence and the prediction is conditioned only on past tokens. Note that unlike ARMs, MLMs do not yield a tractable likelihood over complete sequences. The loss is a sum of cross-entropies over the randomly masked positions, which is not equal to the log-likelihood of the whole sequence (as was the case with ARMs): \(\mathcal{L_{\text{MLM}}}(\theta) = -\sum_{i \in M} \log p_\theta(x_i \mid \tilde{\mathbf{x}}).\)</p> <p>Despite relaxing the left-to-right prediction order, MLMs still rely on the underlying positional structure of the sequence (as captured by the positional encodings) to determine which tokens constitute the context for each prediction. Thus, both ARMs and MLMs fundamentally assume an ordered sequence of tokens.</p> <p>If we view masking as a type of corruption process, then iterating the reconstruction step naturally gives rise to <strong>diffusion models</strong> <d-cite key="Ho2020"></d-cite>. Diffusion models define a forward process that starts from the data $x_0$ and progressively adds noise until the sample becomes nearly Gaussian. The reverse process refines the entire sample at once rather than predicting one symbol at a time, meaning that generation is defined without any notion of token order. While diffusion models have historically achieved better generative performance than autoregressive approaches in continuous domains, recent work suggests that this gap may be narrowing as better latent parameterizations become available <d-cite key="tian2024visual"></d-cite>.</p> <p>While diffusion was originally formulated using continuous Gaussian noise, several lines of work show that the same iterative denoising idea extends naturally to discrete domains. Discrete denoising diffusion probabilistic models (D3PM) replace Gaussian noise with a categorical corruption process such as random token replacement <d-cite key="austin2021"></d-cite>. Alternatively, <strong>Masked Diffusion Models (MDMs)</strong> use masking rather than categorical replacement as the corruption operator <d-cite key="lou2024discrete"></d-cite>, where each diffusion step applies a random masking pattern and the model is trained to reconstruct the missing content. As highlighted by Zheng et al. <d-cite key="zheng2025masked"></d-cite>, this makes the <strong>learning problem of MDMs equivalent to MLMs</strong>.</p> <h1 id="footnotes">Footnotes</h1> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:soft"> <p>Recent work has explored the use of continuous or “soft” tokens with an infinite vocabulary to allow for a more semantically rich latent space <d-cite key="tschannen2025jetformer"></d-cite>. <a href="#fnref:soft" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:tradeoff"> <p>The tradeoff between modelability and reconstruction is not strict (“if A increases, B necessarily decreases”): indeed, model-level approaches to computing the optimal ordering of tokens preserve reconstruction while improving modelability. However, the two are generally competing objectives. For example, as noted in CRT <d-cite key="ramanujan2025when"></d-cite>, the optimally modelable tokenization is a single constant token, which fails at reconstruction. <a href="#fnref:tradeoff" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:tokenization"> <p>This naturally dovetails with the theory of tokenization discussed by Rajaraman et al. <d-cite key="rajaraman2025theorytokenizationllms"></d-cite>: there, the advantages of tokenization were related to the limitations of the transformer, which tended to learn unigram models in their setting. <a href="#fnref:tokenization" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:poem"> <p>As is famously capitalized upon in Jonathan Reed’s reversible poem “The Lost Generation” – a clever work that can be read forwards and backwards, with diametrically opposed meanings <a href="#fnref:poem" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:diffusion"> <p>This loosely parallels the way diffusion models reconstruct signals by first resolving low-frequency components and then adding higher-frequency detail. Dieleman <d-cite key="dieleman2025latents"></d-cite> gives an intuitive explanation of the viewpoint that “diffusion is spectral autoregression”, and Falck <d-cite key="falck2025fourier"></d-cite> offers a complementary analysis that clarifies the conditions under which the connection holds. <a href="#fnref:diffusion" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:language"> <p>Even when there is a canonical ordering for the input, it might not be the most faithful to the underlying generative process. For example, one could imagine that the “best” ordering for certain language modeling tasks might be structured according to some hierarchy of abstract concepts or reasoning steps rather than the default left-to-right sequence. There is thus a growing body of work on Transformers that operate in a latent token space <d-cite key="sun2025enhancinglatentcomputationtransformers"></d-cite>. <a href="#fnref:language" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Autoregressive (AR) models are central to modern generative AI systems, yet their sequential inductive bias clashes with modalities that lack an obvious ordering, such as images, graphs, and point clouds. Despite this mismatch, AR models are widely used beyond language, owing to their scalability and controllability. This post highlights the growing set of techniques that make non-sequential data amenable to autoregressive modeling. There are two broad directions: approaches that choose or optimize a generation order for a fixed tokenization, and approaches that redesign the tokenization itself to simplify each next-token prediction step. We emphasize the tradeoffs these methods face, particularly between compression and autoregressive ``modelability”. By drawing these connections, we aim to motivate future work on tokenizations tailored to the needs of autoregressive models for arbitrary datatypes.]]></summary></entry><entry><title type="html">Hypes and Hopes for Causal Inference for Brain Dynamics</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/causal-ts/" rel="alternate" type="text/html" title="Hypes and Hopes for Causal Inference for Brain Dynamics"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/causal-ts</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/causal-ts/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The problem of identifying latent sources from sensor-level time series appears in many scientific domains, including neuroscience. A central question is whether it is possible to recover independent underlying sources from sensor mixtures, especially when the mixing process is nonlinear. We examine this question in the context of EEG recordings and evaluate whether Time-Contrastive Learning (TCL), a nonlinear ICA method, can recover meaningful source-level representations.</p> <blockquote> <p>In short: we find that while TCL can exploit nonstationarity, it does not outperform PCA in recovering components aligned with ground-truth cortical sources.</p> </blockquote> <hr/> <h2 id="background">Background</h2> <p>A classical formulation considers a vector of sources ( s = (s_1, …, s_n) ) and a mixing process:</p> \[x = A s ,\] <p>where ( x ) is observed sensor activity. ICA methods aim to recover sources by assuming statistical independence and non-Gaussianity.</p> <p>Standard ICA succeeds for linear mixtures but fails in general nonlinear settings. This motivated nonlinear ICA approaches such as the method of Hyvärinen &amp; Morioka (2016), which introduces <strong>nonstationarity</strong> as additional information for identifiability.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/mixing-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/mixing-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/mixing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/mixing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="what-is-identifiability">What Is Identifiability?</h2> <p>Identifiability formalizes whether the original sources can be uniquely recovered given observations. For nonlinear mixing ( x = f(s) ), the mapping is typically non-identifiable: many combinations of ( f ) and ( s ) produce the same observed distribution.</p> <p>Nonlinear ICA becomes identifiable only when extra structure—such as temporal nonstationarity—is introduced.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/identifiability-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/identifiability-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/identifiability-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/identifiability.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="time-contrastive-learning">Time-Contrastive Learning</h2> <p>TCL exploits the idea that if latent sources change their distributions differently across <em>time segments</em>, then a classifier trained to predict segment identity must extract features informative about those sources.</p> <p>The steps are:</p> <ol> <li><strong>Segment</strong> the multivariate time series ( x_t ) into windows indexed by ( \tau ).</li> <li><strong>Label</strong> points using their segment index.</li> <li><strong>Train</strong> a neural network encoder ( h(x_t; \theta) ) and a linear classifier:</li> </ol> \[w_\tau^\top h(x_t) + b_\tau.\] <p>The classifier approximates log-density differences</p> \[w_\tau^\top h(x) + b_\tau \approx \log p_\tau(x) - \log p_1(x),\] <p>and the learned representation satisfies:</p> \[h(x) \approx A q(s) + d ,\] <p>under assumptions on nonstationarity.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/tcl-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/tcl-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/tcl-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/tcl.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="experiments">Experiments</h2> <p>We evaluate TCL on both simulated nonlinear mixtures and EEG recordings. For EEG, we also compute source reconstructions to serve as proxy ground truth.</p> <h3 id="workflow-overview">Workflow Overview</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/workflow-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/workflow-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/workflow-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/workflow.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We obtain:</p> <ul> <li><strong>TCL embeddings</strong> from sensor data</li> <li><strong>PCA embeddings</strong> as a baseline</li> <li><strong>Source reconstruction embeddings</strong> as the evaluation target</li> </ul> <h3 id="evaluation">Evaluation</h3> <p>For simulated data, sources are nonstationary Laplacian processes with nonlinear mixing.</p> <p>For EEG, source-level signals correspond to 116 anatomical regions. We compare learned representations ( h_{\mathrm{tcl}} ) and PCA representations ( h_{\mathrm{pca}} ) against these reconstructed sources.</p> <h3 id="sparse-prediction-analysis">Sparse Prediction Analysis</h3> <p>We perform canonical correlation-style linear prediction:</p> <ul> <li>Train a linear model to map ( h_{\mathrm{tcl}} \rightarrow h_r )</li> <li>Train a baseline model ( h_{\mathrm{pca}} \rightarrow h_r )</li> </ul> <p>Ideal recovery would produce a near-permutation matrix. Instead we observe:</p> <ul> <li>PCA: sharp diagonal structure</li> <li>TCL: weak, sparse, non-diagonal mappings</li> </ul> <hr/> <h2 id="results">Results</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/results-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/results-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-causal-ts/results.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Regression coefficients comparing learned components with source-level components:</p> <table> <thead> <tr> <th>method</th> <th>unseen_size</th> <th>MDD</th> <th>ODER</th> </tr> </thead> <tbody> <tr> <td>PCA</td> <td>4</td> <td>0.0004</td> <td>0.9998</td> </tr> <tr> <td>PCA</td> <td>10</td> <td>0.0004</td> <td>0.9998</td> </tr> <tr> <td>PCA</td> <td>25</td> <td>0.0004</td> <td>0.9998</td> </tr> <tr> <td>TCL</td> <td>4</td> <td>0.0003</td> <td>0.9998</td> </tr> <tr> <td>TCL</td> <td>10</td> <td>0.0004</td> <td>0.9997</td> </tr> <tr> <td>TCL</td> <td>25</td> <td>0.0004</td> <td>0.9998</td> </tr> </tbody> </table> <p>TCL did not surpass PCA in alignment with source components. PCA maintained strong diagonal structure, while TCL representations produced weaker and more diffuse correlations.</p> <hr/> <h2 id="discussion">Discussion</h2> <h3 id="why-causal-inference-style-identifiability-fails-on-realistic-eeg">Why causal-inference-style identifiability fails on realistic EEG?</h3> <p>Real EEG includes strong 1/f background activity from many neural populations. The nonstationarity signal needed by TCL is overshadowed. Thus TCL learns features reflecting global variability rather than source-specific fluctuations.</p> <h3 id="are-causal-effects-recoverable-at-the-scalp">Are causal effects recoverable at the scalp?</h3> <p>Some directed-dependency methods (e.g., Granger-style approaches) can reveal predictive relationships, but these differ from structural causal models. Identifiability for nonlinear SCMs would require:</p> <ul> <li>known interventions,</li> <li>control over confounded pathways,</li> <li>or guaranteed invertibility of the generative process,</li> </ul> <p>none of which hold for EEG sensors.</p> <hr/> <h2 id="appendix">Appendix</h2> <p>Dataset details, implementation notes, and source reconstruction parameters follow those in the accompanying project documentation.</p> <p>Additional figures (e.g., replication checks) verify that our TCL implementation behaves as expected.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[We study whether modern identifiability-based nonlinear ICA methods, in particular Time-Contrastive Learning (TCL), can recover meaningful sources from realistic scalp-level brain recordings such as EEG. Using simulated data, EEG sensor data, and source-reconstructed cortical activity, we evaluate whether TCL provides representations aligned with the underlying sources.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/distill-example</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/iclr-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/iclr-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/iclr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-27-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/9-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/9-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/8-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/8-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/8.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/10-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/10-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/10.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/11-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/11-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/11.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/12-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/12-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/12.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
    <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-distill-example/plotly_demo_1.html' | relative_url }}"</span>
    <span class="na">frameborder=</span><span class="s">"0"</span>
    <span class="na">scrolling=</span><span class="s">"no"</span>
    <span class="na">height=</span><span class="s">"600px"</span>
    <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid.js</a> directly. Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a> syntax.</p> <p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post’s front matter:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div> <pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item</li> </ol> <ul> <li>Unordered sub-list.</li> </ul> <ol> <li>Actual numbers don’t matter, just that it’s a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li> <p>Unordered lists can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">dLLM - Rethinking Generation Beyond Autoregressive Models</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/dllm/" rel="alternate" type="text/html" title="dLLM - Rethinking Generation Beyond Autoregressive Models"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/dllm</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/dllm/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Despite the existence of a plethora of architectures and learning objectives, most language models in the current pantheon follow a time-tested recipe: a Transformer backbone trained using a next-token prediction objective, with model outputs generated autoregressively. This recipe, while simple, has proved surprisingly resilient and is still the predominant paradigm for training language models.</p> <p>To recap, autoregressive models factorize the joint probability of a token sequence of length \(T\), where \(x_{1:T}\) is a product of next-step conditionals:</p> \[p(x_{1:T}) = \prod_{t=1}^{T} p(x_t \mid x_{\&lt; t})\] \[x_{\&lt; t} = (x_1, x_2, \ldots, x_{t-1})\] <p>Here, \(p(x_{1:T})\) denotes the joint probability of the entire sequence and \(\(p(x_t \mid x_{\&lt; t})\)\) represents the probability of the next token given all previous tokens, and \(\(x_{\&lt; t}\)\) refers to the prefix of the sequence up to time step \(t-1\). This decomposition expresses a high-dimensional distribution as a product of simpler conditional distributions, which is the defining property of autoregressive models.</p> <p>These models generate sequences one token at a time. This formulation comes with some inherent disadvantages:</p> <ol> <li>Generating tokens one at a time imposes a sequential bottleneck. This means that generation latency scales with output length. While methods like KV caching make subsequent decoding faster, they are not able to eliminate the sequential dependency.</li> <li>Errors can rapidly accumulate and snowball, as the generation of one incorrect token causes it to be added as context for all subsequent tokens to rely on.</li> <li>Once a token is generated, there is no possibility for editing or revision in-place; the generated token remains part of the context for the rest of the sequence.</li> <li>ARMs optimize for token-level likelihood, which does not always correlate with sequence-level goals. The resulting tunnel vision can impede the ability of the model to plan over long-horizons or maintain coherence over longer outputs</li> <li>Hallucinations get exacerbated with ARMs, because once a token is generated, it is used as context for subsequent generations, therefore leading the model to generate coherent narratives over incorrect facts.</li> <li>Theoretically, certain distributions can be represented more efficiently by models that marginalize over latent variables; representing the same distributions via a purely autoregressive formulation can require scaling up the model parameter size super-polynomially with input length.</li> </ol> <p>As the bitter lesson <d-cite key="sutton2019bitter"></d-cite> shows, working towards learning more general capabilities and scaling them has proven more fruitful than injecting tasks or domain-specific rules into a model. In a similar vein, we can challenge the inductive biases of autoregressive models (i.e. that language sequences are generated left-to-right), thus providing the model with more freedom.</p> <p>One such alternative to autoregressive models is the diffusion paradigm. Diffusion models have seen great success in computer vision, but just like many other techniques that have found success in computer vision, adapting them to text has been hard, primarily due to the discrete nature of language. Therefore, more focus has been placed on discrete diffusion models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/MaskedDiffusion-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/MaskedDiffusion-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/MaskedDiffusion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/MaskedDiffusion.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In this blog, we focus specifically on masked discrete diffusion models, termed as dLLMs. Masked diffusion models have garnered a lot of interest recently, with a rapidly growing body of work in this paradigm.</p> <p>Masked diffusion works by:</p> <ol> <li>Defining a forward noising process that gradually replaces tokens with a special token (typically <code class="language-plaintext highlighter-rouge">[MASK]</code>), and</li> <li>Learning a reverse denoising process that iteratively predicts and “locks in” tokens until the sequence is fully unmasked.</li> </ol> <p>The masked diffusion learning objective looks similar to masked language modeling (e.g. BERT), or denoising autoencoding (e.g. BART). However, there is a key difference differentiating diffusion from these other objectives. Diffusion models are trained across a range of corruption levels and use an iterative sampling process that starts from a fully or almost fully noisy sequence and progressively denoises, rather than doing a single reconstruction from a fixed corruption scheme like BERT or BART.</p> <p>Diffusion models are still generative models. Unlike masked language modeling where the masking rate is fixed through the training process, diffusion models randomly sample a masking rate (or “time”) between 0 and 1 for each example <d-cite key="nie2025largelldm"></d-cite>.</p> <p>A typical masked diffusion training objective can be expressed as a time-weighted cross entropy calculated over masked tokens:</p> \[t \sim \mathcal{U}(0,1), \qquad x_t \sim q(x_t \mid x_0, t)\] \[\mathcal{L}(\theta) = \mathbb{E}_{x_0,\, t,\, x_t} \Big[ w(t)\, \sum_{i \in \mathcal{M}(x_t)} -\log p_\theta(x_{0,i} \mid x_t, t) \Big]\] <p>In this formulation, we first sample a time variable</p> \[t \sim \mathcal{U}(0,1)\] <p>representing a point along a continuous noising schedule. Given the original data \(x_0\), we then generate a partially corrupted version</p> \[x_t \sim q(x_t \mid x_0, t)\] <p>where the corruption is controlled by the sampled time \(t\).</p> <p>The model is trained to reverse this corruption using the loss function:</p> \[\mathcal{L}(\theta) = \mathbb{E}_{x_0, t, x_t} \Big[ w(t) \sum_{i \in \mathcal{M}(x_t)} -\log p_\theta(x_{0,i} \mid x_t, t) \Big]\] <p>Here, \({M}(x_t)\) indicates the positions in \(x_t\) that have been masked or noised, and \(w(t)\) is a weighting term that ensures heavily corrupted examples do not dominate the learning signal.</p> <p>Intuitively, the loss encourages the model to predict the original tokens \(x_{0,i}\) at the masked positions, given the noisy input \(x_t\) and the noise level \(t\). By learning to undo the corruption at every point along the noise schedule, the model effectively learns a denoising process that can reconstruct clean data from partially corrupted inputs. This principle is central to diffusion-based generative modeling and related reconstruction tasks.</p> <p>Autoregressive models optimize the maximum likelihood objective directly. Diffusion models are derived from a variational formulation (ELBO / NELBO), though many practical implementations use a weighted mixture of masked-token cross-entropies as shown above.</p> <h1 id="characteristics-of-diffusion-models">Characteristics of Diffusion Models</h1> <p>From a user standpoint, diffusion models are said to generate by infilling (iterative refinement of a partially completed sequence). This is especially suitable for tasks like coding or reasoning, which are often non-linear. Infilling also provides opportunities for personalization and enhances structured generation. The decoding order is also configurable.</p> <p>Some obvious benefits of diffusion models include the ability to perform any-order modeling, in-place context modification, and parallel token prediction.</p> <p>Let’s now explore the mechanics of masked diffusion in detail.</p> <h2 id="masked-diffusion-explained">Masked Diffusion Explained</h2> <p>Masked diffusion can be implemented independent of the architecture. For example, masked diffusion can use state-space models as the backbone.</p> <h3 id="forward-process">Forward Process</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ForwardProcessA-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ForwardProcessA-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ForwardProcessA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ForwardProcessA.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ForwardProcessB-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ForwardProcessB-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ForwardProcessB-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ForwardProcessB.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Consider an example sequence x in the training dataset:</p> <p><code class="language-plaintext highlighter-rouge">‘He invented the parallelogram as a means to exact vengeance upon his detractors’</code></p> <p>A number \(t\) sampled randomly between 0 and 1 (often chosen from a discrete mask schedule in practice), is chosen to be the mask strength. Each token in the given sequence is replaced by a <code class="language-plaintext highlighter-rouge">[MASK]</code> token with probability \(t\).</p> <p>For example, if \(t=0.2\),</p> <p><code class="language-plaintext highlighter-rouge">‘He invented the [MASK] as a means to [MASK] vengeance upon [MASK] detractors’</code></p> <p>If \(t=0.8\),</p> <p><code class="language-plaintext highlighter-rouge">‘[MASK] invented [MASK] [MASK] [MASK] means [MASK] exact vengeance [MASK] [MASK] [MASK]</code></p> <p>Let’s refer to the masked sequence as \(xt\).</p> <p>The objective of the model is to predict the masked tokens in \(xt\). The training loss is typically the cross entropy over the masked tokens, with a normalization/weighting such that examples with higher masking rates do not contribute disproportionately to the training signal. One common way of normalization is to divide the loss by the masking strength \(t\). Equivalently, some implementations instead normalize by the number of masked tokens.</p> <p>The key difference between masked language models like BERT and diffusion models is that in BERT the corruption policy (masking rate) is fixed throughout training, while in masked diffusion models the masking rate \(t\) varies per example across a range of masking rates.</p> <h3 id="reverse-process">Reverse Process</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ReverseProcess-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ReverseProcess-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ReverseProcess-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ReverseProcess.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><code class="language-plaintext highlighter-rouge">Prompt: ‘Is Socotra a real place?’</code> <code class="language-plaintext highlighter-rouge">Response: ‘Yes, Socotra is an island in Yemen.’</code></p> <p>A typical reverse process proceeds like this:</p> <p>For a given prompt \(p\), an initial answer is generated consisting entirely of <code class="language-plaintext highlighter-rouge">[MASK]</code> tokens. The response length is typically a hyperparameter.</p> <p>The reverse process (called denoising) runs for K steps, which is typically a hyperparameter.</p> <p>At each step, the model predicts tokens for all the <code class="language-plaintext highlighter-rouge">[MASK]</code> positions at once, conditioned on the prompt and the currently unmasked tokens. It then commits some tokens (unmasks them) and remasks a portion of tokens (often low-confidence ones), either randomly or via heuristics. Generation stops after all denoising steps are completed. If an <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token is present in the final output, then the tokens after it are discarded.</p> <p>In practice, there is a discrepancy between training and inference; during inference, the whole output often starts off as masked and is gradually de-masked.</p> <h2 id="training">Training</h2> <p>Diffusion language models can continue using the same Transformer backbones that underpin today’s language models. The primary change is in the learning objective, where instead of predicting the next token in a sequence as autoregressive models do, diffusion models are taught to predict all the masked tokens in a sequence simultaneously. This also means that dLLMs can be built using non-Transformer backbones (e.g., state-space models), as long as the architecture supports conditioning on a partially observed sequence.</p> <h3 id="typical-training-pipeline">Typical Training Pipeline</h3> <ol> <li>Pre-train from scratch OR Continued pre-training</li> <li>Midtraining/annealing</li> <li>Instruction tuning</li> <li>Reinforcement learning</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/TrainingPipeline-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/TrainingPipeline-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/TrainingPipeline-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/TrainingPipeline.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Note that dLLMs can either be trained from scratch or can be adapted from a base ARM.</p> <h3 id="pre-training-from-scratch">Pre-training From Scratch</h3> <p>While training from scratch, training with next-token prediction objective is more sample and compute efficient than diffusion in practice. This is because in dLLMs, the loss is typically calculated only over the masked tokens, so each forward pass in dLLMs supervises fewer target positions than an AR pass . As a result, given the same architecture, compute, and data, AR baselines typically train faster and reach higher quality, though the exact gap depends on masking schedules, weighting, and decoding strategies used.</p> <h3 id="adapting-ar-models-to-dllms">Adapting AR Models to dLLMs</h3> <p>Pre-training from scratch is not the only option; one can also adapt existing autoregressive models to support diffusion. The adaptation is typically carried out using continual pre-training. In this technique, we take a stable checkpoint of an autoregressive LLM, replace the causal mask with a bidirectional mask and then continue pre-training it with the diffusion learning objective.</p> <p>Chandrasegaran et al. <d-cite key="chandrasegaran2025rnd1"></d-cite> propose that self-attention weights are trained with a relatively higher learning rate to help adapt the model to the diffusion paradigm. The feed forward layers are trained at a relatively lower learning rate so that world knowledge and other capabilities learned during the AR pre-training stage are retained. This helps mitigate catastrophic forgetting. They also observe that dLLMs benefit from larger batch sizes during continual pre-training. Other techniques for adaptation from ARMs include</p> <ul> <li>Grafting, where the architecture is edited by swapping causal attention blocks for bidirectional attention blocks.</li> <li>Attention mask annealing, where the causal mask is gradually converted into a bidirectional one during training.</li> </ul> <p>Masked language models like BERT can also be converted into diffusion models using the continued pre-training approach.</p> <h2 id="inference">Inference</h2> <p>During inference, the model starts from a masked output sequence and generates tokens through a series of denoising steps. Within a denoising step, the masked positions are typically predicted in parallel, followed by an accept or remask decision.</p> <p>A basic denoise-and-remask procedure works as follows. We first initialize the output sequence to be entirely masked:</p> \[y^{(0)} = [\text{MASK}]^L\] <p>where \(L\) is the sequence length, typically chosen as a hyperparameter. Then, for a fixed number of iterations \(k = 1, \dots, K\), we perform the following steps:</p> <ol> <li><strong>Predict masked token distributions:</strong> Using the model, we estimate the probability distribution over tokens at the currently masked positions:</li> </ol> \[p_\theta(\cdot \mid p, y^{(k-1)}, k)\] <p>where \(p\) may represent any conditioning information (e.g., a prompt or context), \(y^{(k-1)}\) is the sequence from the previous iteration, and \(k\) indicates the current step.</p> <ol> <li> <p><strong>Commit a subset of positions:</strong> We select a subset of tokens to “commit” to the output sequence. This is usually based on a confidence criterion, such as selecting the highest-probability tokens or those with the lowest entropy.</p> </li> <li> <p><strong>Optional remasking:</strong> To refine the sequence, a heuristic or schedule may remask a subset of previously committed tokens that are considered low-confidence. This allows the model to revisit uncertain predictions in subsequent iterations.</p> </li> <li> <p><strong>Update the sequence:</strong> The newly committed tokens replace the previous masked positions to form the updated sequence \(y^{(k)}\).</p> </li> </ol> <p>After completing all \(K\) iterations, the final sequence \(y^{(K)}\) is returned. If an end-of-sequence token <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> appears, any tokens following it are discarded.</p> <p>This iterative denoising procedure gradually replaces masks with high-confidence predictions, while optionally revisiting uncertain tokens. Over multiple steps, the sequence converges toward a coherent output that reflects both the learned model distribution and any conditioning context.</p> <p>The initial output sequence can be fully masked or it can contain parts of the output we already know; delegating the model to perform infilling for the tokens we do not know. This can be operationalized in a few ways, such as constrained endings or structured infilling.</p> <h3 id="structured-infilling">Structured Infilling</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/StructuredInfilling-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/StructuredInfilling-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/StructuredInfilling-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/StructuredInfilling.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Instead of asking the model to generate output in a specific structured format (like JSON), we use a structured format template and let the dLLM fill in the blanks.</p> <p>For a given structured format, we have:</p> <ul> <li>Invariant tokens (syntax, labels, brackets etc), which stay unmasked</li> <li>Variable tokens (value, content) which are masked</li> </ul> <p>An advantage with infilling templates is that it shrinks the search space during the generation, now that it need only choose content tokens and not tokens related to the syntax. Another advantage is that it implicitly ensures the structured format is adhered to during generation.</p> <p>The tricky part of this technique is in deciding how many masked tokens to allocate for the variables. If the masked tokens added are inadequate, the output has to be truncated. If too many masked tokens are added, then the model tries to fill in the extraneous masked tokens with content, leading to unpredictable outcomes.</p> <p>Self-adaptive schema scaffolding (S3) addresses this issue by allowing the model to output a special null token upon which generation for that variable block stops, leaving the remaining slots empty.</p> <h2 id="decoding-strategies">Decoding Strategies</h2> <h3 id="random">Random</h3> <p>A simple baseline is random unmasking, where the positions to commit/unmask at each step are chosen uniformly at random. However in practice, heuristics tend to be more efficient and higher quality.</p> <h3 id="confidence-based-sampling">Confidence-Based Sampling</h3> <p>Confidence-based sampling is a common strategy in iterative denoising or masked sequence generation. In this approach, tokens with high confidence are “locked in,” while low-confidence tokens may be remasked for further refinement.</p> <p>However, this strategy is not always optimal. High-confidence tokens are often syntactic or structurally predictable, which can cause the model to commit to the surface structure of the sequence too early, potentially limiting the flexibility of subsequent generation.</p> <p>A typical way to quantify the confidence of a token at position (i) is:</p> \[c_i = \max_v p_\theta(v \mid \text{context})\] <p>where \(p_\theta(v \mid \text{context})\) is the predicted probability of token \(v\) given the current context, and \(c_i\) represents the confidence score for position \(i\). Tokens with higher \(c_i\) are more likely to be committed, while those with lower confidence can be remasked and reconsidered in future iterations.</p> <p>This method provides a simple and interpretable heuristic for guiding which positions to finalize versus which to refine, balancing stability and flexibility in the generated sequence.</p> <h3 id="entropy-based-sampling">Entropy-Based Sampling</h3> <p>This technique uses entropy as a confidence measure, where lower entropy implies higher confidence. This is often more robust than raw probability thresholds. A common way to calculate the entropy at position \(i\) is:</p> \[H_i = - \sum_v p_i(v) \log p_i(v)\] <p>where \(p_i(v)\) is the probability assigned to token \(v\) at position \(i\). Here, \(H_i\) measures the uncertainty of the model’s prediction: positions with low entropy correspond to confident predictions, while positions with high entropy indicate ambiguity.</p> <h3 id="margin-based-sampling">Margin-Based Sampling</h3> <p>Margin-based sampling uses a second-order confidence measure: we take the difference between the confidence of the most probable token and the second most probable token as the margin, and select only tokens that have a high enough margin.</p> <p>Formally, let \(v_1\) and \(v_2\) be the most probable and second-most probable tokens at position \(i\). The margin is defined as:</p> \[m_i = p_i(v_1) - p_i(v_2)\] <p>where \(p_i(v_1)\) and \(p_i(v_2)\) are the probabilities assigned to these tokens.</p> <p>A higher margin \(m_i\) indicates that the model is strongly favoring the top token over the runner-up, while a small margin suggests uncertainty. During iterative generation, margin-based sampling allows the model to commit tokens with high certainty while deferring those with ambiguous predictions for further refinement.</p> <h3 id="eb-sampler">EB Sampler</h3> <p>Entropy-bounded (EB) sampling typically commits tokens until an entropy budget/constraint is met (e.g. keep committing the lowest-entropy positions until the remaining masked positions have entropy above a target, or until a step-wise budget is exhausted).</p> <h3 id="pc-sampler">PC Sampler</h3> <p>Position-calibrated (PC) samplers add a position-aware calibration term to avoid pathological early commitments to “easy” regions (e.g. always unmasking near the prefix first). Without calibration, models may tend to unmask or commit tokens near the beginning of a sequence first, which can reduce diversity and flexibility in later steps.</p> <p>One way to implement this is to adjust the confidence score of each position with a position-dependent bias:</p> \[\tilde{s}_i = s_i + b(i)\] <p>where \(s_i\) is a base confidence score—such as the negative entropy \(-H_i\) or the raw probability \(c_i\) and \(b(i)\) is a bias or penalty term that depends on the position \(i\).</p> <p>By adding \(b(i)\), positions that are typically “easy” to commit (like the prefix) can be down-weighted, encouraging the sampler to consider less obvious positions first. The calibrated score \(\tilde{s}_i\) is then used to select which positions to commit or remask in the current iteration, promoting a more balanced and robust sequence generation process.</p> <h2 id="unmaskingremasking-strategies">Unmasking/Remasking Strategies</h2> <h3 id="static-low-confidence-remasking">Static Low-Confidence Remasking</h3> <p>In this masking regime, the denoising occurs over K steps. At each step, a fixed number of tokens N/K, where N is the size of the output, are unmasked, usually chosen by a confidence criterion. The low-confidence tokens are remasked.</p> <h3 id="dynamic-low-confidence-remasking">Dynamic Low-Confidence Remasking</h3> <p>A confidence threshold t is set. At a given denoising step, each token is unmasked only if it crosses the threshold t. If too few positions cross the threshold, then a minimum number of highest confidence tokens are unmasked.</p> <h3 id="dilute-unmasking-schedule">Dilute Unmasking Schedule</h3> <p>Instead of committing aggressively every step, the schedule “dilates” commits, by committing fewer tokens early, more in the middle, and fewer near the end, so that more global context can settle before locking in too many tokens.</p> <h2 id="speculative-decoding">Speculative Decoding</h2> <p>Speculative decoding in diffusion models is more challenging than in autoregressive models because generation can happen in parallel, and some models use block-based decoding. Gao et al. <d-cite key="gao2025selfspec"></d-cite> propose Self-Speculative Decoding (SSD) to address these challenges. SSD consists of two main phases: self-drafting and verification.</p> <h3 id="self-drafting">Self-Drafting</h3> <p>In this step, we construct a partial sequence that includes the prompt tokens, the tokens already committed in previous steps, and the currently masked positions. We then perform a single denoising step on this sequence to produce predictions for all masked tokens. This initial prediction step is referred to as self-drafting.</p> <p>If the model uses block-based decoding, the self-drafting procedure is applied within the current block as follows:</p> <ul> <li>Sort the positions in the block by a confidence measure.</li> <li>Select the top-k positions as candidates for verification.</li> <li>If there are fewer than \(k\) positions in the block, extend the selection into subsequent blocks until \(k\) positions are chosen.</li> </ul> <h3 id="verification">Verification</h3> <p>The \(k\) drafted tokens are then verified using a <strong>verification tree</strong>, which efficiently checks multiple token proposals at once. Tokens that pass verification are committed to the sequence, while tokens that fail are remasked and will be reconsidered in later iterations.</p> <p>This two-phase procedure allows speculative decoding to leverage parallel generation while maintaining reliability, committing only those tokens for which the model demonstrates sufficient confidence.</p> <h2 id="hyperparameters">Hyperparameters</h2> <h3 id="generation-length">Generation length</h3> <p>In autoregressive models, the generation length is dynamic for a given query, and generation stops when the <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token is generated. However, in diffusion models, tokens are predicted in parallel, so we typically allocate a maximum output length in advance, and discard tokens after the first <EOS> in the final output. This means that the generation length becomes a hyperparameter. Large generation lengths will be expensive due to the quadratic nature of self-attention.</EOS></p> <h3 id="number-of-denoising-steps">Number of denoising steps</h3> <p>The number of denoising steps K is also a hyperparameter. More steps give the model more chances to revise tokens. If optimizing for latency, K can be lower. If optimizing for task performance, K can be larger.</p> <h3 id="block-length">Block length</h3> <p>In practice, large output sequences are not amenable to being generated in one go. Hence semi-autoregressive (blockwise) diffusion is used. The output sequence is divided into blocks, and each block is generated sequentially. Within each block, tokens are generated through diffusion. Using this method, it becomes possible to perform diffusion across very long horizons.</p> <h3 id="temperature">Temperature</h3> <p>Similar to ARMs, temperature is a key hyperparameter in the dllm paradigm. Increasing temperature not only increases token diversity but also diversity in generation order (Gong et al., 2025) <d-cite key="gong2025diffucoder"></d-cite>.</p> <h3 id="classifier-free-guidance-cfg">Classifier-Free Guidance (CFG)</h3> <p>Classifier-free guidance (CFG) for discrete diffusion combines a conditional prediction with an “unconditional” prediction (often a null prompt or dropped conditioning).</p> <p>A common formulation is:</p> \[z_{\text{cfg}} = z_{\text{uncond}} + s \cdot (z_{\text{cond}} - z_{\text{uncond}})\] <p>where \(z_{\text{uncond}}\) is the unconditional prediction, \(z_{\text{cond}}\) is the conditional prediction, and \(s\) is the guidance scale.Increasing \(s\) strengthens adherence to conditioning, while smaller values typically yield more diverse outputs.</p> <h1 id="pitfalls--solutions">Pitfalls &amp; Solutions</h1> <h2 id="output-length-is-a-hyperparameter">Output Length is a Hyperparameter</h2> <p>Unlike autoregressive models, where the model continues generating output until an end of sequence (<code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>) token is seen, the output length in dLLMs is typically a hyperparameter that is chosen before generation.</p> <p>If the preset output length is too short for a given query, the model may skip steps, be very terse, or just fail entirely. If the preset output length is too long, neural degeneration may occur and the performance may drop. Longer output lengths also results in significantly more computation due to the quadratic nature of self-attention.</p> <p>These problems can be resolved if the model can dynamically adapt its output length for each query. To this end, Li et al. introduce the inventively named DAEDAL (<strong>D</strong>ynamic <strong>A</strong>daptive Length <strong>E</strong>xpansion for <strong>D</strong>iffusion L<strong>a</strong>rge <strong>L</strong>anguage Models), a training-free decoding technique that leverages <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token probabilities to dynamically adjust output length.</p> <p>DAEDAL has 2 stages, an initial global estimate and iterative local mask insertions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/DAEDAL-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/DAEDAL-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/DAEDAL-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/DAEDAL.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="initial-length-adjustment">Initial Length Adjustment</h3> <p>A short initial length (say 64 or 128 tokens) is assigned for generation. The model then goes through a single denoising step to produce its initial predictions. For the last few tokens of the sequence, the probabilities of the <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token are taken and averaged. If the probability exceeds a threshold \(T\), then the current length is likely to be sufficient, otherwise the length is extended by a predetermined amount.</p> <p>This step is repeated until the <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token confidence exceeds the threshold \(T\) or the global maximum length \(L\) is reached.</p> <h3 id="iterative-mask-insertion">Iterative Mask Insertion</h3> <p>During the denoising process, there might be local regions in the output where more elaboration is merited. An example would be a tricky code block where it might serve well to reserve more lines of code for it. To facilitate this, after each denoising step, the lowest-confidence masked positions are taken as expansion points. At these expansion points, multiple <code class="language-plaintext highlighter-rouge">[MASK]</code> tokens are inserted, thus dynamically allowing the model to expand generation in that output region.</p> <p>The authors show that DAEDAL leads to a massive jump in effective token ratio (proportion of tokens in the output sequence actually used for the output) compared to fixed length baselines.</p> <h2 id="denosing-steps-is-a-hyperparameter">Denosing Steps is a Hyperparameter</h2> <p>In most contemporary dLLMs, the number of denoising steps is also a hyperparameter. However, the optimal number of denoising steps depends on the query. The number of denoising steps is analogous to test-time compute in reasoning models, and leads to similar issues that scaling test-time compute encounters: (1) if the number of denoising steps is too low, then the model might not have arrived at the answer yet, and (2) if it is too high, the model may have overshot the answer. This phenomenon of drifting away from the correct answer that was generated during an intermediate denoising step is called temporal oscillation.</p> <p>In order to quantify temporal oscillation, we can use the <em>ever pass</em> rate metric. The ever pass rate is the accuracy of the model as measured across all denoising steps.</p> <p>Let \(N\) be the number of evaluation queries, \(K\) the number of denoising steps, and let<br/> \(\mathrm{Correct}(i,k) \in \{0,1\}\)<br/> indicate whether query \(i\) is solved correctly at step \(k\). Then:</p> \[\mathrm{EverPass} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}\left[\max_{1 \le k \le K} \mathrm{Correct}(i,k) = 1\right].\] <p>A query is counted as correct under this metric if the model produces the right answer at least once during the denoising trajectory. This is contrasted with the full pass rate, which measures accuracy only at the final denoising step:</p> \[\mathrm{FullPass} = \frac{1}{N} \sum_{i=1}^{N} \mathrm{Correct}(i, K).\] <p>Temporal oscillation can then be summarized as:</p> \[\mathrm{Oscillation} = \mathrm{EverPass} - \mathrm{FullPass}.\] <p>Ideally, the model would adaptively choose the number of denoising steps based on query difficulty, similar to thought budgeting in reasoning models. Wang et al. <d-cite key="wang2025timeisafeature"></d-cite> propose an interim approach that leverages intermediate outputs via the concept of temporal semantic entropy.</p> <h3 id="temporal-semantic-entropy">Temporal semantic entropy</h3> <p>For a given query \(q\), the model produces a sequence of intermediate answers across the \(K\) denoising steps. These answers are grouped into clusters based on semantic equivalence. Temporal semantic entropy (TSE) is defined as the entropy of the resulting cluster distribution:</p> \[\mathrm{TSE}(q) = - \sum_{c \in \mathcal{C}} p_c \log p_c,\] <p>where \(p_c\) is the proportion of intermediate answers assigned to cluster \(c\).</p> <p>If all intermediate answers fall into a single cluster, TSE is low. If the model oscillates between semantically distinct answers, TSE increases. Empirically, datasets on which the model performs poorly tend to exhibit higher mean TSE. Individual queries answered correctly typically have lower TSE than those answered incorrectly.</p> <p>TSE can be used to mitigate temporal oscillation and improve final accuracy. Wang et al. also introduce Temporal Self-Consistency Voting, a strategy that selects the final answer via majority vote across all denoising steps.</p> <h3 id="temporal-self-consistency-voting">Temporal Self-Consistency Voting</h3> <p>The model performs majority voting over the solution space. The output from each denoising step is normalized to a semantic form. Outputs are weighted based on which denoising step it came from. The weighting scheme can be:</p> <ol> <li>Fixed - all denoising steps carry the same weight</li> <li>Linear - the weight increases along with the number of steps, i.e the later steps are weighted more</li> <li>Exponential - similar to linear, except that the weight increases exponentially at the end of the sequence</li> </ol> <p>The authors observed that empirically, the exponential weighting scheme is the best performing.</p> <p>If the model overshot the answer due to a longer-than-needed denoising process, then temporal self-consistency can ensure that intermediate solutions still stand a chance of being picked up as the final answer.</p> <h2 id="block-length-is-a-hyperparameter">Block Length is a Hyperparameter</h2> <p>Diffusion over very large output sequences is suboptimal due to both latency issues and inability or difficulty in using KV-caches. Therefore in practice, it is customary to divide the output sequence into fixed-length blocks, where each block is generated sequentially but the tokens inside each block are generated via diffusion. Typically, the number of denoising steps is divided equally among each block.</p> <p>However, having a fixed block size comes with pitfalls. In their paper, Lu et al. <d-cite key="lu2025adablock"></d-cite> showcase two common inefficiencies: (1) late decoding overhead and (2) premature decoding error.</p> <h3 id="late-decoding-overhead">Late Decoding Overhead</h3> <p>Consider an output sequence broken down into 3 blocks. Let’s say the tokens in the second block are high confidence and easy to predict. However they are not predicted until all tokens in the first block have been predicted. If the tokens in the first block are also high confidence, then the model will wastefully perform denoising steps even when there are high confidence tokens outside the block boundary waiting to be unmasked. The authors term this as <em>late decoding overhead</em>.</p> <h3 id="premature-decoding-error">Premature decoding error</h3> <p>On the flip side, with block diffusion, all the tokens in the current block need to be predicted before moving on to the next block. This means that there is a chance of low confidence tokens being locked in prematurely. These tokens can cause the errors to propagate, as they will be used as context for generation of tokens in subsequent blocks. The authors term this as <em>premature decoding error</em>.</p> <p>In order to mitigate these issues, the model should ideally have dynamic block lengths. Lu et al. <d-cite key="lu2025adablock"></d-cite> introduce AdaBlock, an adaptive block size scheduler that leverages token confidence dynamics to draw semantically aware block boundaries.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ConfidenceLandscape-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ConfidenceLandscape-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ConfidenceLandscape-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ConfidenceLandscape.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The authors observed that the confidence dynamics of the output tokens vary across token positions. The physical landscape of output tokens can be divided into three types of regions:</p> <ol> <li>High-confidence plateau - this includes the already decoded tokens and mask tokens in their vicinity.</li> <li>Low-confidence floor - this includes token positions at the end of the output sequence</li> <li>Volatility bands - a band of positions where the confidence is fluctuating.</li> </ol> <p>Active decoding happens in the volatility band. The volatility band encodes local semantic structure. With this in mind, the block size can be made adaptive by dividing blocks based on semantic steps. A semantic step can be a span of tokens that are potentially self-contained, like a reasoning step, a line of code, or a statement.</p> <p>In order to identify semantic step boundaries, we can perform the following:</p> <ol> <li>Identify a set of delimiter tokens (newline, period, etc.) that can convey the end of a semantic unit</li> <li>Slide a window forward from the current generation position</li> <li>Pick the delimiter in the window with the highest confidence</li> <li>If the delimiter with the highest confidence surpasses a threshold \(T\), it is treated as the end of the semantic step, and the block size is adapted such that the block ends at the delimiter</li> <li>If no delimiters exist or none of them have confidence surpassing \(T\), then the default block size is used for the current generation</li> </ol> <p>Because each block corresponds to a self-contained semantic unit, the KV cache representations age more gracefully. This method also helps mitigate the late decoding overhead and premature decoding error issues.</p> <h2 id="confidence-threshold-is-a-hyperparameter">Confidence Threshold is a Hyperparameter</h2> <p>With confidence based decoding, typically the same threshold is employed throughout the generation process regardless if (1) it is an earlier denoising step or a later denoising step or (2) if it is an easy prompt or a difficult prompt.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ConfidenceCurves-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ConfidenceCurves-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ConfidenceCurves-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ConfidenceCurves.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In practice, we could use a dynamic confidence threshold, because confidence dynamics vary through the diffusion phase. It has been observed that mean confidence is low during earlier denoising steps, peaks in the middle, and then becomes low during the final steps again, forming an up-side-down U-shape <d-cite key="shen2025osdt"></d-cite>. The curve of the U-shape depends on the type of task being performed. Shen et al. note that GSM8K’s curve is different from GPQA’s curve, but the confidence dynamics are similar for problems within each dataset. This indicates we can treat the confidence trajectory over denoising steps as a signature for a task type.</p> <p>The confidence signature for a given task can now be calculated by taking into account the confidence over all diffusion steps and blocks. The metric could be mean, median, etc.. This metric can then be treated as the confidence threshold for all instances of the given task. To prevent the confidence levels from being too restrictive, an upper bound \(B\) can be set.</p> <h2 id="information-is-not-carried-over-across-denoising-steps">Information is not carried over across denoising steps</h2> <p>Consider the partially unmasked sequence:</p> <p><code class="language-plaintext highlighter-rouge">Michael [MASK] to New York.</code></p> <p>At denoising step \(p\), suppose the top-3 predicted tokens for the masked position are:</p> <ul> <li><code class="language-plaintext highlighter-rouge">went</code>: 0.4</li> <li><code class="language-plaintext highlighter-rouge">moved</code>: 0.3</li> <li><code class="language-plaintext highlighter-rouge">galloped</code>: 0.1</li> </ul> <p>If the model uses confidence-based unmasking, the <code class="language-plaintext highlighter-rouge">[MASK]</code> token is not unmasked at step \(p\) because none of these candidates exceed the confidence threshold. However, at step \(p + 1\), the model restarts the prediction process from scratch. The information that <code class="language-plaintext highlighter-rouge">went</code> and <code class="language-plaintext highlighter-rouge">moved</code> had relatively high probabilities at the previous step is not retained.</p> <p>This leads to redundant and inefficient computation: the model repeatedly re-evaluates similar candidate sets without leveraging prior signals. Ideally, diffusion-style language models would propagate information across denoising steps, allowing the model to refine or reweight earlier hypotheses instead of discarding them at each iteration.</p> <h3 id="soft-masking">Soft Masking</h3> <p>Hersche et al. <d-cite key="hersche2025softmasked"></d-cite> introduce soft masking, which augments the discrete <code class="language-plaintext highlighter-rouge">[MASK]</code> token with continuous feedback. Instead of treating denoising as a binary decision (unmask or keep masked), each masked position is represented as an interpolation between the <code class="language-plaintext highlighter-rouge">[MASK]</code> embedding and a weighted combination of the top-k token embeddings.</p> <p>This can be expressed as:</p> \[soft embed = (1 - \alpha) embed([MASK]) + \alpha \sum_{j \in \text{top-}k} \tilde{p}_j embed(j)\] <p>where:</p> <ul> <li>\(\alpha\) is calculated by taking the negative entropy of the token probability vector and passing it through a sigmoid to obtain a weight between 0 and 1</li> <li>\(\tilde{p}_j\) is normalized over the top-k tokens so that the weights sum to 1</li> <li>\(k\) is typically 3–4 tokens</li> </ul> <p>If the token distribution is flat, \(\alpha\) is small and the <code class="language-plaintext highlighter-rouge">[MASK]</code> embedding barely changes. If the distribution is peaked, the <code class="language-plaintext highlighter-rouge">[MASK]</code> embedding is mostly replaced by the mean embedding of the top-k tokens.</p> <p>Hersche et al. report that applying soft masking on roughly 80% of denoising steps yields the best results, with most benefits occurring if applied during only the first 20% of steps.</p> <h3 id="credit-score">Credit Score</h3> <p>Wang et al. <d-cite key="wang2025creditdecoding"></d-cite> propose CreditDecoding, which maintains a credit value \(C_t(i, v)\) for each token position \(i\), each token \(v\), and each denoising step \(t\). The credit is updated as:</p> \[C_t(i, v) = \begin{cases} \gamma \cdot C_{t-1}(i, v) + p_t(i)^{\beta}, &amp; \text{if } v = v_t(i) \\ \gamma \cdot C_{t-1}(i, v), &amp; \text{otherwise} \end{cases}\] <p>where:</p> <ul> <li>\(v_t(i)\) is the top-1 token at position \(i\)</li> <li>\(p_t(i)\) is its probability</li> <li>\(\gamma\) is a decay factor in (0, 1)</li> <li>\(\beta\) is an exponent that amplifies mid-range probabilities</li> </ul> <p>Credit scores are incorporated into the logits of the next step:</p> \[\tilde{z}_t(i, v) = z_t(i, v) + \lambda \cdot C_t(i, v)\] <p>where \(\lambda\) controls the strength of the credit prior.</p> <h2 id="entropy-sink-phenomenon">Entropy Sink Phenomenon</h2> <p>Many diffusion LLMs still exhibit semi-autoregressive behavior. Gong et al. (2025) introduced the concept of local versus global AR-ness. Models adapted from autoregressive pretraining retain latent left-to-right dependencies, while models trained from scratch can be more flexible.</p> <p>Confidence-based unmasking—selecting the highest probability or lowest entropy tokens—tends to favor positions near the prefix. The first committed token biases the immediate right neighbor, creating an entropy sink. This bias induces a left-to-right autoregressive (AR) pattern.</p> <h3 id="degeneration-to-ar">Degeneration to AR</h3> <p>Although diffusion models can, in principle, generate tokens in any order, in practice generation often degenerates toward AR behavior. Two metrics quantify this:</p> <ul> <li>Local AR-ness: over a sliding window of length \(k\), the proportion of contiguous tokens generated reflects local AR behavior</li> <li>Global AR-ness: for each unmasked token, if it is the left-most unmasked token in the sequence, it is considered autoregressively generated. The proportion of such tokens gives global AR-ness</li> </ul> <p>Models continually pre-trained from AR bases retain higher AR-ness than models trained from scratch. Empirically, math generation shows high local AR-ness, while code generation shows lower AR-ness. This mirrors human behavior: math is typically solved sequentially, while code is edited in a non-linear fashion.</p> <p>Confidence-based remasking reinforces AR behavior because high-confidence tokens are usually near the prefix. Increasing the sampling temperature decreases AR-ness by flattening token distributions and increasing uncertainty in token commitments.</p> <h1 id="future-directions">Future Directions</h1> <h2 id="latency-play">Latency Play</h2> <p>Currently diffusion models are being promoted as a faster alternative to autoregressive models, as they support parallel token prediction. In practice, they need multiple denoising steps and often blockwise decoding. Latency can be reduced with fewer denoising steps, but it comes at the cost of performance</p> <h2 id="data-play">Data Play</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ScalingLawsA-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ScalingLawsA-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ScalingLawsA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ScalingLawsA.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ScalingLawsB-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ScalingLawsB-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ScalingLawsB-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/ScalingLawsB.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>If we assume a data-limited world where we run out of unique, quality data much sooner than we do compute, dLLMs pose an obvious advantage over ARMs. In data-constrained settings, dLLMs perform much better than ARMs due to better sample efficiency as dLLMs can be trained using repeated data over many epochs with the epochs remaining effective. Repeating data will become a common necessity in cases where data is constrained; however, in ARMs, repeating training data for more than a few epochs will lead to learning plateau and yield diminishing returns. ARMs will fit to the training data after a few epochs and reach saturation and/or overfitting, leading to degraded performance (Ni et al. 2025) <d-cite key="ni2025diffusion"></d-cite>.</p> <p>Diffusion models can be trained on the same data for way more epochs than AR; Prabhudesai et al. <d-cite key="prabhudesai2025diffusion"></d-cite> show that AR models can benefit from data repetition for only around ~4 epochs, while diffusion models can benefit from data repetition for up to ~100 epochs. They also show that diffusion’s best validation loss can occur at dramatically higher epoch counts (e.g., ~500 epochs vs ~50 for AR), suggesting diffusion keeps extracting signal where AR saturates/overfits.</p> <h2 id="reasoning-play">Reasoning Play</h2> <p>In practice, diffusion and auto-regressive modes are likely to co-exist in the foreseeable future. A plausible way to combine these modes together is to use diffusion for reasoning and AR for answer generation.</p> <p>Diffusion can be used for task decomposition, planning, outlines, where revision is natural. After the reasoning step, the AR model can produce a clean left-to-right final response,</p> <p>This aligns with what diffusion is naturally good at (global revision) while keeping AR’s strengths (fast sequential emission, stable length control, and efficient serving).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/Tweets-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/Tweets-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/Tweets-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dllm/Tweets.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="the-case-against-dllms">The Case Against dLLMs</h1> <p>Several critiques have sprung up recently making the case against diffusion LLMs. We will elaborate a few of them in this section.</p> <h2 id="any-order-model-is-inefficient-and-ineffective">Any-order model is inefficient and ineffective</h2> <p>Masked diffusion can be viewed as an any-order model, as it is trained to predict arbitrary masked subsets in any order. However in practice, left-to-right and right-to-left orders are easier to learn. This is due to the Markovian nature of language <d-cite key="sun2025why"></d-cite> , where conditioning on nearby tokens provides more predictive power. An any-order model wastes model capacity on modeling hard and unnatural orders.</p> <h2 id="models-predict-marginals-not-a-joint-distribution">Models predict marginals, not a joint distribution</h2> <p>The denoiser predicts a distribution for each <code class="language-plaintext highlighter-rouge">[MASK]</code> position, conditioned on the unmasked tokens, but not the joint distribution over all masked positions. As a result, sampling multiple tokens in parallel has no guarantee of joint coherence.</p> <h2 id="training-inference-mismatch">Training-Inference mismatch</h2> <p>During training, the model is taught to predict from arbitrary masking patterns. However, during inference, denoising is typically done via confidence-based measures, which causes tokens closer to already unmasked tokens to be generated first, making it nearly autoregressive. Once inference becomes AR-like, most situations covered during training will not actually occur at inference time.</p> <h1 id="conclusion">Conclusion</h1> <p>In summary, at present diffusion LLMs are best viewed as a complementary modeling paradigm rather than a universal replacement for autoregressive models. They offer clear advantages in certain regimes: the masked diffusion objective naturally supports infilling and in-place revision, and recent results indicate that diffusion-based training can be significantly more data-efficient than autoregressive training when the amount of unique high-quality data is limited but can be repeated many times. At the same time, current masked diffusion formulations face structural limitations. They optimize over many token orders despite language exhibiting strong directional biases, and their decoding procedures often behave in a semi-autoregressive manner in practice, reducing the practical benefits of full any-order generation.</p> <p>Taken together, these observations suggest a more nuanced role for diffusion LLMs. In settings where data is the primary bottleneck and compute is relatively abundant, or where flexible infilling and structured editing are central requirements, dLLMs are a compelling choice. In contrast, for latency-sensitive, streaming, or purely left-to-right generation workloads, autoregressive models remain highly competitive and often preferable. A promising direction for future systems is therefore hybrid: using diffusion-style models for planning, reasoning, or structural refinement, and relying on autoregressive models for efficient, stable surface realization.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Diffusion large language models (dLLMs) provide an alternative to autoregressive Transformers, supporting parallel token generation and flexible infilling. They excel in structured, long-horizon, or data-constrained settings, though challenges remain with output length, denoising, and blockwise generation. Hybrid approaches combining diffusion for reasoning and AR for generation show promise.]]></summary></entry><entry><title type="html">Dynamics of Forgetting</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/dynamics-of-forgetting/" rel="alternate" type="text/html" title="Dynamics of Forgetting"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/dynamics-of-forgetting</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/dynamics-of-forgetting/"><![CDATA[<h2 id="introduction">Introduction</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Most of our understanding of the world is built through a continuous interaction with our surroundings.</p> <p>This marks a very distinct difference between Artificial and Human intelligence: while modern AI models are stuck in a training / inference dichotomy, human intelligence (and, more generally, intelligence found in nature) blurs the line between the two, integrating a subconscious process of accumulation and processing of new information throughout our interaction with the world.</p> <p>This makes continual learning, to us, the most pressing problem in modern Artificial Intelligence.</p> <p>Most existent continual learning methods (<d-cite key="kirkpatrick_overcoming_2017"></d-cite><d-cite key="saha_gradient_2020"></d-cite><d-cite key="wang_training_2021"></d-cite><d-cite key="zenke_continual_2017"></d-cite><d-cite key="aljundi_memory_2018"></d-cite><d-cite key="farajtabar_orthogonal_2020"></d-cite><d-cite key="lopez_paz_gradient_2017"></d-cite><d-cite key="chaudhry_efficient_2019"></d-cite><d-cite key="riemer_learning_2019"></d-cite><d-cite key="shin_continual_2017"></d-cite>) revolve around the idea of making a specific subset of the training data <em>harder to forget.</em> Through either replay buffers, reduced learning rates on the parameters that are more important to interpret <em>a specific</em> subset of data, or orthogonalization of the updates to task-specific directions, the model is strongly penalized whenever relevant information is forgotten.</p> <p>All of these methods might work in specific multi-task scenarios, but the interactions that we, as humans, experience with the surrounding world are rarely ever categorizable in separate tasks.</p> <p>To begin pondering the possibility of human-like lifelong learning, we can’t rely on manually “saving” a small amount of specific subsets of data from catastrophic forgetting: we need a learning paradigm that allows to simply <em>not forget anything,</em> and learn new things without damaging previous knowledge.</p> <p>This blogpost will propose a strongly personal point of view on:</p> <ul> <li>Why do models forget, and whether we can distinguish between different types of forgetting;</li> <li>How does the core phenomenon of catastrophic forgetting look like from a spectral perspective;</li> <li>A possible approach towards more graceful optimization, to preserve learned structure.</li> </ul> <h2 id="why-do-models-forget">Why Do Models Forget</h2> <p>Catastrophic forgetting stems as an intrinsic consequence of how models are optimized. Gradient descent would like to minimize a loss function over a dataset of samples. Since this is usually computationally unfeasible, <em>Stochastic</em> Gradient Descent only concerns itself with a tiny, random portion of the data, and tunes the parameters to minimize the objective function on that subset of data. This means that each iteration has only one goal: optimize the parameters to predict the current batch as accurately as possible, as if no other batch ever existed.</p> <p>This leads to a constant flow of <em>construction</em> and <em>destruction</em> of structure in the weight matrices, that hopefully balances itself over enough training steps with large enough batches.</p> <p>To mitigate the damage of this oversimplification of the learning paradigm, two main practices stuck around over the years:</p> <ul> <li>Learning rate schedulers apply a very simple yet effective heuristic: if the model has been updated a lot of times, then probably it has built a lot of structure in the parameters that is worth preserving. Therefore, it makes sense to lower the magnitude of the updates to avoid destroying that structure.</li> <li>Better optimizers, like Adam, add two improvements: i) stabilize the trajectory of the updates, by accelerating descent through consistent paths, and decelerating through inconsistent ones; ii) optimize more strongly parameters that have been optimized fewer times.</li> </ul> <h2 id="setting-the-stage">Setting the Stage</h2> <p>This kind of techniques can be seen, from a continual learning point of view, as heuristics that try to balance the ability of a model to keep learning from new examples, while mitigating the amount of destruction that a new batch induces on the information that was previously learned. We like to think of this balance as a trade-off between <em>stability</em> and <em>plasticity</em> (<d-cite key="kim_achieving_2023"></d-cite><d-cite key="chen_stabilityplasticity_2023"></d-cite><d-cite key="jung_new_2023"></d-cite><d-cite key="lu_rethinking_2025"></d-cite>). The former represents the ability of a model to preserve learned structure (i.e: circuits, patterns…), the latter stands for the potential to learn new structures when needed (for example, when a change in the distribution of training samples occurs).</p> <p>In the traditional optimization scenario, this trade-off looks like a zero-sum game: stronger optimization (higher learning rate), increases plasticity at the cost of heavier forgetting; more delicate optimization (lower learning rate), increases stability by reducing the model’s ability to learn new information.</p> <p>We think that, to better navigate this tradeoff, a change in the optimization paradigm is needed. We like to think of this in terms of <em>gracefulness</em> of the updates. A graceful update is one that improves performance on the current batch without overwriting or degrading previously accumulated structure in the weights. There is a very fine line that keeps this from being an intrinsically contradictory statement: a good update should, by definition, update the model’s perception of the world (the training data) in a generalizable way; so how could it work without altering and refining previously learned connections and circuits?</p> <h2 id="observing-spectral-training-dynamics">Observing Spectral Training Dynamics</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover2-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover2-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To answer, we need to go deeper into the dynamics of learning and refining knowledge. Each optimization step computes update directions that are entirely specific to the current batch. Intuitively, we can think of an update as a mixture of two macro-types of operations:</p> <ul> <li><em>Refining present structure</em>: existing information encoded in the weights is updated according to its performance on the current batch;</li> <li><em>Building new structure</em>: a new encoding is built ad-hoc for the data in this batch.</li> </ul> <p>The first targets encoded structure that is strongly learned and optimized and tries to tweak it to make it work on the current batch. The second takes a weak, raw part of the network and begins to shape and refine it using the data in the current batch.</p> <p>We can try to observe these behaviors by decomposing with SVD both a weight matrix</p> \[W = U_w \Sigma_w V^T_w\] <p>and the update applied to it at each optimization step,</p> \[\Delta = U_\Delta \Sigma_\Delta V^T_\Delta\] <p>and plot the alignment between the $U_w$ vectors of the weight and the $U_\Delta$ vectors of the update.</p> \[M_{i, j} = \frac{|U_w^i \cdot U_\Delta^j|}{\left\lVert U_w^i \right\rVert \|U_\Delta^j\|}\] <p>An alignment between strong update directions and strong weight directions shows destructive interference of an already highly optimized structure in the weights. Alignment between top update directions and weak weight directions represents the second type of update, in which new structure is being built to encode new information.</p> <p>Let’s now observe an EMA of this matrix while training a 2-layer MLP on MNIST with Adam.</p> <div class="l-page"> <iframe src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-dynamics-of-forgetting/video1.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <div class="caption"> Left, top: singular values of the weight matrix; Left, bottom: singular values of the update proposed by Adam for the same matrix; Center: heatmap of alignment between left singular vectors of the update and left singular vectors of the weight; Right: Validation Accuracy on MNIST. The high values concentrated on the top left of the heatmap show strong alignment between the most important spectral components of the update and the weight. In other words, the optimizer focuses mostly on updating structures which are already very strong. </div> <p>We clearly see that, after an initial stage of randomness, the training quickly converges to always updating a few directions in weight space.</p> <p>This behavior is even more visible when we reduce the hidden dimension of the linear layer analyzed to 2. The training clearly switches from a <em>structure building</em> state, in which updates are not necessarily aligned with the top directions in the weight matrix, to a <em>refining</em> state, in which the update assigns its focus proportionately to the importance of each direction. This switch also synchronized with the moment in which the validation accuracy’s derivative drops below 1 and the model enters a training phase of much slower improvement.</p> <div class="l-page"> <iframe src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-dynamics-of-forgetting/video2.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <h2 id="two-types-of-forgetting">Two Types of Forgetting</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover3-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover3-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/cover3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>So what causes forgetting, exactly?</p> <p>Our previous distinction between two macro-types of updates leads us to think about forgetting in terms of two easily distinguishable patterns. As an example, let’s think of the case of training a simple MNIST classifier.</p> <ol> <li> <p><em>The current sample destroys structure that was learned on different data.</em></p> <p>In our example, this could look like the model encountering a new digit, and applying to it a series of weights built to recognize other digits, leading to a wrong prediction. The weight update based on this new sample will interfere with the existing circuits built from the previous digits, overwriting weights that were needed to recognize them.</p> </li> <li> <p><em>The model learns entirely new structure on the current batch of data.</em></p> <p>Here, the model might see a new digit and build ad-hoc structure to encode / recognize it. These new paths might now incorrectly fire for other digits as well, interfering with their prediction.</p> </li> </ol> <p>In both cases, the problem consists in the model learning something on the current data which doesn’t apply to different data, but the approach to mitigate these two phenomena is drastically different. The first can be solved by applying updates that preserve the current structure. The second cannot.</p> <p>Most continual learning methods, as well as the rest of this blogpost, are only concerned with the first <em>(destructive)</em> type of forgetting, which is much easier to address.</p> <p>We believe that analyzing and determining how much of each of these types of forgetting actually occurs at a given time in a continual learning procedure is one of the most interesting open problems in continual learning.</p> <p>To observe spectral behavior in presence of strong forgetting, we can look at the alignment matrix when switching training tasks from MNIST to Fashion-MNIST.</p> <div class="l-page"> <iframe src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-dynamics-of-forgetting/video3.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <p>It seems that, when switching to the new task of Fashion-MNIST:</p> <ul> <li>The top 3 components of the update switch from targeting already strong structures to building weaker ones (in the first 3 columns of the matrix, the “heat” disappears from the top and shifts downward). This would represent type 2, or <em>constructive</em> forgetting.</li> <li>The remaining important components of the update keep targeting already strong structure (a strong heat spot remains visible in the top-left sector of the matrix). This would seem to indicate type 1, or <em>destructive</em> forgetting.</li> </ul> <p>So, are we able to do something to at least mitigate the <em>destructive</em> forgetting that happens at each optimization step?</p> <h2 id="towards-spectrally-graceful-updates">Towards (Spectrally) Graceful Updates</h2> <p>Given this (very raw and certainly incomplete) analysis, we can try to intervene by observing the spectral characteristic of each weight matrix, and manually tweaking each update to make it more graceful. Ultimately, we want to <em>refuse</em> components of the update that wrongfully penalize weights that were optimized for very different data.</p> <p>Let’s start with an oversimplification and divide the spectral decomposition of a matrix into low-rank (or strong) components and high-rank (or weak) ones. Let us also assume that a low-rank update on a direction changes it strongly, while a high-rank update only refines it slightly.</p> <p>Now we can define a policy that decides which components of an update can be allowed, and which ones need to be refused as potentially destructive, based on their alignment with the corresponding singular vectors of the weight matrix.</p> <p>A very simple example policy could look as follows:</p> <ul> <li>Strong updates on strong directions are clearly destructive of previously accumulated knowledge: we want to refuse them;</li> <li>Strong updates on weak directions are exactly what we want: they build new structure for the new data without destroying old ones;</li> <li>Weak updates on strong directions are also probably ok, as they only slightly refine strongly optimized weights;</li> <li>Weak updates on weak structures are hard to characterize: they might be noise, or very low-importance updates in general. For this example, we refuse them.</li> </ul> <p>We can implement this policy and immediately see the structure we are enforcing in the alignment matrix: our policy divides the matrix into four quadrants and only allows updates belonging to two of the four possible combinations (top-right and bottom-left).</p> <div class="l-page"> <iframe src="/2026-iclr-blogpost-earth4d/assets/html/2026-04-27-dynamics-of-forgetting/video4.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <h2 id="going-deeper">Going Deeper</h2> <p>A matrix-wise intervention on modern deep learning models can be extremely impactful, with the <a href="https://kellerjordan.github.io/posts/muon/">Muon optimizer</a> being the prime example. Yet, we can’t but notice that models will inevitably encode information in the composition of their layers, their <em>circuitry</em>, rather than just atomically in the weights of each. This exposes a clear weak point in our technique so far: it only tries to identify and preserve structure in each matrix individually, but it never looks at their composition.</p> <p>While we’re still far from having a practical solution to this, we have some initial approaches that seem to bring improvements over the previous update conditioning function (or <em>post-conditioner</em>, as we like to call it).</p> <p>Our deeper approach shares the same core idea as the previous one, but it does so taking into account a key element: cross-layer alignment.</p> <p>Let</p> \[W^{(l)} = U^{(l)}\Sigma^{(l)}V^{(l)T} , \qquad \Delta^{(l)} = U_\Delta^{(l)}\Sigma_\Delta^{(l)}V_\Delta^{(l)T}\] <p>be a weight matrix for layer $l$ in a model and the corresponding update proposed by a chosen optimizer, such as AdamW or Muon</p> <p>In a matrix-wise post-conditioner, we use some function $f(W^{(l)},\Delta^{(l)})$ to re-scale the singular sub-spaces spanned by $\Delta^{(l)}$, so that the update is not destructive.</p> <p>For this post-conditioner, we instead re-scale them based on a function</p> \[f_{comp}(W^{(l-1)},\Delta^{(l-1)}, W^{(l)},\Delta^{(l)}, W^{(l+1)},\Delta^{(l+1)}).\] <p>Of course, using $l+1$ and $l-1$ implies a notion of ordering of the weight matrices, which is definitely not trivial to define in a more complex network, such as an LLM.</p> <p>The intuition of $f_{comp}$ is that the subspaces of $\Delta^{(l)}$ are now also rescaled based on the alignment matrices</p> \[T^{(l+1)} = V^{(l+1)T}U^{(l)} , \qquad T^{(l-1)} = V^{(l)T}U^{(l-1)}.\] <p>The core insight is as follows: if incoming and outgoing spectral spaces align, then there is sign of structure in the weights, and we don’t want to be updating it too aggressively, so we reduce the magnitude of the update based on how aligned they are.</p> <p>So, can these style of approaches actually mitigate destructive updates and help in the continual learning setting?</p> <h2 id="experimental-results">Experimental Results</h2> <p>Truth is, we don’t really know yet. In some specific scenarios, our preliminary results seemed extremely encouraging; in others, lackluster.</p> <p>We present here a small collection of results, to be taken with a nice pinch of salt, which highlight the intrinsic difficulty of a proper navigation of the Stability-Plasticity trade-off.</p> <p>We experiment with our post-conditioning technique across two distinct scenarios:</p> <ul> <li> <p><strong>Toy model: 2-layer MLP trained on MNIST</strong></p> <p>We train a tiny 2-layer MLP from scratch on two subsequent classification tasks. This lets us study how our method’s hyperparameters behave across settings when training from scratch.</p> </li> <li> <p><strong>LLMs: sequential SFT across 3 datasets</strong></p> <p>We fine-tune a Transformer-based Large Language Model with Supervised Fine-Tuning on three different datasets in sequence. We evaluate accuracy on all three datasets throughout training, and measure forgetting and learning metrics for all checkpoints. This setting lets us monitor how the technique scales and how it interacts with sparsity in large models.</p> </li> </ul> <p>In our experiments, we call <em>softmask</em> the matrix-wise post-conditioner, and <em>compsoft</em> its circuit-aware counterpart.</p> <h3 id="toy-model---mlp-on-mnist">Toy Model - MLP on MNIST</h3> <p>We train on subsets of MNIST, where each task is binary classification over two digits. The first task is classifying digits $1$ vs. $2$, and the second task is classifying digits $3$ vs. $4$. Once accuracy reaches $0.95$ on the first task, the second task is activated and the model is trained on the new digit pair. We train with a constant learning rate.</p> <p>Here we report <strong>AccΣ := Acc1 + Acc2</strong> , representing the sum of the accuracies of the two tasks at the end of the training steps.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig1-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig1-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="llms---sequential-sft-on-qa-tasks">LLMs - Sequential SFT on Q&amp;A Tasks</h3> <p>We perform Supervised Fine-Tuning on <strong>Qwen3 0.6B</strong> (<d-cite key="yang2025qwen3"></d-cite>) and <strong>Llama 3.2 1B</strong> (<d-cite key="grattafiori2024llama"></d-cite>) on a sequence of three distinct datasets. As training progresses, we evaluate the model’s capabilities on all tasks for every checkpoint. The resulting accuracies let us gauge how well training navigates the robustness–plasticity tradeoff: an ideal model learns strongly while retaining high accuracy on previous tasks.</p> <p>We train sequentially on three Q&amp;A datasets (inspired by <d-cite key="shenfeld2025rl"></d-cite>):</p> <ul> <li>Open Reasoner-zero (<d-cite key="hu2025open"></d-cite>)</li> <li>SciKnowEval (<d-cite key="feng2024sciknoweval"></d-cite>)</li> <li>HellaSwag (<d-cite key="zellers2019hellaswag"></d-cite>)</li> </ul> <p>Here, we plot the validation accuracy of the model on each of the three datasets throughout the training, when using a fixed learning rate of $4 \times 10^{-5}$:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig2a-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig2a-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig2a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig2a.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>And the same with a learning rate of $1 \times 10^{-5}$:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig2b-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig2b-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig2b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig2b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It is crucial to notice how the post-conditioning method strongly outperforms Adam, when used with a higher learning rate, but it starts losing when the learning rate is tuned to a level that is optimal for the task.</p> <p>We can get an idea of the model’s ability to navigate the stability-plasticity tradeoff by plotting the last task’s accuracy (plasticity) vs an average of the accuracies for the previous tasks (stability) at the end of the training.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig3a-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig3a-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig3a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig3a.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig3b-480.webp 480w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig3b-800.webp 800w,/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig3b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-dynamics-of-forgetting/fig3b.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="final-notes">Final Notes</h2> <p>We are publishing this blogpost with the main intent of conveying our point of view on continual learning: how it is a core distinguishing factor between human and artificial intelligence, how catastrophic forgetting plays a central role in the optimization procedure of a model, even outside of traditional sequential multi-task scenarios, and how we believe that forgetting is made of two very different dynamics, one of which is commonly addressed by most CL methods, the other still largely unexplored.</p> <p>We argue that modern continual learning methods cannot rely on knowing what knowledge must be retained, and should instead be capable of preserving existing structure in the weights.</p> <p>We test a couple early implementations in two distinct settings, where we find some mixed results, which we hope will stimulate discussion in the community and help us develop on this first step towards producing gracefully updating, continually learning models. Overall, we think the point stands: we must find a new way to build non-destructive updates that respect the model’s learned structure. We can’t wait to know what you think about it.</p> ]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[We analyze catastrophic forgetting through spectral decompositions of weights and updates, revealing when optimization refines existing circuits versus builds interfering new ones. Leveraging this, we design spectral techniques that suppress destructive update components while preserving structure.]]></summary></entry><entry><title type="html">Earth4D: Production-Ready Space-Time Positional Encoding for World Models</title><link href="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/earth4d-world-models/" rel="alternate" type="text/html" title="Earth4D: Production-Ready Space-Time Positional Encoding for World Models"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/earth4d-world-models</id><content type="html" xml:base="https://qhuang62.github.io/2026-iclr-blogpost-earth4d/blog/2026/earth4d-world-models/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Imagine predicting wildfire risk across California’s 163,000 square miles using nothing but coordinates: no satellite imagery, no weather data, no topography. Just latitude, longitude, elevation, and time. Impossible?</p> <p>That’s what we thought too—until Earth4D proved otherwise.</p> <p>World models—AI systems that learn to simulate and predict complex spatiotemporal dynamics—have emerged as a powerful paradigm for understanding our planet. From climate forecasting to disaster response, these models traditionally rely on massive multimodal datasets: satellite imagery, weather reanalysis, topographic maps, and sensor networks. The assumption has been that <strong>rich inputs are necessary for rich predictions</strong>.</p> <p>But at the heart of all Earth observation data lies a more fundamental structure: <strong>space and time</strong>. Every measurement, every pixel, every sensor reading exists at specific coordinates (x,y,z,t). What if we could capture the essence of Earth’s dynamics by encoding this 4D spatiotemporal structure more effectively?</p> <p>This is the central question behind <strong>Earth4D</strong>, a production-ready 4D space-time positional encoder developed as part of the DeepEarth world model. Earth4D extends multi-resolution hash encoding from 3D graphics to four dimensions, achieving surprising results:</p> <ul> <li><strong>Surpasses multimodal foundation models</strong> using only (x,y,z,t) coordinates—no satellite imagery, weather data, or topography required</li> <li><strong>99% parameter reduction</strong> (724M → 5M) with 4× training speedup while maintaining strong performance</li> <li><strong>Planetary-scale coverage</strong> from sub-meter to continental scale, with temporal precision from sub-second to centuries</li> </ul> <p>More importantly, Earth4D challenges a fundamental assumption in world modeling: rather than requiring massive multimodal pretraining, we can achieve state-of-the-art performance by learning rich spatiotemporal representations from coordinates alone. The key is getting the positional encoding right.</p> <h3 id="earth4d-in-the-deepearth-world-model">Earth4D in the DeepEarth World Model</h3> <p>Earth4D is a core component of <strong>DeepEarth</strong>, a self-supervised multi-modal world model for planetary-scale Earth observation. While DeepEarth can process diverse data types—satellite imagery, sensor readings, text descriptions—its secret weapon is Earth4D’s rich spatiotemporal encoding.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/deepearth_main_figure.png" alt="DeepEarth Architecture Overview" style="width: 100%;"/> <figcaption><b>DeepEarth World Model Architecture.</b> Multi-modal data (images, text, sensor data) sampled around spatiotemporal events are encoded by modality-specific encoders and fused with Earth4D space-time embeddings. These universal tokens are jointly processed through an autoencoder that learns to reconstruct and simulate masked data. Earth4D provides the spatiotemporal grounding that enables the model to reason about where and when phenomena occur. (Figure from DeepEarth paper)</figcaption> </figure> <p>In this blog post, we focus specifically on <strong>Earth4D</strong>—the 4D positional encoder—and demonstrate that even in isolation, without multimodal data, it achieves remarkable performance. This validates that Earth4D captures genuinely useful spatiotemporal structure, not just auxiliary context for other modalities.</p> <hr/> <h2 id="building-blocks-what-we-built-upon">Building Blocks: What We Built Upon</h2> <p>Earth4D combines several existing techniques in a novel way for planetary-scale Earth observation. Before diving into our work, it’s important to acknowledge the foundational methods we adapted:</p> <p><strong>InstantNGP (Müller et al., 2022)</strong><d-cite key="muller2022instant"></d-cite>: Introduced multi-resolution hash encoding for 3D neural graphics. Their insight: compress spatial features into fixed-size hash tables across multiple resolution levels. We extend this from 3D to 4D.</p> <p><strong>Grid4D (Xu et al., 2024)</strong><d-cite key="xu2024grid4d"></d-cite>: Pioneered decomposing 4D space-time into four 3D grids (xyz, xyt, yzt, xzt) for dynamic Gaussian splatting. We adopt their decomposition strategy wholesale, adapting it from computer graphics to Earth observation.</p> <p><strong>Learned Hash Probing (Takikawa et al., 2023)</strong><d-cite key="takikawa2023compact"></d-cite>: Developed at NVIDIA Toronto AI Lab to intelligently resolve hash collisions through learned probe offsets. We apply their technique to enable extreme compression for edge deployment.</p> <p><strong>Our contribution</strong>: We didn’t invent these components. Instead, Earth4D demonstrates that combining these graphics techniques—originally designed for rendering virtual scenes—works remarkably well for modeling our actual planet. The novelty is in the application domain, scale (planetary), and empirical validation (state-of-the-art ecological forecasting with coordinates alone).</p> <hr/> <h2 id="the-positional-encoding-challenge">The Positional Encoding Challenge</h2> <h3 id="why-earth-observation-needs-4d-encoding">Why Earth Observation Needs 4D Encoding</h3> <p>Earth observation data presents unique challenges that distinguish it from typical deep learning tasks:</p> <p><strong>Continuous spatiotemporal coordinates</strong>: Unlike images with discrete pixel positions or text with sequential tokens, Earth data exists in continuous 4D space-time. A wildfire measurement at (37.77°N, -122.42°W, 50m elevation, March 23 2023 8:58 AM) needs precise encoding.</p> <p><strong>Extreme scale variation</strong>: We need to reason about phenomena at vastly different scales simultaneously—a climate model might track both individual storms (10-100km) and global circulation patterns (10,000km+). Temporal scales range from seconds (lightning strikes) to decades (climate change).</p> <p><strong>Planetary coverage</strong>: Unlike 3D graphics which operate in local coordinate frames, Earth observation requires global consistency. The same encoding must work for data from San Francisco, Sydney, and the South Pole.</p> <p><strong>Memory constraints</strong>: Processing planet-scale data quickly exhausts GPU memory. A naive grid representation at 1-meter resolution would require \(10^{18}\) grid cells globally—completely infeasible.</p> <h3 id="limitations-of-existing-approaches">Limitations of Existing Approaches</h3> <p><strong>Sinusoidal positional encodings</strong><d-cite key="vaswani2017attention"></d-cite>, while elegant for transformers, provide fixed basis functions that cannot adapt to complex spatiotemporal patterns in Earth data. They work well for sequential data but struggle with the intricate multi-scale structure of geospatial phenomena.</p> <p><strong>Learned embeddings</strong> require discretizing continuous coordinates. While Vision Transformers<d-cite key="dosovitskiy2021image"></d-cite> discretize images into patches, Earth observation data doesn’t have natural “patch” boundaries. Discretization either loses fine-grained resolution or creates memory-prohibitive lookup tables.</p> <p><strong>3D multi-resolution hash encoding</strong><d-cite key="muller2022instant"></d-cite> (InstantNGP) solved many of these problems for neural graphics by using hash tables to achieve memory-efficient multi-resolution encoding. However, it was designed for static 3D scenes. Earth observation fundamentally requires modeling <strong>how</strong> spatial patterns evolve <strong>over time</strong>—a 4D problem.</p> <p>Extending to 4D isn’t trivial. A naive 4D grid would explode memory requirements. Simply treating time as another spatial dimension loses the distinct characteristics of temporal dynamics (irreversibility, causality, different resolution requirements).</p> <p>What we need is a 4D encoding that:</p> <ol> <li>Handles continuous coordinates without discretization</li> <li>Scales efficiently to planetary coverage</li> <li>Captures multi-resolution structure in both space and time</li> <li>Adapts to data through learning</li> <li>Fits in GPU memory</li> </ol> <p>Earth4D addresses all of these requirements.</p> <hr/> <h2 id="earth4d-architecture">Earth4D Architecture</h2> <h3 id="decomposed-spatiotemporal-representation">Decomposed Spatiotemporal Representation</h3> <p>Earth4D’s core innovation is a <strong>decomposed 4D encoding</strong> that separates spatial and temporal structure while capturing their interactions. Rather than a single 4D hash grid (which would be memory-prohibitive), Earth4D uses four 3D grids:</p> <ol> <li><strong>XYZ Grid</strong>: Pure spatial encoding in Earth-Centered Earth-Fixed (ECEF) coordinates</li> <li><strong>XYT Grid</strong>: Equatorial plane + time</li> <li><strong>YZT Grid</strong>: 90°E meridian plane + time</li> <li><strong>XZT Grid</strong>: Prime meridian plane + time</li> </ol> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/earth4d_spacetime_encoder.png" alt="Earth4D Space-Time Positional Encoding" style="width: 100%;"/> <figcaption><b>Earth4D Space-Time Positional Encoding.</b> A planetary-scale 4D encoder with fully decomposable spatio-temporal representation. Geographic coordinates (latitude, longitude, elevation) are converted to Earth-Centered Earth-Fixed (ECEF) coordinates and normalized. Four grids (xyz, xyt, yzt, xzt) are each learned in 3D space and computed in parallel. Each grid has multiple resolution levels, enabling deep learning of complex joint distributions in multi-modal data across space-time scales. (Figure from DeepEarth paper)</figcaption> </figure> <p><strong>Credit where credit is due</strong>: This decomposition strategy comes directly from <strong>Grid4D</strong><d-cite key="xu2024grid4d"></d-cite> (Xu et al., 2024), developed for high-fidelity dynamic Gaussian splatting. Grid4D pioneered the idea of decomposing 4D space-time into four 3D grids (xyz, xyt, yzt, xzt) rather than using a single 4D grid. We adapted their decomposition approach from computer graphics to planetary-scale Earth observation, but the core architectural insight is theirs.</p> <p>By using three orthogonal spatiotemporal projections, we capture how spatial patterns evolve over time from different perspectives. The XYZ grid provides pure spatial context, while XYT, YZT, and XZT encode temporal dynamics in complementary subspaces—exactly as Grid4D designed.</p> <p><strong>Why ECEF coordinates?</strong> We use Earth-Centered Earth-Fixed (ECEF) coordinates internally rather than latitude/longitude because:</p> <ul> <li>ECEF provides uniform spatial hashing globally (no polar singularities)</li> <li>Distances are Euclidean, making interpolation consistent</li> <li>3D Cartesian coordinates align naturally with hash encoding</li> </ul> <p>Input coordinates (latitude, longitude, elevation, time) are automatically converted to ECEF and normalized to \([-1, 1]\).</p> <h3 id="multi-resolution-hierarchy">Multi-Resolution Hierarchy</h3> <p>Each of the four grids operates at multiple resolution levels simultaneously. This multi-resolution structure is crucial for capturing both local details and global patterns.</p> <p>At level \(L\), the grid resolution is:</p> \[r_L = b \cdot g^L\] <p>where \(b\) is the base resolution (typically 32) and \(g\) is the growth factor (typically \(\sqrt{2}\)). With 24 levels (default configuration), spatial resolution ranges from:</p> <ul> <li><strong>Level 1</strong>: 398.2 km/cell (continental scale)</li> <li><strong>Level 12</strong>: 194.4 m/cell (city scale)</li> <li><strong>Level 24</strong>: 4.75 cm/cell (sub-meter precision)</li> </ul> <p>Temporal resolution similarly spans from years to sub-second precision.</p> <h3 id="hash-encoding-mechanics">Hash Encoding Mechanics</h3> <p><strong>Intuition first</strong>: Think of Earth4D like a hierarchical address system. At the coarsest level, we divide the planet into large regions (like countries). At finer levels, we divide into cities, neighborhoods, streets, and buildings. Each location gets multiple “addresses” at different zoom levels.</p> <p>The challenge: storing a unique feature for every possible location at every resolution level would require astronomical memory. The solution: <strong>hash encoding</strong> compresses this into a fixed-size table using a clever mathematical trick.</p> <p><strong>The process</strong> at each resolution level \(L\):</p> <ol> <li> <p><strong>Discretize</strong>: Convert continuous coordinates to grid positions <code class="language-plaintext highlighter-rouge">pos_grid = floor(coordinate × resolution_L)</code></p> </li> <li><strong>Index or Hash</strong>: <ul> <li>Coarse levels (few grid cells): Store directly, no collisions</li> <li>Fine levels (many grid cells): Hash multiple positions to the same memory slot</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if grid_size ≤ hashmap_size:
    index = pos_grid.x + pos_grid.y × stride_y + pos_grid.z × stride_z
else:
    index = hash(pos_grid) mod hashmap_size
</code></pre></div> </div> </li> <li><strong>Interpolate</strong>: Blend features from 8 surrounding grid corners (trilinear interpolation)</li> </ol> <p>The hash function mixes coordinates using XOR with large primes:</p> \[\text{hash}(\mathbf{p}) = \bigoplus_{d=1}^{D} p_d \cdot \pi_d \pmod{T}\] <p><strong>Why does this work?</strong> The hash function scrambles nearby positions to distant memory locations, preventing clusters of similar coordinates from competing for the same slot. Think of it like distributing people across hotel rooms—random assignment prevents overcrowding.</p> <p><strong>Smoothstep interpolation</strong> \((S(t) = 3t^2 - 2t^3)\) provides C¹ continuous gradients, better than linear interpolation for smooth Earth phenomena like temperature fields or elevation gradients.</p> <p>The final output concatenates features from all grids and levels:</p> <ul> <li>4 grids × 24 levels × 2 features = <strong>192D embedding</strong> per (x,y,z,t) coordinate</li> </ul> <p>This entire process runs on GPU via custom CUDA kernels, enabling <strong>massively parallel encoding</strong> of millions of coordinates simultaneously.</p> <hr/> <h2 id="the-hash-collision-problem">The Hash Collision Problem</h2> <h3 id="understanding-hash-collisions">Understanding Hash Collisions</h3> <p>Hash encoding’s memory efficiency comes with a tradeoff: <strong>hash collisions</strong>. When the grid size exceeds the hash table size, multiple different spatial positions can map to the same hash index.</p> <p>For example, with a hash table size of \(2^{22}\) (4 million entries) and level 24 grid resolution of \(2^{28}\) cells, only 1 in 64 grid cells gets a unique hash entry. The other 63 collide.</p> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/hash_collision_1m_simulation.png" alt="Earth4D Hash Collision Analysis" style="width: 100%;"/> <figcaption><b>Earth4D Hash Collision Analysis.</b> Simulation results for 1M points across 10 spatiotemporal distribution scenarios (rows) and 24 resolution levels (columns) for all four grids (XYZ, XYT, YZT, XZT). Yellow indicates high collision rates, purple indicates low collision rates. Fine levels (right side, high resolution) show expected 2-4% collision rates across most scenarios. Extreme spatial/temporal clustering scenarios show higher collisions at intermediate levels where grid resolution exceeds hash table capacity. (Figure adapted from DeepEarth paper Figure 6)</figcaption> </figure> <p><strong>Are collisions bad?</strong> Not necessarily. The hash encoding literature<d-cite key="muller2022instant"></d-cite> shows that downstream networks (MLPs) can learn to disambiguate collisions when they’re relatively rare. The hash function’s randomness actually acts as a form of regularization.</p> <p>However, <strong>catastrophic collision patterns</strong> destroy information. If all temporal variations at a location map to the same index, we lose the ability to model temporal dynamics.</p> <h3 id="the-uint32-overflow-discovery">The uint32 Overflow Discovery</h3> <p><strong>A debugging story</strong>: During development, our temporal predictions were inexplicably bad. The model couldn’t distinguish between summer and winter at the same location—as if time didn’t exist.</p> <p>Investigation revealed catastrophic collision patterns:</p> <ul> <li><strong>Level 8</strong>: 100% collision rate (41,261 coordinates mapped to only ~978 unique indices)</li> <li><strong>Levels 13-19</strong>: 99.9% collision rate (coordinates with different timestamps but identical spatial positions mapped to the same memory slot)</li> </ul> <p>This violated fundamental expectations—collision rates should decrease as we move to finer resolutions, not stay constant at 99.9%. The hash function was broken, but how?</p> <p>After extensive debugging, we discovered a <strong>critical integer overflow bug</strong> in the CUDA kernel:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// BUGGY CODE</span>
<span class="kt">uint32_t</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">;</span> <span class="n">d</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">index</span> <span class="o">+=</span> <span class="n">pos_grid</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride</span><span class="p">;</span>
    <span class="n">stride</span> <span class="o">*=</span> <span class="n">resolution</span><span class="p">[</span><span class="n">d</span><span class="p">];</span>  <span class="c1">// OVERFLOW!</span>
<span class="p">}</span>
</code></pre></div></div> <p>At level 8 with resolution 2048 per dimension:</p> <ul> <li>After processing first two dimensions: <code class="language-plaintext highlighter-rouge">stride = 2048 × 2048 = 4,194,304</code></li> <li>Next multiplication: <code class="language-plaintext highlighter-rouge">4,194,304 × 2048 = 8,589,934,592</code></li> <li><strong>This overflows uint32 (max 4,294,967,295) and wraps to 0!</strong></li> </ul> <p>When stride became 0, the temporal dimension contributed nothing to the hash index. All temporal variation was lost.</p> <p><strong>The fix</strong> was simple but critical—use 64-bit arithmetic for intermediate calculations:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// FIXED CODE</span>
<span class="kt">uint64_t</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// Prevents overflow</span>
</code></pre></div></div> <p>After this fix:</p> <ul> <li>Level 8: 100% → <strong>40.5%</strong> collision rate</li> <li>Level 13: 99.9% → <strong>2.4%</strong> collision rate</li> <li>Level 19: 99.9% → <strong>2.1%</strong> collision rate</li> </ul> <p>This bug hunt revealed an important lesson: <strong>subtle integer overflow can catastrophically corrupt spatiotemporal encodings</strong>. The bug only manifested at specific resolution/hash-table size combinations, making it nearly invisible without careful analysis.</p> <blockquote> <p><strong>Key Takeaway</strong>: When implementing multi-resolution encodings, always use 64-bit arithmetic for index calculations, even if final indices fit in 32 bits. Intermediate products can overflow silently, destroying temporal/spatial information in ways that appear as poor model performance rather than obvious crashes.</p> </blockquote> <h3 id="collision-patterns-across-scales">Collision Patterns Across Scales</h3> <p>Even with the overflow bug fixed, hash collisions are inherent to memory-efficient encoding. We profiled collision rates across 10 different spatiotemporal data distributions:</p> <ol> <li><strong>Uniform Random</strong>: Global Earth surface sampling</li> <li><strong>Continental Sparse</strong>: Sparse coverage of North America</li> <li><strong>City-Scale Cluster</strong>: 10km × 10km dense sampling</li> <li><strong>Building-Scale</strong>: Single 10m × 10m area over time</li> <li><strong>Time Series</strong>: Fixed locations sampled repeatedly over time</li> </ol> <p>Results showed collision rates ranging from 0% (coarse levels) to 2-4% (fine levels), with expected power-of-2 artifacts at level 23 (67M grid cells, 4M hash entries = exact 16× ratio).</p> <p>While 2-4% collision rate is acceptable for many applications, we wanted to push further: <strong>Can we reduce hash table size even more while maintaining quality?</strong></p> <p>The answer lies in making collisions smarter, not just rarer.</p> <hr/> <h2 id="learned-hash-probing">Learned Hash Probing</h2> <p>Standard hash encoding treats all collisions equally—coordinates compete randomly for memory slots. But what if we could teach the model to <strong>intelligently allocate</strong> memory based on the actual data distribution?</p> <p>This is where learned hash probing becomes crucial for achieving extreme compression.</p> <h3 id="how-learned-probing-works">How Learned Probing Works</h3> <p><strong>The collision problem visualized</strong>: Imagine multiple coordinates competing for the same memory slot. Standard hash encoding assigns them randomly—some get lucky and land in empty slots, others collide and degrade quality.</p> <p><strong>Learned probing solution</strong>: Give the model multiple candidate slots and let it learn which to use. It’s like having backup hotel rooms—if your first choice is crowded, the system learns to route you to a better alternative.</p> <p><strong>Credit</strong>: Learned hash probing was developed by <strong>Takikawa et al. (2023)</strong> at NVIDIA Toronto AI Lab<d-cite key="takikawa2023compact"></d-cite> as an improvement to InstantNGP. We did not invent this technique—we applied it to Earth observation. Their method uses <strong>dual hashing with learned offsets</strong>:</p> \[\text{index} = N_p \times h_1(\mathbf{x}) + \mathcal{D}_c[h_2(\mathbf{x})]\] <p>Breaking this down:</p> <ul> <li>\(h_1(\mathbf{x})\): Primary hash (rough neighborhood)</li> <li>\(h_2(\mathbf{x})\): Secondary hash selecting among \(N_p\) candidates (typically 4-32 options)</li> <li>\(\mathcal{D}_c\): <strong>Learned codebook</strong>—the model discovers which offsets work best for the data</li> </ul> <p><strong>The learning process</strong>: Initially, the model randomly distributes data across all candidate slots. During training, gradients flow backward through a <strong>straight-through estimator</strong>—the forward pass selects a discrete index (hard choice), but the backward pass distributes gradients across all candidates weighted by softmax probabilities (soft gradients).</p> <p>Over thousands of training steps, the model learns patterns like:</p> <ul> <li>“Densely sampled urban regions should use probes 0-7”</li> <li>“Sparse oceanic regions can share probe 31”</li> <li>“Temporal clusters need dedicated probes to avoid interference”</li> </ul> <p>This <strong>data-adaptive collision resolution</strong> outperforms fixed hash functions because it learns the actual distribution of your training data rather than assuming uniform randomness.</p> <h3 id="extreme-compression-results">Extreme Compression Results</h3> <p>Learned hash probing enables dramatic parameter reduction. On the Globe-LFMC 2.0 benchmark<d-cite key="yebra2024globelfmc"></d-cite>:</p> <table> <thead> <tr> <th>Configuration</th> <th>Parameters</th> <th>GPU Memory</th> <th>Speed</th> <th>MAE</th> <th>R²</th> <th>vs Baseline</th> </tr> </thead> <tbody> <tr> <td><strong>Baseline</strong> (\(2^{22}\) hash)</td> <td>724M</td> <td>12GB+</td> <td>1×</td> <td>16.6 pp</td> <td>0.582</td> <td>—</td> </tr> <tr> <td><strong>Learned Probing</strong> (\(2^{22}\))</td> <td>724M</td> <td>12GB+</td> <td>1.7×</td> <td><strong>12.4 pp</strong></td> <td><strong>0.745</strong></td> <td>+28% R²</td> </tr> <tr> <td><strong>Compressed</strong> (\(2^{14}\) hash)</td> <td><strong>5.1M</strong></td> <td><strong>850MB</strong></td> <td><strong>4×</strong></td> <td><strong>15.0 pp</strong></td> <td><strong>0.668</strong></td> <td>+14.7% R²</td> </tr> </tbody> </table> <p>The compressed configuration achieves:</p> <ul> <li><strong>99.3% parameter reduction</strong> (724M → 5.1M)</li> <li><strong>93% memory reduction</strong> (12GB → 850MB)</li> <li><strong>4× training speedup</strong></li> <li><strong>Still outperforms baseline</strong> by 14.7% R²</li> </ul> <p>This is remarkable: by shrinking the hash table by \(256×\) (\(2^{22}\) → \(2^{14}\)) and adding learned probing, we maintain—and even improve—performance while fitting on edge devices.</p> <p><strong>Why does compression improve performance?</strong> We hypothesize that extreme compression acts as regularization, similar to how restricting a student’s note-taking forces deeper understanding rather than verbatim transcription.</p> <p><strong>Concrete analogy</strong>: Imagine learning geography with different-sized notebooks:</p> <ul> <li><strong>Large notebook (724M params)</strong>: Write down every detail about every location → risk memorizing specific training examples</li> <li><strong>Small notebook (5M params)</strong>: Must capture essential patterns → forced to learn “coastal regions are moist in winter” rather than “GPS coordinate 37.7749°N has LFMC 87% on Jan 15, 2019”</li> </ul> <p>The forced sharing of hash table entries encourages the model to discover <strong>reusable spatiotemporal features</strong> rather than overfitting to training coordinates. This parallels how dropout or weight decay improve generalization—constraints prevent memorization.</p> <hr/> <h2 id="experimental-validation">Experimental Validation</h2> <p>We evaluate Earth4D through three research questions, each testing a critical capability for world models:</p> <blockquote> <p><strong>Q1: Can coordinates alone match multimodal foundation models?</strong> Tests whether spatiotemporal encoding captures enough information to compete with satellite imagery and weather data → <strong>Result</strong>: Earth4D surpasses Galileo foundation model (12.4 vs 12.6 MAE on ecological forecasting)</p> <p><strong>Q2: Can we achieve 99% parameter reduction while maintaining performance?</strong> Tests whether learned hash probing enables extreme compression → <strong>Result</strong>: 724M → 5M parameters with improved R² (0.582 → 0.668)</p> <p><strong>Q3: Can Earth4D learn arbitrary spatiotemporal functions?</strong> Tests generality by predicting RGB pixels from elevation alone → <strong>Result</strong>: 18% lower loss with learned probing on Houston wetlands reconstruction</p> </blockquote> <p>Let’s examine each in detail.</p> <h3 id="q1-matching-foundation-models-with-coordinates-alone">Q1: Matching Foundation Models with Coordinates Alone</h3> <p><strong>Question</strong>: Can Earth4D achieve state-of-the-art performance using only spatiotemporal coordinates, without satellite imagery, weather data, or other multimodal inputs?</p> <p><strong>Dataset</strong>: Globe-LFMC 2.0<d-cite key="yebra2024globelfmc"></d-cite>, a global benchmark for predicting Live Fuel Moisture Content (LFMC)—the percentage of water in vegetation relative to dry weight. LFMC is critical for wildfire risk assessment.</p> <ul> <li>89,764 field measurements across diverse plant species, geographic regions, and temporal periods (2000-2023)</li> <li>Train/test split: 76,467 / 13,297 (official AI2 split for fair comparison)</li> </ul> <p><strong>Baseline</strong>: Galileo<d-cite key="tseng2025galileo"></d-cite>, a Vision Transformer (5.3M parameters) pre-trained by Allen Institute for AI on:</p> <ul> <li>Sentinel-2 optical imagery (10m resolution, 13 spectral bands)</li> <li>Sentinel-1 SAR (radar, cloud-penetrating)</li> <li>ERA-5 weather reanalysis (temperature, precipitation, etc.)</li> <li>TerraClimate soil moisture and climate data</li> <li>SRTM topography (elevation, slope, aspect)</li> <li>(x,y,z,t) coordinates and species type</li> </ul> <p><strong>Earth4D Architecture</strong>:</p> <ul> <li>Earth4D encodes (x,y,z,t) into 192D embeddings</li> <li>Concatenated with learnable species embedding (initialized randomly)</li> <li>MLP predicts LFMC percentage</li> </ul> <p><strong>Results</strong>:</p> <table> <thead> <tr> <th>Model</th> <th>Data Inputs</th> <th>MAE</th> <th>R²</th> </tr> </thead> <tbody> <tr> <td><strong>Galileo</strong> (pretrained)</td> <td>Coordinates + Species + <strong>Multimodal Remote Sensing</strong></td> <td>12.6 pp</td> <td>0.72</td> </tr> <tr> <td><strong>Earth4D</strong> (learned probing)</td> <td><strong>Coordinates + Species only</strong></td> <td><strong>12.4 pp</strong></td> <td><strong>0.745</strong></td> </tr> </tbody> </table> <p>Earth4D <strong>surpasses the pretrained foundation model</strong> (12.4 vs 12.6 MAE, 0.745 vs 0.72 R²) using only coordinates and species embeddings. No satellite imagery. No weather data. No topography.</p> <figure> <div style="display: flex; flex-direction: column; gap: 10px;"> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/geospatial_error_map_epoch_2500.png" alt="LFMC Geographic Error Distribution" style="width: 100%;"/> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/temporal_predictions_epoch_2500.png" alt="LFMC Temporal Predictions" style="width: 100%;"/> </div> <figcaption><b>Earth4D LFMC Prediction Performance.</b> <b>Top</b>: Geographic error distribution across CONUS shows low error in well-sampled regions, with median absolute error of 7.1 percentage points. <b>Bottom</b>: Temporal predictions (black line) closely track ground truth LFMC measurements (gray distribution) across seasons (2017-2023), demonstrating Earth4D's ability to capture seasonal vegetation moisture dynamics using only spatiotemporal coordinates. (Figures from DeepEarth paper)</figcaption> </figure> <p><strong>How is this possible?</strong> Earth4D learns that spatiotemporal coordinates encode surprisingly rich information about vegetation moisture:</p> <p><strong>Spatial patterns</strong>:</p> <ul> <li>Coastal California (37°N, -122°W, low elevation) → high LFMC in winter (Pacific moisture)</li> <li>Arizona desert (33°N, -111°W, moderate elevation) → low LFMC year-round (arid climate)</li> <li>Pacific Northwest (47°N, -122°W, high elevation) → consistently high LFMC (temperate rainforest)</li> </ul> <p><strong>Temporal patterns</strong>:</p> <ul> <li>Summer months (June-August) → lower LFMC across most regions (heat stress, reduced precipitation)</li> <li>Winter months (December-February) → higher LFMC in Mediterranean climates (wet season)</li> <li>Spring months (March-May) → peak LFMC in temperate zones (snowmelt, spring rains)</li> </ul> <p><strong>Elevation effects</strong>:</p> <ul> <li>Low elevation (&lt;500m) → follows regional climate patterns directly</li> <li>Mid elevation (500-1500m) → extended moisture retention from orographic precipitation</li> <li>High elevation (&gt;1500m) → snowpack buffering creates delayed moisture dynamics</li> </ul> <p><strong>Spatiotemporal interactions</strong>: The same location exhibits different LFMC at different times, and the same time period exhibits different LFMC at different locations. Earth4D’s multi-resolution structure captures both:</p> <ul> <li><strong>Coarse levels (398km/cell)</strong>: Encode climate zones—Mediterranean, desert, temperate, tropical</li> <li><strong>Fine levels (4.75cm/cell)</strong>: Encode microclimate variations—north-facing vs south-facing slopes, proximity to water sources, local topographic moisture traps</li> </ul> <p>Crucially, the <strong>species embedding provides botanical context</strong>. Earth4D learns that chaparral shrubs in coastal California have different moisture dynamics than pine trees in the same location, despite identical coordinates. The model discovers species-specific water use strategies encoded in the learnable embedding.</p> <p>This result challenges a fundamental assumption: <strong>you don’t always need satellite imagery and weather data if your positional encoding is expressive enough</strong>. Rich spatiotemporal structure, learned through multi-resolution hash encoding, captures climate patterns that manifest in vegetation moisture.</p> <p><strong>Important caveat</strong>: This doesn’t mean coordinates are universally superior to multimodal data. Earth4D succeeds on LFMC prediction because:</p> <ol> <li>LFMC correlates strongly with climate zones, which are fundamentally spatiotemporal</li> <li>The dataset spans multiple years, allowing temporal patterns to be learned</li> <li>Species identity provides crucial botanical context</li> </ol> <p>For tasks requiring fine-grained visual understanding (crop disease detection, infrastructure damage assessment), satellite imagery would likely remain essential. Earth4D’s success highlights that <strong>some Earth observation tasks may be overengineered</strong>, relying on expensive multimodal data when simpler coordinate-based approaches suffice.</p> <h3 id="q2-can-we-achieve-99-parameter-reduction">Q2: Can We Achieve 99% Parameter Reduction?</h3> <p><strong>Question</strong>: Does the extreme compression result (99% parameter reduction, 4× speedup) shown earlier generalize across different configurations?</p> <p><strong>Experiment</strong>: We systematically vary hash table size and probing range across the LFMC benchmark:</p> <table> <thead> <tr> <th>Hash Size</th> <th>Probing</th> <th>Parameters</th> <th>Speed</th> <th>MAE</th> <th>R²</th> <th>Memory</th> </tr> </thead> <tbody> <tr> <td>\(2^{22}\)</td> <td>Disabled</td> <td>724M</td> <td>1.0×</td> <td>16.6</td> <td>0.582</td> <td>12GB+</td> </tr> <tr> <td>\(2^{22}\)</td> <td>\(N_p=4\)</td> <td>724M</td> <td>1.5×</td> <td>13.2</td> <td>0.698</td> <td>12GB+</td> </tr> <tr> <td>\(2^{22}\)</td> <td>\(N_p=32\)</td> <td>724M</td> <td>1.7×</td> <td><strong>12.4</strong></td> <td><strong>0.745</strong></td> <td>12GB+</td> </tr> <tr> <td>\(2^{18}\)</td> <td>\(N_p=32\)</td> <td>45M</td> <td>1.8×</td> <td>13.8</td> <td>0.672</td> <td>1.5GB</td> </tr> <tr> <td>\(2^{14}\)</td> <td>\(N_p=32\)</td> <td><strong>5.1M</strong></td> <td><strong>4.0×</strong></td> <td>15.0</td> <td>0.668</td> <td>850MB</td> </tr> </tbody> </table> <p><strong>Key findings</strong>:</p> <ol> <li><strong>Learned probing consistently improves performance</strong> even at full hash table size (16.6 → 12.4 MAE)</li> <li><strong>Larger probing range (\(N_p\)) improves quality</strong> but adds training overhead</li> <li><strong>Sweet spot: \(2^{18}\) hash + \(N_p=32\)</strong> balances quality and efficiency (93.8% reduction, strong performance)</li> <li><strong>Extreme compression (\(2^{14}\)) remains viable</strong> for edge deployment</li> </ol> <p>The 99% reduction result is robust across multiple trials and random seeds. The key enabler is learned probing’s ability to adaptively resolve collisions based on data distribution.</p> <h3 id="q3-rgb-reconstruction-from-elevation">Q3: RGB Reconstruction from Elevation</h3> <p><strong>Question</strong>: Can Earth4D learn to infer RGB pixel values from (x,y,z,t) coordinates alone?</p> <p>This tests a different capability: <strong>pure spatiotemporal function approximation</strong> without any auxiliary labels (like species type in LFMC).</p> <p><strong>Dataset</strong>: 5.8M coordinate-color pairs from Houston coastal wetlands:</p> <ul> <li><strong>Input</strong>: USGS 3DEP LiDAR elevation (x,y,z in ECEF, t = acquisition date)</li> <li><strong>Target</strong>: USDA NAIP RGB imagery (R,G,B values at corresponding location/time)</li> </ul> <p>The objective is \((x,y,z,t) \rightarrow (r,g,b)\): given only coordinates, predict the RGB color.</p> <p><strong>Architecture</strong>: Earth4D (192D) → MLP (3 hidden layers, 128 units) → RGB (3 channels)</p> <p><strong>Results</strong>:</p> <table> <thead> <tr> <th>Configuration</th> <th>Validation Loss</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>Baseline (no probing)</td> <td>0.0847</td> <td>—</td> </tr> <tr> <td>Learned Probing (\(N_p=32\))</td> <td><strong>0.0694</strong></td> <td><strong>-18%</strong></td> </tr> </tbody> </table> <figure> <img src="/2026-iclr-blogpost-earth4d/assets/img/2026-04-27-earth4d-world-models/rgb_reconstruction_houston.png" alt="RGB Reconstruction from LiDAR Elevation" style="width: 100%;"/> <figcaption><b>RGB Reconstruction from LiDAR Elevation.</b> Houston coastal wetlands, 2018. <b>Left to right</b>: LiDAR height (input), ground truth RGB, baseline reconstruction (no probing), learned probing reconstruction (18% lower loss). Earth4D learns to infer RGB pixel values from elevation and coordinates alone, capturing correlations between topography and land cover. Water bodies (low, flat elevation) appear blue/green, vegetation (moderate elevation) appears green, and urban areas (varied elevation) show gray/brown tones. (Data from USGS 3DEP and USDA NAIP)</figcaption> </figure> <p>The model learns complex correlations:</p> <ul> <li><strong>Water bodies</strong> (low elevation, flat) → blue/green hues</li> <li><strong>Vegetation</strong> (moderate elevation, rough terrain) → green</li> <li><strong>Urban areas</strong> (high elevation variance) → gray/brown</li> <li><strong>Coastal transitions</strong> (elevation gradients) → color gradients</li> </ul> <p>Learned probing significantly improves reconstruction quality, especially in fine-detail regions like coastline boundaries and vegetation patches.</p> <p><strong>Limitations of this experiment</strong>: While visually impressive, the RGB reconstruction task has significant limitations:</p> <ol> <li><strong>Overfitting to local correlations</strong>: The model learns Houston-specific patterns (local vegetation types, soil colors, urban development). It wouldn’t generalize to other regions with different elevation-color relationships (e.g., desert regions where low elevation doesn’t imply water).</li> <li><strong>Limited semantic understanding</strong>: The model doesn’t understand “what” it’s reconstructing—it’s purely statistical correlation between elevation and color in this specific dataset.</li> <li><strong>Validation loss vs perceptual quality</strong>: 18% loss reduction doesn’t necessarily mean 18% better visual quality. Some fine details may improve while overall structure remains similar.</li> </ol> <p>This experiment demonstrates Earth4D’s ability to fit <strong>implicit spatiotemporal functions</strong>, but doesn’t prove it learns generalizable representations beyond the training distribution. For production applications, task-specific validation would be essential.</p> <hr/> <h2 id="implications-for-world-models">Implications for World Models</h2> <p>Earth4D’s results suggest several important implications for building world models of Earth observation data:</p> <h3 id="1-positional-encodings-as-first-class-features">1. Positional Encodings as First-Class Features</h3> <p>Traditional approaches treat positional information as auxiliary context for satellite imagery or sensor data. Earth4D flips this: <strong>positional encodings can be primary features</strong> that capture rich spatiotemporal patterns.</p> <p>This is analogous to how CLIP<d-cite key="radford2021learning"></d-cite> showed that text alone (without pixel-level annotations) could guide image understanding through contrastive learning. Here, coordinates alone (without multimodal data) can achieve competitive performance through expressive encoding.</p> <p>For world models, this means we can:</p> <ul> <li><strong>Bootstrap</strong> from coordinate-only data when multimodal inputs are unavailable</li> <li><strong>Reduce dependency</strong> on expensive satellite imagery or weather reanalysis</li> <li><strong>Generalize</strong> to regions/times with sparse observational coverage</li> </ul> <h3 id="2-extreme-efficiency-enables-edge-deployment">2. Extreme Efficiency Enables Edge Deployment</h3> <p>The 99% parameter reduction (724M → 5M) makes Earth4D viable for real-world deployment scenarios that were previously impossible:</p> <p><strong>Satellite onboard processing</strong>: Modern satellites like Planet Labs’ Doves have limited computational resources. Earth4D’s 5M parameter compressed model (850MB memory) can run on satellite GPUs, enabling real-time wildfire risk assessment as imagery is captured—eliminating the latency of downlinking data to ground stations.</p> <p><strong>Mobile disaster response</strong>: During wildfires or floods, field teams often operate in areas with limited connectivity. A tablet or smartphone running Earth4D can provide location-specific risk predictions (fire spread likelihood, flood extent forecasts) using only GPS coordinates and local time—no network connection required.</p> <p><strong>IoT sensor networks</strong>: Environmental monitoring stations deployed across forests or agricultural lands often run on solar power with limited energy budgets. Earth4D’s 4× faster inference enables battery-powered edge devices to perform hourly moisture monitoring, triggering alerts when fire risk exceeds thresholds.</p> <p><strong>Developing regions</strong>: Many countries lack access to expensive satellite imagery subscriptions or high-performance computing clusters. Earth4D democratizes Earth observation AI by requiring only coordinate data—freely available from GPS—rather than costly multimodal datasets.</p> <p>This shifts world models from datacenter-scale infrastructure to <strong>ubiquitous deployment</strong>, enabling real-time decision-making where it matters most: on satellites, in the field, and in resource-constrained environments.</p> <h3 id="3-learned-probing-as-universal-compression">3. Learned Probing as Universal Compression</h3> <p><strong>Broader applications</strong>: Takikawa et al.’s learned hash probing technique isn’t specific to Earth observation—it’s a <strong>general method for compressing hash-based neural representations</strong>. Beyond our Earth4D application, it has been used for:</p> <ul> <li>Neural radiance fields (NeRF) for 3D reconstruction</li> <li>Implicit neural representations for any spatiotemporal data</li> <li>Memory-efficient transformers with positional embeddings</li> </ul> <p>The key insight: <strong>let the model learn to resolve collisions</strong> rather than sizing hash tables conservatively.</p> <h3 id="4-downstream-task-agnostic">4. Downstream Task Agnostic</h3> <p>Earth4D produces a 192D embedding per (x,y,z,t) coordinate. This embedding can feed into:</p> <ul> <li><strong>Classification</strong>: Crop type, land cover, disaster detection</li> <li><strong>Regression</strong>: Temperature, precipitation, soil moisture</li> <li><strong>Segmentation</strong>: Flood extent, deforestation boundaries</li> <li><strong>Generation</strong>: Synthesizing satellite imagery from coordinates</li> <li><strong>Forecasting</strong>: Predicting future states from current embeddings</li> </ul> <p>By separating the positional encoder from task-specific heads, we enable <strong>transfer learning</strong> across Earth observation tasks. Pretrain Earth4D on one task (LFMC), then fine-tune on another (crop yield), reusing the spatiotemporal representations.</p> <h3 id="5-foundation-for-multimodal-world-models">5. Foundation for Multimodal World Models</h3> <p>While Earth4D succeeds with coordinates alone, it’s designed for <strong>fusion with multimodal encoders</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Satellite Image] → Vision Encoder → 512D
[Weather Data]    → Time Series Enc → 256D
[(x,y,z,t)]       → Earth4D         → 192D
────────────────────────────────────────────
Concatenate       → Transformer     → Predictions
</code></pre></div></div> <p>The 4D positional embedding provides <strong>spatiotemporal grounding</strong> for other modalities. An image patch at (lat, lon) gets enriched with Earth4D’s multi-resolution features encoding its geospatial context.</p> <p>This mirrors how language models use positional encodings—not as replacements for tokens, but as essential context that enables attention mechanisms to reason about structure.</p> <hr/> <h2 id="limitations-and-future-work">Limitations and Future Work</h2> <p>While Earth4D demonstrates strong performance, several limitations and open questions remain:</p> <h3 id="power-of-2-collision-artifacts">Power-of-2 Collision Artifacts</h3> <p>At level 23 (resolution \(2^{26}\), hash table \(2^{22}\)), collision rate jumps to 3.8% due to exact power-of-2 ratio. This creates periodic artifacts in hash distribution.</p> <p><strong>Mitigation</strong>: Use non-power-of-2 hash table sizes (large primes) or increase hash capacity. However, power-of-2 sizes align with GPU memory boundaries and enable bitwise optimizations.</p> <h3 id="hyperparameter-sensitivity">Hyperparameter Sensitivity</h3> <p>Earth4D has several hyperparameters:</p> <ul> <li>Number of resolution levels (default 24)</li> <li>Hash table size per grid (\(2^{14}\) to \(2^{22}\))</li> <li>Probing range \(N_p\) (2, 4, 8, 16, 32)</li> <li>Codebook size \(N_c\) (512 to 4096)</li> </ul> <p>While we provide reasonable defaults, <strong>optimal settings vary by task</strong>. Automated hyperparameter search (e.g., using validation loss) would improve usability.</p> <h3 id="learning-rate-tuning-for-probing">Learning Rate Tuning for Probing</h3> <p>Index logits gradients are 5-7 orders of magnitude smaller than embedding gradients (inherent to straight-through estimators). We recommend 100× higher learning rate for index logits:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span>
    <span class="p">{</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">:</span> <span class="n">encoder</span><span class="p">.</span><span class="n">embeddings</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">},</span>
    <span class="p">{</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">:</span> <span class="n">encoder</span><span class="p">.</span><span class="n">index_logits</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1e-1</span><span class="p">}</span>
<span class="p">])</span>
</code></pre></div></div> <p>This dual learning rate requirement adds complexity. <strong>Automatic gradient rescaling</strong> could simplify training.</p> <h3 id="global-vs-regional-tradeoffs">Global vs Regional Tradeoffs</h3> <p>Earth4D uses a single hash table for the entire planet. This is memory-efficient but treats all regions equally. Some regions (densely sampled urban areas) may benefit from finer-grained encoding than sparse regions (oceans, deserts).</p> <p><strong>Future work</strong>: Adaptive hash allocation based on data density. Allocate more hash capacity to high-information regions, less to homogeneous areas.</p> <h3 id="temporal-resolution-assumptions">Temporal Resolution Assumptions</h3> <p>Our experiments normalize time to \([0, 1]\) over the dataset’s temporal range. For applications spanning centuries (climate modeling), we may need explicit multi-scale temporal encoding (years, months, days, hours) similar to spatial multi-resolution.</p> <h3 id="interpretability">Interpretability</h3> <p>While Earth4D learns effective representations, understanding <strong>what</strong> it learns remains challenging. Visualization of hash table features could reveal:</p> <ul> <li>Which spatiotemporal patterns activate specific hash entries?</li> <li>How do features at different resolution levels specialize?</li> <li>Can we interpret learned probe offsets?</li> </ul> <p>Techniques from mechanistic interpretability<d-cite key="olah2020zoom"></d-cite> could shed light on Earth4D’s internal representations.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>When we set out to build Earth4D, we had a simple hypothesis: <strong>spatiotemporal structure matters more than we think</strong>. The conventional wisdom in Earth observation AI emphasizes collecting more modalities—higher resolution imagery, more spectral bands, denser weather data, richer metadata. But what if the key isn’t adding more data types, but rather encoding the fundamental structure—space and time—more effectively?</p> <p>Earth4D validates this hypothesis. Through decomposed 4D hash encoding and learned hash probing, it achieves state-of-the-art ecological forecasting using only (x,y,z,t) coordinates. No satellite imagery. No weather reanalysis. No topography. Just position in space-time.</p> <p><strong>Key results</strong>:</p> <ol> <li><strong>Surpasses multimodal foundation models</strong> pretrained on diverse Earth observation data (12.4 vs 12.6 MAE on Globe-LFMC 2.0)</li> <li><strong>99% parameter reduction</strong> (724M → 5M) with maintained or improved performance</li> <li><strong>4× training speedup</strong> enabling edge deployment on satellites, mobile devices, and IoT sensors</li> <li><strong>Planetary-scale coverage</strong> from sub-meter (4.75cm) to continental (398km) resolution, across sub-second to century timescales</li> </ol> <p>These results have implications beyond Earth observation. They suggest that for spatiotemporal problems more broadly—video understanding, robotics, autonomous navigation—<strong>positional encoding deserves first-class treatment</strong>, not just auxiliary context for pixel features.</p> <p><strong>The path forward</strong>: Earth4D is a component of the DeepEarth world model, where it provides spatiotemporal grounding for multimodal encoders. We envision future world models that:</p> <ul> <li><strong>Bootstrap</strong> from coordinates when multimodal data is unavailable (sparse regions, historical periods, privacy-sensitive applications)</li> <li><strong>Compress</strong> through learned hash probing, enabling deployment beyond datacenters</li> <li><strong>Generalize</strong> by learning spatiotemporal patterns that transfer across tasks</li> </ul> <p>As we build AI systems to understand and simulate our planet—for climate adaptation, disaster response, agricultural resilience, and ecosystem monitoring—<strong>how we encode space and time fundamentally shapes what patterns models can discover</strong>.</p> <p>Earth4D demonstrates that with the right positional encoding, coordinates alone can capture the essence of Earth’s spatiotemporal dynamics. The future of world models may not require encoding everything about our planet. Perhaps we just need to encode the structure of space-time effectively—and let the model discover the rest.</p> <hr/> <h2 id="acknowledgments">Acknowledgments</h2> <p>We thank the Allen Institute for AI for releasing the Globe-LFMC 2.0 dataset and Galileo baseline. We thank NVIDIA for open-sourcing InstantNGP, which inspired Earth4D’s architecture. We thank the USGS 3DEP and USDA NAIP programs for providing public LiDAR and imagery data.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[How decomposed 4D hash encoding with learned probing enables planetary-scale deep learning with 99% parameter reduction while matching foundation model performance]]></summary></entry></feed>